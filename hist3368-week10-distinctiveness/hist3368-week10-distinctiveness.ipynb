{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For citation information, please see the \"Source Information\" section listed in the associated README file: https://github.com/stephbuon/digital-history/tree/master/hist3368-week10-distinctiveness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist 3368 - Week 10: Measuring Distinctiveness with Log Likelihood "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Jo Guldi and Steph Buongiorno"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For citation information, please see the \"Source Information\" section listed in the associated README file: https://github.com/stephbuon/digital-history/tree/master/hist3368-week9-log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've been counting many things so far: stop-worded words and words from a controlled vocabulary, words over time, grammatical constructions and parts of speech.  \n",
    "\n",
    "Sometimes, however, what one really wants to know is not the *raw* counts, but rather how two or more categories of counts compare to each other.\n",
    "\n",
    "For instance, imagine that Steph and Jo are friends.  Imagine that they pool their emails to each other into a dataset.  The top words for both of the friends are almost all the same: \"time,\" \"when,\" \"friend,\" \"meet\", \"thanks,\" \"work,\" etc.  The words that are quantitatively the most frequent aren't very informative.  \n",
    "\n",
    "However, if I measure the words that only *one friend* uses and the other friend doesn't, those words are very interesting: \"bike,\" \"handlebar,\" \"crampon,\" \"scuba,\" etc.  We can learn from these words what sports and hobbies are enjoyed by one friend and not the other. \n",
    "\n",
    "Because we've introduced a new rule -- we're looking for words that one person uses but not both -- we're finding \"distinctive\" words.  These words that may not be the most frequent overall, but they are nevertheless some of the most informative.  \n",
    "    \n",
    "Looking for the most *distinctive* words for each of us takes you straight to the give-away clues that I'm a cyclist and my friend is a cave-diver. \n",
    "\n",
    "### Log Likelihood as a Score of Distinctiveness\n",
    "\n",
    "Just so, in this notebook, we're going to learn how to use one of the most common statistical measures for calculating \"distinctiveness.\"  That measure is \"log likelihood.\"  Technically, it's a mathematical improvement on the naive version that I describe here, where I look for my-email(not my-friend's-email), because it performs a calculation that results in a \"log likelihood score,\" which is a guess by the computer of how \"distinctive\" a word is.\n",
    "\n",
    "Applied to the example above, the output might look like this: \n",
    "\n",
    "##### Jo's most distinctive words, with their distinctiveness scores calculated by log-likelihood\n",
    "\n",
    "    bike : 1\n",
    "    handlebar: 1\n",
    "    lube: 1\n",
    "    chain : .3\n",
    "    path : .3\n",
    "\n",
    "##### Steph's most distinctive words, with their distinctiveness scores calculated by log-likelihood\n",
    "\n",
    "    scuba : 1\n",
    "    backroll: 1\n",
    "    bootie: 1\n",
    "    buoyancy: .9\n",
    "    tank : .3\n",
    "    mask : .3\n",
    "    cave : .3\n",
    "\n",
    "Notice that there's a lot more information with these scores than in the way I originally described the experiment. The score of .3 for \"path,\" \"cave,\" etc. Show you that there are some words that I tend to use more than my friend, and conversely her more than me -- but we both *sometimes* use these words.  They're only 30% distinctive, whereas words like \"bike\" or \"scuba\" that only one of us uses are 100% distinctive, and therefore have a higher score. \n",
    "\n",
    "### The Log Likelihood equation\n",
    "\n",
    "Here is what it looks like for one word, \"bike.\" We need four pieces of input and one calculation: \n",
    "    \n",
    "        a = how many times the word \"bike\" appears in Jo's email\n",
    "        b = how many times the word \"bike\" appears in everyone else's email -- Steph's, and if we're measuring others as well, those other friends too: let's say Audre's and Beth's\n",
    "        c = how many times OTHER words than \"bike\" \n",
    "        d = how many times OTHER words than \"bike\" appear in everyone else's email --  which is the equivalent of all the words in the corpus, minus a, minus b, minus c\n",
    "        total_corpus_words = the total number of words in the corpus.  This should be the equivalent of a + b + c + d.\n",
    "      \n",
    "Next, we make three calculations to get the log likelihood score:\n",
    "\n",
    "        E1 = (a + c) * (a + b) / total_corpus_words \n",
    "        E2 = (b + d) * (a + b) / total_corpus_words \n",
    "        \n",
    "        LL = 2 * (a * log(a / E1)) + 2 * b * log(b / E2)\n",
    "\n",
    "If you're a math nerd, you might think about how we're taking a calculation of the specificity of \"a\" (how often I write \"bike\") relative to \"c\" (how many words I wrote in all) and the number of total corpus words, and conversely how specific \"bike\" is to other people (b, d). \n",
    "\n",
    "The function \"log\" normalizes both calculations.  Further multiplications make the calculations more specific to \"a\" (my usage of the word) and \"b\" (my friends' usage of the word).  \n",
    "\n",
    "If you feel overwhelmed: don't worry about this section.  Just accept that we're creating a numerical score of distinctiveness.  It's more important, in this class, to be able to use the distinctiveness measure than to understand where it comes from.  So long as you can run the code in the one section where we tell Python to execute a log likelihood score, you should be fine. \n",
    "\n",
    "\n",
    "### Why you want to know about distinctiveness\n",
    "\n",
    "Distinctiveness scores can be applied to anything. If you're trying to understand what makes speakers in Congress different from each other, you can recreate the above experiment with their words and get a working definition of how different each is -- one that will be far more meaningful than the list of top words used by each, or the words used by only one but not the other.  \n",
    "\n",
    "But you can also do so much more with this wonderful, flexible measure! You can use it to improve your understanding of time. You can use distinctiveness measures to figure out which words were more distinctive of the 1970s than the 1980s, or the 1980s than the 1990s -- and you can then make *those* words the subject of your charts over time or your questions about which speaker introduced a particularly meaningful word.  \n",
    "\n",
    "All you have to do is understand some basic questions about how to format data, how to perform a statistical calculation, and how to use that calculation.\n",
    "\n",
    "### What this notebook will do\n",
    "\n",
    "This notebook will use a sample dataset from EDGAR, the quarterly reports of American corporations.  It will walk you through reformatting the data to get rid of punctuation and words that only appear once (which may be typos, and which, in any case, will throw off the log likelihood calculations, making them less useful).\n",
    "\n",
    "We'll then bring in some tools from the SKLEARN package of statistics and machine learning software, especially CountVectorizer(), vectorizer.get_feature_names(),  and vectorizer.todense().  \n",
    "\n",
    "A \"vector\" is a matrix of words and their frequency, representing every word in the corpus.  For instance, for the example above, a descriptive vector for my emails' words might look like this: \n",
    "\n",
    "    time : 100\n",
    "    when : 100\n",
    "    meet : 99\n",
    "    thanks : 90\n",
    "    bike: 15\n",
    "    handlebar: 10\n",
    "    path: 10\n",
    "    lube: 4\n",
    "    chain: 3\n",
    "\n",
    "You'll use the tools of CountVectorizer to make some word vectors with data that preserves everything else you know about the words -- for instance, which speaker they relate to, when they were published, etc.  Along the way, we'll inspect the data so that you know what's being done to the word vector. \n",
    "\n",
    "We'll then apply the \"log likelihood\" calculation to these vectors to produce a score of what is most distinctive about each speaker's report.  \n",
    "\n",
    "### A word to those new to code\n",
    "\n",
    "If you are learning code for the first time, being able to recreate every single step along the way is far less important than getting a general feel for what the steps are:\n",
    "* cleaning (which you've seen before)\n",
    "* counting (which you've seen before)\n",
    "* vectorizing (this is new, but it's a lot like counting words)\n",
    "* working with vectors (to get them in the right format for statistical analysis)\n",
    "* statistical analysis (a loop that runs the log likelihood equation)\n",
    "* visualization\n",
    "\n",
    "Try to pay attention to just a few new commands to see how they work:\n",
    "* CountVectorizer() -- to turn data into a vector \n",
    "* vectorizer.get_feature_names() -- to inspect a vector \n",
    "* vectorizer.todense -- to reformat a vector for future analysis\n",
    "* the for loop that runs the log likelihood equation.\n",
    "\n",
    "Most importantly, pay attention to what the most distinctive words look like for the corporations we sample, and how we use visualizations to interpret them. Think about how this might apply to your data challenge!\n",
    "\n",
    "### A few suggestions as you start on this week's notebooks:\n",
    "\n",
    "* You will note that the str_replace() method of looking for words and phrases is REALLY slow: \n",
    "\n",
    "        df['Content'] = df['Content'].str.replace(pattern, \"\") \n",
    "        \n",
    "\n",
    "  Don't be afraid to use the alternative code, which is hidden behind a comment (#):\n",
    "\n",
    "        df['Content'] = df['Content'].apply(lambda x: \" \".join(x for x in x.split() if x not in words_appear_once)) \n",
    "        \n",
    "\n",
    "  We're not teaching the grammar of these variations, because they're deep Python rather than the easy syntax of pandas. But you are welcome to use them by copying and pasting!\n",
    "\n",
    "* As always, if you run into trouble around *import* functions, try \"installing\" the software package in question: \n",
    "      \n",
    "          !pip install [package] --user \n",
    "          \n",
    "        \n",
    "* There are at least two places where errors are supposed to occur: \n",
    "\n",
    "     (1) in the first few lines of loading EDGAR, where we drop or delete columns (really a cosmetic step, as I suggested in class). and \n",
    "        \n",
    "     (2) where I tell you to take the log of zero, which will produce an error because the log of zero is infinity.  \n",
    "    In both cases, you should read the commentary.\n",
    "\n",
    "With that, let's dive in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/group/history/hist_3368-jguldi\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/group/history/hist_3368-jguldi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Give this several minutes; we're reading in big data:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_csv(\"congress1967-2010.csv\")\n",
    "#congress = pd.read_csv(\"eighties_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = congress[congress['year'] == 1968]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the dataframe to make sure that the contents match our expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>speech</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185175</th>\n",
       "      <td>185175</td>\n",
       "      <td>185175</td>\n",
       "      <td>Mr. President. I suggest the absence of a quorum.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. MANSFIELD</td>\n",
       "      <td>9</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185176</th>\n",
       "      <td>185176</td>\n",
       "      <td>185176</td>\n",
       "      <td>The clerk will call the roll.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>6</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185177</th>\n",
       "      <td>185177</td>\n",
       "      <td>185177</td>\n",
       "      <td>I announce that the Senator from New Mexico . ...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. BYRD of West Virginia</td>\n",
       "      <td>200</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185178</th>\n",
       "      <td>185178</td>\n",
       "      <td>185178</td>\n",
       "      <td>I announce that the Senator from Massachusetts...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. KUCHEL</td>\n",
       "      <td>66</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185179</th>\n",
       "      <td>185179</td>\n",
       "      <td>185179</td>\n",
       "      <td>A quorum is present.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>4</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185180</th>\n",
       "      <td>185180</td>\n",
       "      <td>185180</td>\n",
       "      <td>The Chair appoints the majority leader. the Se...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>32</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185181</th>\n",
       "      <td>185181</td>\n",
       "      <td>185181</td>\n",
       "      <td>Mr. President. a parliamentary inquiry.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. MORSE</td>\n",
       "      <td>5</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185182</th>\n",
       "      <td>185182</td>\n",
       "      <td>185182</td>\n",
       "      <td>The Senator from Oregon will state it.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>7</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185183</th>\n",
       "      <td>185183</td>\n",
       "      <td>185183</td>\n",
       "      <td>I have two or three parliamentary inquiries. F...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. MORSE</td>\n",
       "      <td>64</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185184</th>\n",
       "      <td>185184</td>\n",
       "      <td>185184</td>\n",
       "      <td>The Chair knows of no such rule that would den...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>14</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1  \\\n",
       "185175      185175        185175   \n",
       "185176      185176        185176   \n",
       "185177      185177        185177   \n",
       "185178      185178        185178   \n",
       "185179      185179        185179   \n",
       "185180      185180        185180   \n",
       "185181      185181        185181   \n",
       "185182      185182        185182   \n",
       "185183      185183        185183   \n",
       "185184      185184        185184   \n",
       "\n",
       "                                                   speech        date  \\\n",
       "185175  Mr. President. I suggest the absence of a quorum.  1968-01-15   \n",
       "185176                      The clerk will call the roll.  1968-01-15   \n",
       "185177  I announce that the Senator from New Mexico . ...  1968-01-15   \n",
       "185178  I announce that the Senator from Massachusetts...  1968-01-15   \n",
       "185179                               A quorum is present.  1968-01-15   \n",
       "185180  The Chair appoints the majority leader. the Se...  1968-01-15   \n",
       "185181            Mr. President. a parliamentary inquiry.  1968-01-15   \n",
       "185182             The Senator from Oregon will state it.  1968-01-15   \n",
       "185183  I have two or three parliamentary inquiries. F...  1968-01-15   \n",
       "185184  The Chair knows of no such rule that would den...  1968-01-15   \n",
       "\n",
       "                          speaker  word_count  year  month  month_year  \n",
       "185175              Mr. MANSFIELD           9  1968      1  1968-01-01  \n",
       "185176         The VICE PRESIDENT           6  1968      1  1968-01-01  \n",
       "185177  Mr. BYRD of West Virginia         200  1968      1  1968-01-01  \n",
       "185178                 Mr. KUCHEL          66  1968      1  1968-01-01  \n",
       "185179         The VICE PRESIDENT           4  1968      1  1968-01-01  \n",
       "185180         The VICE PRESIDENT          32  1968      1  1968-01-01  \n",
       "185181                  Mr. MORSE           5  1968      1  1968-01-01  \n",
       "185182         The VICE PRESIDENT           7  1968      1  1968-01-01  \n",
       "185183                  Mr. MORSE          64  1968      1  1968-01-01  \n",
       "185184         The VICE PRESIDENT          14  1968      1  1968-01-01  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use a tool that will do the lemmatization, lowercasing, and stopwording for us.  All we have to do now is remove punctuation, get rid of digits standing on their own, and eliminate any one-letter items, which are probably initials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_congress = congress.copy()\n",
    "clean_congress['speech'] = clean_congress['speech'].str.replace('[^\\w\\s]','') # remove punctuation\n",
    "clean_congress['speech'] = clean_congress['speech'].str.replace('\\d+', '') # for digits\n",
    "clean_congress['speech'] = clean_congress['speech'].str.replace(r'(\\b\\w{1}\\b)', '') # for short words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the exercise that follows, we'll be measuring what is 'distinctive' about each speaker.  We could configure the experiment in a variety of ways -- comparing speakers; comparing years; comparing all speakers and all years.  What we decide to compare governs how we will arrange the database and measure difference.  But in this experiment, we're just comparing speakers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we want to create a dataframe where every speaker is listed only once, and the 'Content' column holds all the text related to that speaker.  In df, every speaker has several rows, each one with the content from a different year.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>speech</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185175</th>\n",
       "      <td>185175</td>\n",
       "      <td>185175</td>\n",
       "      <td>Mr President  suggest the absence of  quorum</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. MANSFIELD</td>\n",
       "      <td>9</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185176</th>\n",
       "      <td>185176</td>\n",
       "      <td>185176</td>\n",
       "      <td>The clerk will call the roll</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>6</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185177</th>\n",
       "      <td>185177</td>\n",
       "      <td>185177</td>\n",
       "      <td>announce that the Senator from New Mexico  th...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. BYRD of West Virginia</td>\n",
       "      <td>200</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185178</th>\n",
       "      <td>185178</td>\n",
       "      <td>185178</td>\n",
       "      <td>announce that the Senator from Massachusetts ...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. KUCHEL</td>\n",
       "      <td>66</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185179</th>\n",
       "      <td>185179</td>\n",
       "      <td>185179</td>\n",
       "      <td>quorum is present</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>4</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1  \\\n",
       "185175      185175        185175   \n",
       "185176      185176        185176   \n",
       "185177      185177        185177   \n",
       "185178      185178        185178   \n",
       "185179      185179        185179   \n",
       "\n",
       "                                                   speech        date  \\\n",
       "185175       Mr President  suggest the absence of  quorum  1968-01-15   \n",
       "185176                       The clerk will call the roll  1968-01-15   \n",
       "185177   announce that the Senator from New Mexico  th...  1968-01-15   \n",
       "185178   announce that the Senator from Massachusetts ...  1968-01-15   \n",
       "185179                                  quorum is present  1968-01-15   \n",
       "\n",
       "                          speaker  word_count  year  month  month_year  \n",
       "185175              Mr. MANSFIELD           9  1968      1  1968-01-01  \n",
       "185176         The VICE PRESIDENT           6  1968      1  1968-01-01  \n",
       "185177  Mr. BYRD of West Virginia         200  1968      1  1968-01-01  \n",
       "185178                 Mr. KUCHEL          66  1968      1  1968-01-01  \n",
       "185179         The VICE PRESIDENT           4  1968      1  1968-01-01  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_congress[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's move from this long dataframe to a list of the top speaker, their speeches, and the count of how much they spoke overall.\n",
    "\n",
    "* We are grouping by \"speaker\" and adding together (sum()) the column word_count.  \n",
    "\n",
    "* We use sort_values() to arrange by total word count and [:10] to grab the top ten.  \n",
    "\n",
    "* We use ['word_count'] to grab the only column that matters to us -- the total words per speaker.\n",
    "\n",
    "Here are the top ten speakers by total count of words for the year 1968:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all documents of every speaker\n",
    "top_speakers = clean_congress.groupby('speaker').agg({'speech': ' '.join, 'word_count': 'sum'}).sort_values('word_count', ascending = False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mr. JAVITS</th>\n",
       "      <td>Mr President  ask unanimous consent that the S...</td>\n",
       "      <td>278818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. LONG of Louisiana</th>\n",
       "      <td>Mr President will the Senator yield Mr Preside...</td>\n",
       "      <td>254330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The PRESIDING OFFICER</th>\n",
       "      <td>The clerk will call the roll Without objection...</td>\n",
       "      <td>237972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. MANSFIELD</th>\n",
       "      <td>Mr President  suggest the absence of  quorum M...</td>\n",
       "      <td>217784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. BYRD of West Virginia</th>\n",
       "      <td>announce that the Senator from New Mexico  th...</td>\n",
       "      <td>216406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. WILLIAMS of Delaware</th>\n",
       "      <td>Mr President  should like to join my colleague...</td>\n",
       "      <td>215164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. PROXMIRE</th>\n",
       "      <td>Mr President last night President Johnson deli...</td>\n",
       "      <td>189169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. DODD</th>\n",
       "      <td>Mr President immediately after the Presidents ...</td>\n",
       "      <td>174365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. HOLLAND</th>\n",
       "      <td>Mr Speaker no one values more highly than  do ...</td>\n",
       "      <td>171353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. TYDINGS</th>\n",
       "      <td>Mr President as those more familiar with the C...</td>\n",
       "      <td>169010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      speech  \\\n",
       "speaker                                                                        \n",
       "Mr. JAVITS                 Mr President  ask unanimous consent that the S...   \n",
       "Mr. LONG of Louisiana      Mr President will the Senator yield Mr Preside...   \n",
       "The PRESIDING OFFICER      The clerk will call the roll Without objection...   \n",
       "Mr. MANSFIELD              Mr President  suggest the absence of  quorum M...   \n",
       "Mr. BYRD of West Virginia   announce that the Senator from New Mexico  th...   \n",
       "Mr. WILLIAMS of Delaware   Mr President  should like to join my colleague...   \n",
       "Mr. PROXMIRE               Mr President last night President Johnson deli...   \n",
       "Mr. DODD                   Mr President immediately after the Presidents ...   \n",
       "Mr. HOLLAND                Mr Speaker no one values more highly than  do ...   \n",
       "Mr. TYDINGS                Mr President as those more familiar with the C...   \n",
       "\n",
       "                           word_count  \n",
       "speaker                                \n",
       "Mr. JAVITS                     278818  \n",
       "Mr. LONG of Louisiana          254330  \n",
       "The PRESIDING OFFICER          237972  \n",
       "Mr. MANSFIELD                  217784  \n",
       "Mr. BYRD of West Virginia      216406  \n",
       "Mr. WILLIAMS of Delaware       215164  \n",
       "Mr. PROXMIRE                   189169  \n",
       "Mr. DODD                       174365  \n",
       "Mr. HOLLAND                    171353  \n",
       "Mr. TYDINGS                    169010  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get rid of \"the Presiding Officer,\" since that's an office, not a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Mr. JAVITS</th>\n",
       "      <td>Mr President  ask unanimous consent that the S...</td>\n",
       "      <td>278818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. LONG of Louisiana</th>\n",
       "      <td>Mr President will the Senator yield Mr Preside...</td>\n",
       "      <td>254330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. MANSFIELD</th>\n",
       "      <td>Mr President  suggest the absence of  quorum M...</td>\n",
       "      <td>217784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. BYRD of West Virginia</th>\n",
       "      <td>announce that the Senator from New Mexico  th...</td>\n",
       "      <td>216406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. WILLIAMS of Delaware</th>\n",
       "      <td>Mr President  should like to join my colleague...</td>\n",
       "      <td>215164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. PROXMIRE</th>\n",
       "      <td>Mr President last night President Johnson deli...</td>\n",
       "      <td>189169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. DODD</th>\n",
       "      <td>Mr President immediately after the Presidents ...</td>\n",
       "      <td>174365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. HOLLAND</th>\n",
       "      <td>Mr Speaker no one values more highly than  do ...</td>\n",
       "      <td>171353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. TYDINGS</th>\n",
       "      <td>Mr President as those more familiar with the C...</td>\n",
       "      <td>169010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      speech  \\\n",
       "speaker                                                                        \n",
       "Mr. JAVITS                 Mr President  ask unanimous consent that the S...   \n",
       "Mr. LONG of Louisiana      Mr President will the Senator yield Mr Preside...   \n",
       "Mr. MANSFIELD              Mr President  suggest the absence of  quorum M...   \n",
       "Mr. BYRD of West Virginia   announce that the Senator from New Mexico  th...   \n",
       "Mr. WILLIAMS of Delaware   Mr President  should like to join my colleague...   \n",
       "Mr. PROXMIRE               Mr President last night President Johnson deli...   \n",
       "Mr. DODD                   Mr President immediately after the Presidents ...   \n",
       "Mr. HOLLAND                Mr Speaker no one values more highly than  do ...   \n",
       "Mr. TYDINGS                Mr President as those more familiar with the C...   \n",
       "\n",
       "                           word_count  \n",
       "speaker                                \n",
       "Mr. JAVITS                     278818  \n",
       "Mr. LONG of Louisiana          254330  \n",
       "Mr. MANSFIELD                  217784  \n",
       "Mr. BYRD of West Virginia      216406  \n",
       "Mr. WILLIAMS of Delaware       215164  \n",
       "Mr. PROXMIRE                   189169  \n",
       "Mr. DODD                       174365  \n",
       "Mr. HOLLAND                    171353  \n",
       "Mr. TYDINGS                    169010  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_speakers = top_speakers[top_speakers.index != 'The PRESIDING OFFICER']\n",
    "top_speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the entire dataset is only nine rows long.  \n",
    "\n",
    "All the 'speech' columns for multiple entries for each speaker have been pasted into one column by the commands \"groupby\" and \".agg\" (where we told .agg to use .join on the 'speech' column)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Wordcount Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we're going to import some statistics tools from the SKLEARN package (pronounced 'sci kit learn.').  The first tool is CountVectorizer().   CountVectorizer() turns a series of documents into \"vectors,\" or long lists of counts, each of which corresponds to a unique word in the corpus.  \n",
    "\n",
    "*Why is this important?  Because modeling our documents as word vectors will make it easy for us to compute the distinctiveness of words related to speakers.*\n",
    "\n",
    "Here's a brief introduction to CountVectorizer() which explains what wordcount vectors are:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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\n",
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"300\"\n",
       "            src=\"https://www.youtube.com/embed/NhgGEE2BJJo\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.YouTubeVideo at 0x2aab8acc0290>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('NhgGEE2BJJo', width=800, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use as necessary\n",
    "#!pip install sklearn --user\n",
    "#!pip install numpy --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-add123f7f637>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#import some software\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mLemmaTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "#import some software\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Countvectorizer() will begin thinking about making a dataframe for counting every word of the corpus.  for it to work, we have to set some presets: how many features a giant column of entries of counts for every word in the entire corpus (a 'wordcount vector', which we're calling 'allwordcountsvector').  The word 'finance' will have a row; 'security; will have a row, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(\n",
    "    max_features=10000, \n",
    "    lowercase=True, \n",
    "    stop_words = 'english',\n",
    "    ngram_range=(1,2), \n",
    "    analyzer = \"word\",\n",
    "    #tokenizer=LemmaTokenizer()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there isn't anything IN vectorizer yet; we can't look at it.  It's just a collection of commands:\n",
    "- max_features -- tells the vectorizer to look for only the most frequent 10000 terms in the corpus; this means it will throw out many one-time or two-time features.\n",
    "- lowercase=True tells the vectorizer to lowercase everything -- strictly speaking it's redundant since we already did that above.\n",
    "- stop_words -- applies stopwording\n",
    "- ngram_range=(min, max) means that we're asking it to find all one-word ngrams, but nothing over two n-grams in a row. we could play with this.  \n",
    "-analyzer -- can be \"word\" or \"char\".  \n",
    "- LemmaTokenizer() -- lemmatizes words (if used -- slows down the code)\n",
    "\n",
    "SKLEARN has a lot of built-in features for using these tools to make word vectors very smart -- for example, creating custom stop-word lists.  To read up on countvectorizer(), see  https://kavita-ganesan.com/how-to-use-countvectorizer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the fit_transform() command from SKLEARN to create a giant column of entries of counts for every word in the entire corpus (a 'wordcount vector', which we're calling 'vectorizer').  The word 'finance' will have a row; 'security; will have a row, etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = vectorizer.fit_transform(top_speakers['speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You made a word vector. Now what's inside?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigating a Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The section below goes into word vectors deep.  It contains far more information than you need to complete this assignment. However, this background information may be useful for you in understanding what a word vector is.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Word Vector stores the counted up word tallies in the specified column, in this case, ['Content']. They are grouped by the rows in the dataframe, in this case, ['speaker']. So we should be able to extract lots of information: a list of words, a list of counts, and how many different matrices (one per speaker) there are.\n",
    "\n",
    "To extract the word names, we use \n",
    "\n",
    "        vectorizer.get_feature_names() \n",
    "        \n",
    "        \n",
    "To extract the word counts, we use\n",
    "\n",
    "        vectorizer.todense()\n",
    "        \n",
    "\n",
    "To extract general information about how many words and counts there are, we use\n",
    "\n",
    "        vectorized.shape\n",
    "\n",
    "\n",
    "We can also look at any individual item in the word vector by calling its coordinates:\n",
    "\n",
    "        vectorized[speaker_id, word_id]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it actually looks like.  To make wordcount vectors swift, the computer stores the wordcounts as numbers; the labels 'word' and 'speaker' are stored elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer.get_feature_names()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'vectorized' variable also has information about the wordcounts per speaker.  It is a matrix with eight columns and 10,000 words each (remember how we set the word vectors to count the most common 10,000 words?) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call the wordcount vector matrix by giving it the coordinate of every speaker (0 is Javits, 7 is Dodd) and the id number of every word (0 to 10000)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it actually looks like.  To make wordcount vectors swift, the computer stores the wordcounts as numbers; the labels 'word' and 'speaker' are stored elsewhere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's try it out.  How many times does Mr. Mansfield (#3 when you start counting from zero, as Python does) mention the  \"14th amendment\" (#2 in the wordlist when you start counting from zero)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized[3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting information out of a Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section is also outside the bounds of what you really need to know to work with word vectors.  Continue reading if you'd like to see word vectors in action, or skip to the next section.*\n",
    "\n",
    "So far we know what a word vector looks like.  Perhaps we trust that the word vector has modeled our documents. But we might understandably feel frustrated.  Vectors are a little hard to use, frankly; matrices are notoriously hard to get labels out of.  Ideally we'd like a dataframe with a column called \"word\" and another column called \"speaker,\" and the wordcount for each.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NUMPTY and PANDAS packages excel at normalizing difficult-to-read data formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy --user "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of words.  We can use\n",
    "\n",
    "        vectorizer.get_feature_names()\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = np.array(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of speakers. We have this information in the 'row' names of log_df.  \n",
    "\n",
    "Specifying \n",
    "        \n",
    "        .axes[0] \n",
    "\n",
    "\n",
    "lets us call the vertical axis names (that is, the row names of the dataframe).  Two other commands:\n",
    "\n",
    "        to_numpy()  \n",
    "        list()\n",
    "\n",
    "\n",
    "helps with formatting.  Put together:\n",
    "\n",
    "        list(df.axes[0].to_numpy())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_names = list(top_speakers.axes[0].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can glue together all_words and speaker_names into an easy-to-read DataFrame using pandas' pd.DataFrame():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dataframe = pd.DataFrame(vectorized.todense(), # the matrix we saw above is turned into a dataframe\n",
    "                                 columns=all_words,\n",
    "                                 index = speaker_names\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to use this dataframe -- vectors_dataframe -- as the basis for our log likelihood calculations.  It's just a table of word counts that has been standardized across all speakers, so that we can easily count the number of words in the total corpus (= the number of columns), the number of words in any speaker's corpus (= the number of non-zero columns per row), and the count of any word.speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " To use that information, we're going to have to be able to look up the entry for a word per speaker. If we want to call a particular speaker-word count up, like we did above, we use the pandas iat[] command for looking up a value.  \n",
    " \n",
    " * The rule is iat[column, row], or for our purposes, [speaker, word].  \n",
    " * Remember that numbering in Python always starts with 0, so [0, 1] is the entry for the first speaker (Javits) and the second word over (abated):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dataframe.iat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dataframe.iat[7,203]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Who is speaker # 7?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers.index[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is word # 203?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words[203]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Word Vectors to Count Totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you know that you can use Word Vectors as another method to count the number of words in a corpus?  You can!  Just like Counter, most_common(), and .value_counts() -- the tools for counting words we've used so far -- Word Vectors can be used to total the number of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the per-speaker total words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_words_total = vectors_dataframe.sum(axis=1)\n",
    "speaker_words_total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the per-word totals across all speakers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_totals = vectors_dataframe.sum(axis=0) \n",
    "word_totals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words are there overall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_corpus_words = sum(word_totals)\n",
    "total_corpus_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have statistics on how many words or phrases were detected around each speaker, and how many appearances there are per word, as well as word per speaker, we're in a position to do statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring Distinction with Log Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can count the frequency of each word per speaker, we can use that information to calculate the log likelihood of each word per speaker.  As we saw in this week's reading, LL is an excellent proxy for \"distinctiveness;\" high LL scores correspond fairly well to what is unique about each document.  Thus we'll use LL scores to find out which words appear in Long's speeches but not Javits', etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python as a calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, Python can act just like a calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "5 + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 5\n",
    "b = 5\n",
    "a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logarithm refresher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember from high school math that a logarithm is a function that \"normalizes\" big and small numbers from 0 up, so that you can plot enormous differences along a much smaller axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(100000000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, remember that Log doesn't like zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see a pink warning message above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Very Basic Intro to the Log Likelihood Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LL calculation -- which we'll perform for each word-speaker pair -- depends on variables a, b, c, and d: \n",
    "- a: how many times the word being measured appears in ONE speaker whose distinction we're measuring.                                     \n",
    "- b: how many times the word being measured appears in all OTHER speakers               \n",
    "- c: how many OTHER words appear in the speaker whose distinction we're measuring\n",
    "- d: how many times OTHER words appears in all OTHER speakers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've actually already generated all the information we need for a, b, c, d.  Really! We're nearly there.\n",
    "\n",
    "- a: how many times each word occurs per speaker lives in the vectors_dataframe.  We can call it for each speaker and word: \n",
    "                \n",
    "                    vectors_dataframe.iat[speaker number, word id]\n",
    "                    \n",
    "                 \n",
    "- b: How many times each word appears in all other speakers -- is equal to the the total number of occurrences of tha Word, minus a. Recall that we have a dataframe called word_totals, where words are arranged in the same order as they are in vectors_dataframe.  So we can call this number withL \n",
    "            \n",
    "                    word_totals[word_id] - a                 \n",
    "                    \n",
    "                    \n",
    "- c: the speaker's's total words lives in a dataframe called speaker_words_total.  We can get the total of OTHER words by calling that total and subtracting a. We can reference the proper row this way:\n",
    "\n",
    "                speaker_words_total[speaker_id] - a\n",
    "                \n",
    "                \n",
    "- d: the sum of other speakers' use of other words is equal to the total number of words in the corpus, minus a, b, and c.  Fortunately, we already calculated the total number of words and saved it as a variable:\n",
    "                \n",
    "                total_corpus_words - a - b - c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the log likelihood equation that we showed you above, in code. \n",
    "\n",
    "Notice that we make sure that we don't have any zeros in if(b > 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dataframe\n",
    "speakers_loglikelihood = []\n",
    "\n",
    "#loop through every speaker in speaker_names\n",
    "for speaker_id, speaker in enumerate(speaker_names):\n",
    "    loglikely = []\n",
    "    # loop through every word in the wordcount vector:\n",
    "    for word_id in vectorized[speaker_id].indices:\n",
    "        \n",
    "        a = vectors_dataframe.iat[speaker_id, word_id] #  word in speaker\n",
    "        b = word_totals[word_id] - a  # # word in remaining speakers\n",
    "        c = speaker_words_total[speaker_id] - a #  not word in speaker\n",
    "        d = total_corpus_words - a - b - c # not word in remaining speakers\n",
    "       \n",
    "        E1 = (a + c) * (a + b) / total_corpus_words  \n",
    "        E2 = (b + d) * (a + b) / total_corpus_words \n",
    "        \n",
    "        LL = 2 * (a * np.log(a / E1)) # the log likelihood equation\n",
    "        if (b > 0):\n",
    "            LL += 2 * b * np.log(b / E2)\n",
    "        \n",
    "        loglikely.append((LL, all_words[word_id])) # add the log likelihood score to the end of a new dataframe\n",
    "\n",
    "    loglikely = sorted(loglikely, reverse=True) # the loop hits this every time it cycles through all the words in one speaker. \n",
    "    speakers_loglikelihood.append(loglikely) # add on another speaker\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a log-likelihood score for each speaker-word combination.  It's organized as a list of lists.  \n",
    "\n",
    "Each speaker list of words is organized in by descending log-likelihood score. Here's Mr. Long:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speakers_loglikelihood[1][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the most distinctive words per speaker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 20 most distinctive words of each speaker. The words are listed from high to low ranking\")\n",
    "print(\"-------------------------------------------\\n\")\n",
    "for i, speaker in enumerate(speaker_names):\n",
    "    print(speaker + \": \", end='')\n",
    "    distinct_words = [word[1] for word in speakers_loglikelihood[i][:100]]\n",
    "    print(distinct_words)\n",
    "    print(\"\\n-----------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the distinctiveness of the language of each speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can visualizing the data help us to better understand these words and how they relate to the speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use as needed\n",
    "#!pip install adjustText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adjustText\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***If you get an error message here, you might have loaded Jupyter with the \"Spacy\" environment from a previous session.  Restart Jupyter with no special environment and the command should work.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we're going to make a big plot.\n",
    "\n",
    "***This might take a minute or two to run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/digital-history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from adjustText import adjust_text\n",
    "\n",
    "#plt.show()\n",
    "# change the figure's size here\n",
    "#fig = plt.figure(figsize=(15,15), dpi = 500)\n",
    "plt.figure(figsize=(15,15), dpi = 300)\n",
    "\n",
    "# style\n",
    "plt.style.use('seaborn-darkgrid') # this gives us a grid with a dark background.  you can play with this to change the style.\n",
    "  \n",
    "# create a color palette\n",
    "palette = plt.get_cmap('hsv') # this tells matplotlib what colors to use.  you can play with this to change the colors.\n",
    "\n",
    "num = 0\n",
    "\n",
    "texts = []\n",
    "\n",
    "# this is the for loop that creates multiple plots.  \n",
    "for i, speaker in enumerate(speaker_names):\n",
    "        num += 14 # num tells the plot to choose a different color for each speaker\n",
    "        distinct_words = speakers_loglikelihood[i][:15] # plot the top twenty words by LL-score\n",
    "        for word in distinct_words: # for each word-per-speaker instance, plot the ll_score on the y axis\n",
    "            ll_score = word[0] # find just the ll-score from speakers_loglikelihood\n",
    "            word_label = word[1] # find just the keyword name from speakers_loglikelihood\n",
    "            plt.scatter( # draw a dot for each word\n",
    "                    speaker, # with speaker on the x axis\n",
    "                    ll_score, # and ll_score on the y axis\n",
    "                    color=palette(num), # using a different color for each speaker\n",
    "                    linewidth=1, \n",
    "                    edgecolors = 'b',\n",
    "                    s = 55, # size\n",
    "                    alpha=0.3, # make the dots slightly transparent\n",
    "                    label=speaker) # label each dot with the name of the word\n",
    "            texts.append(plt.text(speaker, ll_score, word_label)) # save these coordinates to be used in labeling\n",
    "\n",
    "# Add legend\n",
    "#plt.legend(loc=2, ncol=2)\n",
    "plt.xticks(rotation=90)\n",
    " \n",
    "# Add titles\n",
    "plt.title(\"Figure 1: Highest Log-Likelihood Scored Words per Speaker\", fontsize=30, fontweight=0, color='Red')\n",
    "plt.xlabel(\"Speaker\")\n",
    "plt.ylabel(\"Distinctiveness of Words, Measured by LL Score\")\n",
    "\n",
    "# Code to help with overlapping labels -- may take a minute to run\n",
    "adjust_text(texts, force_points=0.2, force_text=0.2,\n",
    "            expand_points=(1, 1), expand_text=(1, 1),\n",
    "            arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n",
    "\n",
    "\n",
    "\n",
    "# let's save it!\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "fig1.savefig('LL-fig1.jpg', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice in the left-hand sidebar that we've saved a .jpg of this visualization.  \n",
    "\n",
    "We can use visualization to really drill down into the comparisons. We learn that overall, Mr. Javits and Mr. Long have the least unique language.  Mr. Byrd and Mr. Mansfield are more unique.  We could tweak the graph to show more words in a small space, and it gives us a way of inspecting the relative uniqueness of the different word-speaker pairs.\n",
    "\n",
    "We can also learn about some interesting specializations:\n",
    "* Mr. Proximire cares about human rights and genocide\n",
    "* Mr. Dodd talks a lot about the rifle lobby, firearms, and gun control.\n",
    "* Mr. Holland talks about meat and agriculture.\n",
    "* Mr. Williams cares about taxpayers, tax reduction, the budget, and spending\n",
    "* Mr. Javits cares about Israel, coffee and workmen's compensation. \n",
    "* Mr. Long talks about the sea and Louisiana and the continental shelf.\n",
    "\n",
    "But the above visualization is also deeply unsatisfying in some respects.  The graph above doesn't add much new information beyond what we had in the list we printed out.\n",
    "\n",
    "What if we wanted to take things further and compare two speakers?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A 2-D comparison of two speakers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization below shows the same data we just looked at, only zooming it to compare two speakers.  \n",
    "\n",
    "Note that we could also do this -- with more accuracy about how two speakers compare -- by filtering for only two speakers BEFORE we ran the log likelihood equation.  We're not actually creating a measure, here, of the distinctivenss of Mr. Javits and Mr. Long; we're just extracting the information about two speakers from the information about how seven speakers are different from each other. That's a subtle but important mathematical difference.\n",
    "\n",
    "What zooming in on two speakers can do is to remind us that we've got a great deal of information about a great many words -- many more words than showed up in the visualization above, and much more detail than we get from a bar chart of word frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, let's create a dataframe that has the ll_scores from just two speakers. ***The x coordinate will be how distinctive each word is for Javits; the y coordinate will be how distinctive each word is for Long.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new dataframe of the scores and words from both Javits and Long\n",
    "xcoords =  pd.DataFrame(columns=list(['word_label', 'x_llscore']))\n",
    "ycoords = pd.DataFrame(columns=list(['word_label', 'y_llscore']))\n",
    "\n",
    "# get all the words from JAVITS\n",
    "distinctwords = speakers_loglikelihood[0]\n",
    "for word in distinctwords: # for each word-per-speaker instance, plot the ll_score on the y axis\n",
    "    x_llscore = word[0] # find just the ll-score from speakers_loglikelihood\n",
    "    word_label = word[1] # find just the keyword name from speakers_loglikelihood\n",
    "    speaker = 'JAVITS'\n",
    "    xcoords.loc[-1] = [word_label, x_llscore]\n",
    "    xcoords.index = xcoords.index + 1  # shifting index\n",
    "    xcoords = xcoords.sort_index()  # sorting by index\n",
    "\n",
    "# get all the words from LONG\n",
    "distinctwords2 = speakers_loglikelihood[1] \n",
    "for word in distinctwords2: # for each word-per-speaker instance, plot the ll_score on the y axis\n",
    "    y_llscore = word[0] # find just the ll-score from speakers_loglikelihood\n",
    "    word_label = word[1] # find just the keyword name from speakers_loglikelihood\n",
    "    speaker = 'LONG'\n",
    "    ycoords.loc[-1] = [word_label, y_llscore]\n",
    "    ycoords.index = ycoords.index + 1  # shifting index\n",
    "    ycoords = ycoords.sort_index()  # sorting by index\n",
    "\n",
    "coords = pd.merge(xcoords, ycoords, on='word_label')\n",
    "coords = coords.dropna(axis = 0, how ='any') # drop rows with any NA's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to only look at the top words by the log-likelihood score that relates those words to Mr. Javits, what would you do?\n",
    "\n",
    "Remember that the x coordinates tell us how distinctive each word is of Mr. Javits.  Let's take the words with the highest x score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_javits_coords = coords.nlargest(20,['x_llscore']) #notice that nlargest can take multiple arguments, in order of priority\n",
    "top_javits_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_long_coords = coords.nlargest(20,['y_llscore']) #notice that nlargest can take multiple arguments, in order of priority\n",
    "top_long_coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the LL scores, in this case, have already been calculated against all other speakers; these aren't the top scores of how different Javits is from Long.  To get those scores, we'd have to run LL again on just the two speakers.  Instead, we're zooming in on the differences between Javits and Long: the word \"gun\" is very important to both of them, while the words \"consent\" and \"matter\" are distinctive of Javits but not Long.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compare just two speakers, we may want a different visualization to dramatize their differences.\n",
    "\n",
    "***Note that this visualization is slow to draw***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install adjustText --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# change the figure's size here\n",
    "#fig = plt.figure(figsize=(15,15), dpi = 500) \n",
    "plt.figure(figsize=(15,15), dpi = 300)\n",
    "\n",
    "# label each dot with the name of the word -- note that we have to use a \"for\" loop for this to work; plt.annotate only plots\n",
    "# one label per iteration!\n",
    "for i, txt in enumerate(coords['word_label']):\n",
    "    # draw a dot for each word\n",
    "    plt.scatter( \n",
    "        coords['x_llscore'][i], #x axis\n",
    "        coords['y_llscore'][i], # y axis\n",
    "        linewidth=1, \n",
    "        s = 55, # dot size\n",
    "        alpha=0.2)  # dot transparency\n",
    "    # make a label for each word\n",
    "    plt.annotate(\n",
    "        txt, \n",
    "        (coords['x_llscore'][i], # one x llscore at a time\n",
    "         coords['y_llscore'][i]), # one y llscore at a time\n",
    "        alpha=0.3 # i've made the fonts transparent as well.  you could play with color and size if you wanted to. \n",
    "    )\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# logarithmic axes make big things big and small things small\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')  \n",
    "\n",
    "# Add titles\n",
    "plt.title(\"Figure 2: Highest Log-Likelihood Scored Words per Speaker\", fontsize=30, fontweight=0, color='Red')\n",
    "plt.xlabel(\"How Distinctive Each Word is of Mr. Javits\")\n",
    "plt.ylabel(\"How Distinctive Each Word is of Mr. Long\")\n",
    "\n",
    "fig1 = plt.gcf()\n",
    "plt.show()\n",
    "plt.draw()\n",
    "fig1.savefig('LL-fig2.jpg', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the figure above isn't completely intuitive, but it has a lot of information for us.  Words that are equidistant from the x axis and the y axis -- which run on a diagonal from the lower-left-hand corner to the upper-right-hand-corner -- are equally distinctive of both Javits and Long.  Words that are closer to the y axis are more distinctive of Mr. Long, while words that are closer to the x axis are more distinctive of Mr. Javits.  There are a number of words that are highly distinctive of both, as we would expect from two leading politicians. \n",
    "\n",
    "Notice how you could use both of the last two visualizations to complicate your discussion of what is most distinctive about any two speakers in relation to each other, as well as each speaker in relationship to the whole subset.\n",
    "\n",
    "Also, pay attention to whether you need to tweak the visualizations to make them readable.  You can make FIGURE 2 even bigger; you can also restrict FIGURE 2 to showing the 200 highest-scoring words rather than all the words; any of these moves might make it easier to read and interpret the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Data to Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to really understand the results we've generated, we need to look carefully at the data visualizations we've made and think about all that might be remarkable about them.  Why is Mr. Long concerned about the \"bus,\" for example? We're looking for anything that might surprise or support an argument.  And looking for possible surprises is only the first step -- next we have to look into what those words *actually mean*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, if we want to look back at the original text, we can't just load the dataframe 'df.'  We cleaned it and stopworded it.   We'll have to reload 'df' again from the original text so that you have all the stopwords and punctuation in the original documents.  Fortunately, you made a backup called edgar-data.csv in the first section of this notebook. Let's return to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch/group/history/hist_3368-jguldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_csv(\"congress1967-2010.csv\")\n",
    "#congress = pd.read_csv(\"eighties_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going back to the original data, we have the full text of speeches, stopwords and all. This is useful for the purposes of reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = congress[congress['year'] == 1968]\n",
    "top_speakers_speeches = congress[congress['speaker'].isin(top_speakers.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_speakers_speeches[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup: Make a KWIC Dictionary for a Single Speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out all the text for Mr. Long from the 'speech' columns into one long list of words.  We'll lowercase it and remove punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string    \n",
    "long_speeches = top_speakers_speeches[top_speakers_speeches['speaker'] == \"Mr. LONG of Louisiana\"]['speech']\n",
    "long_speeches = ' '.join(long_speeches).lower() # join back together and lowercase\n",
    "long_speeches = long_speeches.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "long_speeches[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add some functions for defining ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNGrams(wordlist, n):\n",
    "    ngrams = []\n",
    "    for i in range(len(wordlist)-(n-1)):\n",
    "        ngrams.append(wordlist[i:i+n])\n",
    "    return ngrams\n",
    "\n",
    "def nGramsToKWICDict(ngrams):\n",
    "    keyindex = len(ngrams[0]) // 2\n",
    "\n",
    "    kwicdict = {}\n",
    "\n",
    "    for k in ngrams:\n",
    "        if k[keyindex] not in kwicdict:\n",
    "            kwicdict[k[keyindex]] = [k]\n",
    "        else:\n",
    "            kwicdict[k[keyindex]].append(k)\n",
    "    return kwicdict\n",
    "\n",
    "\n",
    "def prettyPrintKWIC(kwic):\n",
    "    n = len(kwic)\n",
    "    keyindex = n // 2\n",
    "    width = 20\n",
    "\n",
    "    outstring = ' '.join(kwic[:keyindex]).rjust(width*keyindex)\n",
    "    outstring += str(kwic[keyindex]).center(len(kwic[keyindex])+6)\n",
    "    outstring += ' '.join(kwic[(keyindex+1):])\n",
    "\n",
    "    return outstring\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Break up the full record of Long's speeches into a wordlist and into nGrams and a KWIC dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullwordlist = long_speeches.split()\n",
    "ngrams = getNGrams(fullwordlist, 20)\n",
    "worddict = nGramsToKWICDict(ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use these tools look up any word in the resulting KWIC dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(worddict[\"insurrection\"][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing Research With Log Likelihood and KWIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what words shall we look up in the KWIC dictionary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we generated a list of the words most distinctive of Mr. Long according to Log Likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[pair[1] for pair in speakers_loglikelihood[1][:200]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What words shall we look up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using the words from the above list of distinctive terms for the variable 'target' below to see how Mr. Long used his distinctive words.  Some of Mr. Long's favorite words include 'dangerous,' 'insurrection,' 'convicts,' 'love,' 'security,' 'gun,' 'aggression,' 'mischief,' 'communists,' 'negro', 'klan,' 'riot,' 'guilty,' 'burn,' 'polluted,' and 'cesspool.' \n",
    "\n",
    "* Try switching out the variable 'target' in the cell below with various of the most distinctive terms for Long to investigate what Long was talking about.  \n",
    "* As you read, ask yourself: which among these terms gives you the most surprising insights into the events of the year 1968?   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output KWIC for target word\n",
    "target = 'gun'\n",
    "outstr = '<pre>'\n",
    "if target in worddict:\n",
    "    for k in worddict[target]:\n",
    "        outstr += prettyPrintKWIC(k)\n",
    "        outstr += '             '\n",
    "else:\n",
    "    outstr += 'Keyword not found in source'\n",
    "\n",
    "outstr += '</pre>'\n",
    "outstr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that he was warning about \"civil disorder\" and \"molotov cocktails\" suggests that he regularly spoke about fear related to public gatherings.   The log likelihood measure tells us that Mr. Long used the word \"gun\" more than other speakers of his era.  Could it be that Mr. Long was preaching gun use essentially as an answer to white fears about black people gaining civil rights?  \n",
    "\n",
    "Mr. Long was a white conservative representing Louisiana from the era of Civil Rights.   At least in these short snippets above, we don't have the material to understand whether Mr. Long associated dangerous gatherings and riots with race.  But we might find answers to our question if we keep researching.\n",
    "\n",
    "*Try switching the variable 'target' above to the word 'black' and read more to see how Mr. Long talked about race.  You will see that he draws parallels between the Ku Klux Klan and Black Power movements.  As you probably know, the Ku Klux Klan was actively involved in murdering black men.  The black power movement, by contrast, was largely involved in activities like escorting black children to school, carrying guns to protect themselves from the police.*\n",
    "\n",
    "If I wanted to research this subject for a final essay, I would probably begin with reading some more of the speeches in question. If I were writing a paragraph about this data, I'd probably want to read at least 50 word chunks, making a list of all the times Sen. Long discussed race, then generalizing about them.   *I can adjust how much of the surrounding passages I see by changing the variable 'n' above to a larger or smaller number: ngrams = getNGrams(fullwordlist, n)*\n",
    "\n",
    "If I were writing a paragraph about Sen. Long based on data, I could look for other words that invoke racial struggles, violence, and the fear of public gatherings.  I would read at least 50 words of each excerpt before deciding what he was talking about. \n",
    "\n",
    "I could also go looking for outside context.  \n",
    "\n",
    "If I were researching Senator Long for a final paper, I could begin by reading some more about the senator in a history book.  \n",
    "\n",
    "Again, being able to use KWIC to get a full view of the issue is important to translating mere data into *analysis* and to effectively *interpret* the data so as to make the analysis sharper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each group should do the following:\n",
    "\n",
    "#### Coding Exercise\n",
    "Remember that you can 'group by' different fields.  In the notebook above, the data is grouped by speaker and we measure the words that are most distinctive of each speaker.  *What would happen if you grouped by words?  What if you  looked for the words that were most distinctive of discussions of 'peace' and 'war'?*  \n",
    "\n",
    "* Using data grouped around the words 'war' and 'peace,' recreate Figure 2.  Paste it into a Word document. Label it \"Figure 3.\"\n",
    "* In the code above, we created a dataset grouped by \"month-year\" rather than by \"speaker.\" Recreate Figure 1 using this data.  Paste it into a word document. Label it \"Figure 4.\" \n",
    "\n",
    "     * Note: Below, you'll find an extra section of code that might be useful for grouping the data for pre-processing.  Use this re-grouped data as the basis for running the visualizations above.  \n",
    "     \n",
    "     \n",
    " \n",
    "#### Interpretation Exercise\n",
    "\n",
    "* In the word document, write an interpretive paragraph explaining what we can learn from the new Figure 3.\n",
    "* In the word document, write an interpretive paragraph explaining what we can learn from the new Figure 4.  \n",
    "\n",
    "Find at least three words in your data worth looking up in the actual text.  These should be words that are, prima facie, surprising to you, like 'bus' for Senator Long.  You can \"print\" df to the screen and then use the Command+F/CONTRL+F function to keyword search everything on the page -- or you can look around the web for some other efficient ways of \"printing\" out the text from a dataframe and keyword searching it -- or you could use the KWIC tool above to inspect the data more thoroughly.  Look up some sentences of context, and use the actual discussions to tell us why your speaker is concerned with those words in particular.  \n",
    "- Paste the three figures and text into a document of their own.  Do not submit a Python Notebook.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can \"group by\" different fields "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, also, that we could use .groupby() here to create a dataset organized around ANY field in the data.  We could organize the data around time:\n",
    "        \n",
    "        grouped_speeches = df.copy() \n",
    "        grouped_speeches = log_df.groupby('year').sum() \n",
    "\n",
    "Alternatively, if we had data about Republicans and Democrats or men and women speakers, or around speeches where \"climate change\" occurred and speeches that reference \"the environment,\" we could organize the data around those fields.\n",
    "\n",
    "Look at how this version of the data looks different:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>month_year</th>\n",
       "      <th>speaker</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1968-05-01</th>\n",
       "      <th>Mr. McCLELLAN</th>\n",
       "      <td>Mr. President. I introduce. by request. a bill...</td>\n",
       "      <td>77089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. TYDINGS</th>\n",
       "      <td>Mr. President. on April 19 I wrote to law scho...</td>\n",
       "      <td>67587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">1968-07-01</th>\n",
       "      <th>Mr. MORSE</th>\n",
       "      <td>Mr. President. reasonable men may disagree on ...</td>\n",
       "      <td>55240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. WILLIAMS of Delaware</th>\n",
       "      <td>Mr. President. the Tampa Tribune of June 11. 1...</td>\n",
       "      <td>53427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The PRESIDING OFFICER</th>\n",
       "      <td>Without objection. it is so ordered. The concu...</td>\n",
       "      <td>51436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968-03-01</th>\n",
       "      <th>Mr. STENNIS</th>\n",
       "      <td>Mr. President. will the majority leader yield ...</td>\n",
       "      <td>49284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968-09-01</th>\n",
       "      <th>Mr. DODD</th>\n",
       "      <td>Mr. President. before the Kremlin staged its t...</td>\n",
       "      <td>47081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1968-07-01</th>\n",
       "      <th>Mr. JAVITS</th>\n",
       "      <td>Mr. President. will the Senator yield? Mr. Pre...</td>\n",
       "      <td>46612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">1968-03-01</th>\n",
       "      <th>Mr. JAVITS</th>\n",
       "      <td>Mr. President. we deal. in this cloture vote. ...</td>\n",
       "      <td>46042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mr. LONG of Louisiana</th>\n",
       "      <td>Mr. President. a parliamentary inquiry. Mr. Pr...</td>\n",
       "      <td>45612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                speech  \\\n",
       "month_year speaker                                                                       \n",
       "1968-05-01 Mr. McCLELLAN             Mr. President. I introduce. by request. a bill...   \n",
       "           Mr. TYDINGS               Mr. President. on April 19 I wrote to law scho...   \n",
       "1968-07-01 Mr. MORSE                 Mr. President. reasonable men may disagree on ...   \n",
       "           Mr. WILLIAMS of Delaware  Mr. President. the Tampa Tribune of June 11. 1...   \n",
       "           The PRESIDING OFFICER     Without objection. it is so ordered. The concu...   \n",
       "1968-03-01 Mr. STENNIS               Mr. President. will the majority leader yield ...   \n",
       "1968-09-01 Mr. DODD                  Mr. President. before the Kremlin staged its t...   \n",
       "1968-07-01 Mr. JAVITS                Mr. President. will the Senator yield? Mr. Pre...   \n",
       "1968-03-01 Mr. JAVITS                Mr. President. we deal. in this cloture vote. ...   \n",
       "           Mr. LONG of Louisiana     Mr. President. a parliamentary inquiry. Mr. Pr...   \n",
       "\n",
       "                                     word_count  \n",
       "month_year speaker                               \n",
       "1968-05-01 Mr. McCLELLAN                  77089  \n",
       "           Mr. TYDINGS                    67587  \n",
       "1968-07-01 Mr. MORSE                      55240  \n",
       "           Mr. WILLIAMS of Delaware       53427  \n",
       "           The PRESIDING OFFICER          51436  \n",
       "1968-03-01 Mr. STENNIS                    49284  \n",
       "1968-09-01 Mr. DODD                       47081  \n",
       "1968-07-01 Mr. JAVITS                     46612  \n",
       "1968-03-01 Mr. JAVITS                     46042  \n",
       "           Mr. LONG of Louisiana          45612  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_speeches = congress.copy() \n",
    "grouped_speeches = grouped_speeches.groupby(['month_year', 'speaker']).agg({'speech': ' '.join, 'word_count': 'sum'}).sort_values('word_count', ascending = False)\n",
    "grouped_speeches[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try annotating each report by whether the report writers use the word 'disaster'.\n",
    "\n",
    "We can use str.contains() to detect whether the text in each report's text column (log_df_disaster['Content']) contains the string 'disaster':\n",
    "\n",
    "        log_df_disaster['Content'].str.contains('disaster')\n",
    "\n",
    "\n",
    "This line makes a new column called 'disaster' with the value 'TRUE' for documents that use the word 'disaster' and 'FALSE' if they don't:\n",
    "\n",
    "        log_df_disaster['disaster'] = log_df_disaster['Content'].str.contains('disaster').copy()\n",
    "\n",
    "\n",
    "We can group all the reports together for future analysis by using groupby().sum() with the name of the column we're grouping by in parentheses:\n",
    "\n",
    "        log_df_disaster.groupby('disaster').sum() \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_speeches = congress.copy()\n",
    "grouped_speeches['is_word_there'] = grouped_speeches['speech'].str.contains('peace').copy() # search for string\n",
    "peace_grouped_speeches = grouped_speeches.groupby('is_word_there').agg({'speech': ' '.join, 'word_count': 'sum'}).sort_values('word_count', ascending = False)  # group_by 'is_word_there' T/F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>speech</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_year</th>\n",
       "      <th>is_word_there</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>185175</th>\n",
       "      <td>185175</td>\n",
       "      <td>185175</td>\n",
       "      <td>Mr. President. I suggest the absence of a quorum.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. MANSFIELD</td>\n",
       "      <td>9</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185176</th>\n",
       "      <td>185176</td>\n",
       "      <td>185176</td>\n",
       "      <td>The clerk will call the roll.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>6</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185177</th>\n",
       "      <td>185177</td>\n",
       "      <td>185177</td>\n",
       "      <td>I announce that the Senator from New Mexico . ...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. BYRD of West Virginia</td>\n",
       "      <td>200</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185178</th>\n",
       "      <td>185178</td>\n",
       "      <td>185178</td>\n",
       "      <td>I announce that the Senator from Massachusetts...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. KUCHEL</td>\n",
       "      <td>66</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185179</th>\n",
       "      <td>185179</td>\n",
       "      <td>185179</td>\n",
       "      <td>A quorum is present.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>4</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185180</th>\n",
       "      <td>185180</td>\n",
       "      <td>185180</td>\n",
       "      <td>The Chair appoints the majority leader. the Se...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>32</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185181</th>\n",
       "      <td>185181</td>\n",
       "      <td>185181</td>\n",
       "      <td>Mr. President. a parliamentary inquiry.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. MORSE</td>\n",
       "      <td>5</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185182</th>\n",
       "      <td>185182</td>\n",
       "      <td>185182</td>\n",
       "      <td>The Senator from Oregon will state it.</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>7</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185183</th>\n",
       "      <td>185183</td>\n",
       "      <td>185183</td>\n",
       "      <td>I have two or three parliamentary inquiries. F...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>Mr. MORSE</td>\n",
       "      <td>64</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185184</th>\n",
       "      <td>185184</td>\n",
       "      <td>185184</td>\n",
       "      <td>The Chair knows of no such rule that would den...</td>\n",
       "      <td>1968-01-15</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>14</td>\n",
       "      <td>1968</td>\n",
       "      <td>1</td>\n",
       "      <td>1968-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  Unnamed: 0.1  \\\n",
       "185175      185175        185175   \n",
       "185176      185176        185176   \n",
       "185177      185177        185177   \n",
       "185178      185178        185178   \n",
       "185179      185179        185179   \n",
       "185180      185180        185180   \n",
       "185181      185181        185181   \n",
       "185182      185182        185182   \n",
       "185183      185183        185183   \n",
       "185184      185184        185184   \n",
       "\n",
       "                                                   speech        date  \\\n",
       "185175  Mr. President. I suggest the absence of a quorum.  1968-01-15   \n",
       "185176                      The clerk will call the roll.  1968-01-15   \n",
       "185177  I announce that the Senator from New Mexico . ...  1968-01-15   \n",
       "185178  I announce that the Senator from Massachusetts...  1968-01-15   \n",
       "185179                               A quorum is present.  1968-01-15   \n",
       "185180  The Chair appoints the majority leader. the Se...  1968-01-15   \n",
       "185181            Mr. President. a parliamentary inquiry.  1968-01-15   \n",
       "185182             The Senator from Oregon will state it.  1968-01-15   \n",
       "185183  I have two or three parliamentary inquiries. F...  1968-01-15   \n",
       "185184  The Chair knows of no such rule that would den...  1968-01-15   \n",
       "\n",
       "                          speaker  word_count  year  month  month_year  \\\n",
       "185175              Mr. MANSFIELD           9  1968      1  1968-01-01   \n",
       "185176         The VICE PRESIDENT           6  1968      1  1968-01-01   \n",
       "185177  Mr. BYRD of West Virginia         200  1968      1  1968-01-01   \n",
       "185178                 Mr. KUCHEL          66  1968      1  1968-01-01   \n",
       "185179         The VICE PRESIDENT           4  1968      1  1968-01-01   \n",
       "185180         The VICE PRESIDENT          32  1968      1  1968-01-01   \n",
       "185181                  Mr. MORSE           5  1968      1  1968-01-01   \n",
       "185182         The VICE PRESIDENT           7  1968      1  1968-01-01   \n",
       "185183                  Mr. MORSE          64  1968      1  1968-01-01   \n",
       "185184         The VICE PRESIDENT          14  1968      1  1968-01-01   \n",
       "\n",
       "        is_word_there  \n",
       "185175          False  \n",
       "185176          False  \n",
       "185177          False  \n",
       "185178          False  \n",
       "185179          False  \n",
       "185180          False  \n",
       "185181          False  \n",
       "185182          False  \n",
       "185183          False  \n",
       "185184          False  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_speeches[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We learn from the above that there are more speeches where peace is not mentioned than speeches that mention peace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this method to try to understand the context of a particular word -- for instance, what is characteristic of speeches that refer to 'peace' versus 'war,' or 'climate change' versus 'the environment.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for 'war'\n",
      "                                                          speech  word_count\n",
      "is_word_there                                                               \n",
      "False          Mr. President. I suggest the absence of a quor...    18409189\n",
      "True           Mr. President. on Tuesday. January 16. the Fed...      112802\n",
      "\n",
      "searching for 'peace'\n",
      "                                                          speech  word_count\n",
      "is_word_there                                                               \n",
      "False          Mr. President. I suggest the absence of a quor...    16442330\n",
      "True           I wish to advise the Vice President. the Presi...     2079661\n"
     ]
    }
   ],
   "source": [
    "grouped_speeches2 = congress.copy()\n",
    "grouped_speeches2['is_word_there'] = grouped_speeches2['speech'].str.contains('environmental').copy() # search for string\n",
    "war_grouped_speeches = grouped_speeches2.groupby('is_word_there').agg({'speech': ' '.join, 'word_count': 'sum'}).sort_values('word_count', ascending = False)   # group_by 'is_word_there' T/F\n",
    "\n",
    "print(\"searching for 'war'\")\n",
    "print(war_grouped_speeches)\n",
    "print(\"\")\n",
    "\n",
    "print(\"searching for 'peace'\")\n",
    "print(peace_grouped_speeches)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### So How Do I Get My Data in the Correct Format?\n",
    "\n",
    "What you have made is not yet the data format you need for creating word vectors, but we're getting close.  Here's what you'll need to do:\n",
    "\n",
    "* Remember that the crucial step is to make a dataframe that is organized around the 'units of interest,' with one row per unit of interest.\n",
    "* If the 'unit of interest' is a word, you need to create a dataframe with one row for each word you want to analyze.  If you are analyzing just 'war' and 'peace,' you need a dataframe two rows long.\n",
    "* In addition to the unit of interest, you also need a column for the text.  The text column should contain all the text from the entire corpus relevant to the unit of interest.  For 'war' and 'peace,' you'll want the text column ('speech') to contain all the speeches about 'war' in one row, and all the speeches about 'peace' in another row.\n",
    "\n",
    "Here are the steps you'll follow to understand 'war' and 'peace' in the Congress data:\n",
    "\n",
    "1) First make the 'war' and 'peace' dataframes, which you've just done. \n",
    "\n",
    "2) Next, choose just the rows from each where is_word_there == True.  \n",
    "            \n",
    "            true_war_df = war_df[war_df['is_word_there' == True]]\n",
    "        \n",
    "3)  Next, create a new column for each with the name of the word.  \n",
    "           \n",
    "           true_df_war['keyword'] = 'war' \n",
    "           true_df_peace['keyword'] = 'peace'\n",
    "          \n",
    "4) Next, glue together the true_df for war and the true_df for peace using pd.concat():\n",
    "\n",
    "            frames = [grouped_speeches2, grouped_speeches]\n",
    "            pd.concat(frames) \n",
    "            \n",
    "    At this point you should have a dataframe with two rows and three columns.  One column will be 'keyword' and it will be 'war' for one row and 'peace' for the other.  One column will be 'speech' and it will be all the speeches related to the keyword. There will also be a 'is_word_there' column which is 'True' for both rows (you can delete this or not).\n",
    "    \n",
    "5) Feed this dataframe's 'speech' column to vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
