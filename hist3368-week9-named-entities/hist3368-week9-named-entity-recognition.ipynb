{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist 3368 - Week 9 - Named Entity Recognition\n",
    "\n",
    "#### by Jo Guldi, expanding on a prompt from https://www.geeksforgeeks.org/python-named-entity-recognition-ner-using-spacy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we'll learn how to use the powerful SPACY software library to search for \"named entities.\" \n",
    "\n",
    "A ***named entity*** can be a person's name, the name of a country, the date of a famous event, a kind of money, a corporation, or practically anything else. \n",
    "\n",
    " * In the case of this notebook, we'll learn about some of the results of named entity recognition.  We'll see how SPACY categorized named entities as people, places, dates, etc.  \n",
    "\n",
    " * We'll then apply SPACY to the Dallas City Council minutes, where we'll see that named entity recognition is capable of detecting the names of local government regulations in Texas\n",
    " \n",
    " We will notice that the results of grammatical analysis look *very* different from the results of using WordNet.  The controlled vocabulary organized by linguists at Princeton, for example, doesn't know anything about local government regulations in Dallas, Texas.  But by using grammatical analysis, Spacy will pick up on the names of common government regulations specific to Dallas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Code to Extract Named Entities\n",
    "\n",
    "We'll learn about the *Spacy* software package.  To work with Spacy, you need to tell M2 to begin a session with special parameters that include loading the background software for Spacy.\n",
    "\n",
    "* Please note that to use spaCy on M2 you must go to My Interactive Sessions/JupyterLab and add **source** /hpc/applications/python_environments/spacy/bin/activate to the “Custom environment settings” field.\n",
    "\n",
    "We'll learn about the\n",
    "\n",
    "    nlp(string)\n",
    "    \n",
    "command from Spacy, which tells spacy to extract all named entities from a string of text.   \n",
    "\n",
    "We'll also learn how to read the output of the nlp() command, which creates a dataset whose contents are typically called with a for loop requesting each of the following:\n",
    "\n",
    "    *.ents -- that is, each of the entities for the document created\n",
    "    *.ent_label_ -- for each entity, there is an ent_label that includes information about what kind of named entity Spacy has found.  \n",
    "\n",
    "#### Counting Named Entities and Thinking About Counting\n",
    "\n",
    "We will also see some familiar commands for grouping and counting, which will allow us to model the number of events, persons, corporations, and nationalities mentioned over time with NER.\n",
    "\n",
    "\t.explode().dropna()\n",
    "\t.value_counts()\n",
    "\t.nlargest()\n",
    "\t.groupby()\n",
    "\t.count()\n",
    "\t.unique()\n",
    "\n",
    "We’ll also talk about the best way of counting over time.  Consider the following propositions.  \n",
    "\n",
    "If I want to understand the most important places discussed over time in the Dallas City Council debates, should I pay most attention to:\n",
    "\n",
    "1. the neighborhoods referenced the highest number of months?\n",
    "2. the neighborhoods that are referenced with the maximum number of times in any single month?\n",
    "3. the neighborhoods that are referenced the most frequently overall? \n",
    "\n",
    "We will give you the code for each of these measurements.\n",
    "\n",
    "In theory, each of these measurements might contribute to some analysis — the question is how each of them produces a different model of “significance.” \n",
    "\n",
    "For instance, the first measure — (1) neighborhoods referenced over the most months — will give you a list of neighborhoods referenced consistently, whereas the second measure — (2) the highest relative peak — will give you neighborhoods that became suddenly important at one moment of time.  Both measures might be potentially useful to know about.  \n",
    "\n",
    "We will  be asking you to think about how changing what is being measured changes the meaning of the analysis. \n",
    "\n",
    "#### Learn How to Write a Function\n",
    "\n",
    "We'll learn how to define a function, using the command\n",
    "\n",
    "    def \n",
    "\n",
    "and how to return information from a function with the command\n",
    "\n",
    "    return()\n",
    "\n",
    "We’ll define a function called ***ner_finder(sentence, Label1)*** -- which will return all the named entities of kind label1 from a string called sentence.  \n",
    "\n",
    "In theory, ner_finder() can be applied to the text column of any dataset and produce answers, although it is very slow, as is the way with grammatical analysis. The simple formula for applying ner_finder to a column of text, looking only for the items labeled 'LAW', is this:\n",
    "\n",
    "    results = [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text']]\n",
    "\n",
    "#### Learn Some More About Speed\n",
    "\n",
    "Finally, this lesson adds a few tools that are useful for speeding up slow code and for estimating how long a coding process will take. These tools, while not essential to the work of this notebook, may be useful as you try to implement code on large-scale datasets.\n",
    "\n",
    "We’ll learn about speeding up code using “parallel” processing, which is often faster for large datasets than list comprehension.  We’ll learn the function\n",
    "\n",
    "\t.apply()\n",
    "\n",
    "which \"applies\" a task over over each item in a list or column -- much like a for-loop. \n",
    "\n",
    "To use .apply() with the function ner_finder(), we’ll use the grammar\n",
    "\n",
    "\t.apply(lambda x: [function to be applied])\n",
    "\n",
    "to apply a function like ner_finder() to each item in a pandas column.\n",
    "\n",
    "\n",
    "\n",
    "Technically, you don't need to understand .apply() to work with NER.  We'll learn that these two lines of code do *exactly the same thing,* although one may be faster:\n",
    "\n",
    "    [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text']] -- this is the 'list comprehension' method explained above\n",
    "\tdallas_minutes['Text'].apply(lambda x: ner_finder(x, 'LAW')) -- this is the .apply() method \n",
    "\n",
    "\n",
    "We’ll learn a little about timing data processes with the **time** package.  We’ll use the command\n",
    "\n",
    "\ttime.time() \n",
    "\n",
    "To take the time in milliseconds. We'll use this timer to compare the speed of different approaches to the same code. \n",
    "\n",
    "This tool will allow you to decide, for yourself, between two comparable coding approaches, choosing the one that is more efficient.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, spacy\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.0.0) requires spaCy v2.0 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "bad escape \\p at position 257",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-12102ff9e1a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDeprecationWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_link\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# installed as package\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_package\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# path to model data directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_package\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;34m\"\"\"Load a model from an installed package.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/en_core_web_sm/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(**overrides)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_init_py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_init_py\u001b[0;34m(init_file, **overrides)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE052\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath2str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model_from_path\u001b[0;34m(model_path, meta, **overrides)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(self, path, exclude, disable)\u001b[0m\n\u001b[1;32m    972\u001b[0m             \u001b[0;31m# Convert to list here in case exclude is (default) tuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0mexclude\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_disk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserializers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mfrom_disk\u001b[0;34m(path, readers, exclude)\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# Split to support file names like meta.json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 690\u001b[0;31m             \u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    691\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m    959\u001b[0m         \u001b[0mdeserializers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeserialize_vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"vocab\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m         )\n\u001b[1;32m    963\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.from_disk\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.from_bytes\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/re.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;34m\"Compile a regular expression pattern, returning a pattern object.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpurge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/re.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(pattern, flags)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first argument must be string or compiled pattern\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_compile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mDEBUG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0m_MAXCACHE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/sre_compile.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(p, flags)\u001b[0m\n\u001b[1;32m    560\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msre_parse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m         \u001b[0mpattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(str, flags, pattern)\u001b[0m\n\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_sub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mSRE_FLAG_VERBOSE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mVerbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;31m# the VERBOSE flag was switched on inside the pattern.  to be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse_sub\u001b[0;34m(source, state, verbose, nested)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         itemsappend(_parse(source, state, verbose, nested + 1,\n\u001b[0;32m--> 416\u001b[0;31m                            not nested and not items))\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msourcematch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"|\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_parse\u001b[0;34m(source, state, verbose, nested, first)\u001b[0m\n\u001b[1;32m    525\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\\\\\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m                     \u001b[0mcode1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_class_escape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                     \u001b[0mcode1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLITERAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_ord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/sre_parse.py\u001b[0m in \u001b[0;36m_class_escape\u001b[0;34m(source, escape)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mASCIILETTERS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bad escape %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mescape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mLITERAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mescape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: bad escape \\p at position 257"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'lemmatizer', 'tagger'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting an error? \n",
    "\n",
    "* Please note that to use spaCy on M2 you must go to My Interactive Sessions/JupyterLab and add **source /hpc/applications/python_environments/spacy/bin/activate** to the **“Custom environment settings”** field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package SPACY uses the grammar of sentences to make an educated \"guess\" about what kind of entity is under consideration. \n",
    "\n",
    "Note that the package *isn't* applying a controlled vocabulary of all possible dates or years. It's just looking at grammar -- using clues like the part-of-speech of each of the words and what prepositions are nearby to make a guess about whether each entity is the name of a law, a place, or a person.\n",
    "\n",
    "It's actually *generating* a new controlled vocabulary on the basis of grammar.\n",
    "\n",
    "Named entity recognition can be used as an alternative to controlled vocabulary to create a much more specific, tailor-made vocabulary appropriate to a textbase.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"The United Kingdom passed the Representation of the People Act in 1928, giving women in Wales and London, among others, the right to vote.\"\n",
    "  \n",
    "doc = nlp(sentence)\n",
    "  \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the program has correctly identified a \"geo political entity,\" or GPE, in \"The United Kingdom.\"  It has correctly guessed that we're talking about a law, although it guesses that the name of the law is \"the People Act\" rather than \"The Representation of the People Act.\"  It notes that the correct year is important, 1928.  \n",
    "\n",
    "It's important for you to notice that *some of the information is correct* and *some of the information is rather bad*.  As always, you'll need to apply your own best judgment to the situation.\n",
    "\n",
    "Other things you might be curious about:\n",
    "\n",
    " * In the program in question, we're parsing through a list of information called *.ents*, which spacy creates when applying the command **nlp()** to a document. Our document is called called *doc*, so the entities list is called *.ents*.  \n",
    "\n",
    " * .ents contains information coded by Spacy as the \".text,\" or the word that appears to have a significance, the starting character number, the ending character number, and a .label_, which corresponds to a grammatical category such as \"date,\" \"law,\" \"geo-political entity,\" etc.\n",
    "\n",
    " * Note: This information is hidden in Spacy's own \"Doc\" data type.  You don't really need to know anything for our purposes beyond how to call information from a Spacy doc by asking for ent.label_ etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a more complicated sentence. This one mentions nations and people as well as languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence2 = \"In 1066, the Norman leader William the Conquerer -- who came from the north of France -- invaded England at the Battle of Hastings, and his success is the reason why the English language has so many French words in it.\"\n",
    "  \n",
    "doc = nlp(sentence2)\n",
    "  \n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this time we have many more named entities.  Some are classified \"DATE\" -- like 1066. Others are classified \"EVENT\" -- like the Battle of Hastings.  We also have a person, William the Conquerer, a LANGUAGE -- English, two nationalities or regional identifications for people (NORP), and two geo-political lentities (French, English).\n",
    "\n",
    "One could imagine creating a digital history project that investigated any of these categories. For instance, if we were interested in how Congress handled immigration over the twentieth century, it might be interesting to count references to different nationalities and languages.  If we were interested in how Reddit talked about climate change, it might be interesting to track the people and place-names that came up in discussions of environmental contamination.  \n",
    "\n",
    "However, in this assignment we're going to ask how parliament talked about events and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "justevents = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'EVENT':\n",
    "        justevents.append(ent.text)\n",
    "        \n",
    "justevents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load some data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to load the Dallas City Council minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "council_dir = '/scratch/group/history/hist_3368-jguldi/city_council'\n",
    "\n",
    "dallas_minutes = pd.read_csv(f'{council_dir}/dallas.csv', sep='|', index_col='index') # read in the Dallas City Council minutes\n",
    "dallas_minutes['Text'] = dallas_minutes['Text'].str.replace(r'(?:_|[^\\w\\s])+', '') # remove special characters and whitespace -- we aren't lowercasing becausethe upper cases help with named entity recogniztion.\n",
    "dallas_minutes['Date'] = pd.to_datetime(dallas_minutes['Date'], format='%Y-%m-%d') # cast the data column to datetime objects \n",
    "dallas_minutes['Year'] = [date.year for date in dallas_minutes['Date']]\n",
    "dallas_minutes['Month-Year'] = [str(date.month) + \"-\" + str(date.year) for date in dallas_minutes['Date']]\n",
    "\n",
    "dallas_minutes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our event recognizer to just a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas_minutes_year1 = dallas_minutes[dallas_minutes['Year'] == 2019]\n",
    "dallas_minutes_year1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas_minutes_year1['Text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a spacy parse just on the one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(dallas_minutes_year1['Text'][0])\n",
    "  \n",
    "for ent in doc.ents[:30]:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for just the dates referenced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundlaws = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'LAW':\n",
    "        foundlaws.append(ent.text)\n",
    "        \n",
    "foundlaws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundpeople = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'PERSON':\n",
    "        foundpeople.append(ent.text)\n",
    "foundpeople"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foundevents = []\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == 'EVENT':\n",
    "        foundevents.append(ent.text)\n",
    "        \n",
    "foundevents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a new function.  Functions are defined using the command\n",
    "\n",
    "    def FUNCTIONNAME(object1, object2):\n",
    "    \n",
    "We'll call our new function ner_finder.\n",
    "\n",
    "It will take two commands: sentence, and label1.\n",
    "\n",
    "It will return any matches for a sentence and a label. To tell Python what to return to the user after running a function, we use \n",
    "\n",
    "    return()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_finder(sentence, label1):\n",
    "    \n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    foundstuff = []\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ ==  label1:\n",
    "            foundstuff.append(ent.text)\n",
    "    \n",
    "    return(foundstuff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out our new function on some of the categories that Spacy can detect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_finder(dallas_minutes_year1['Text'][0], 'GPE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_finder(dallas_minutes_year1['Text'][0], 'LAW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_finder(dallas_minutes_year1['Text'][0], 'PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_finder(dallas_minutes_year1['Text'][0], 'LOC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_finder(dallas_minutes_year1['Text'][0], 'DATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_finder(dallas_minutes_year1['Text'][0], 'ORG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_finder(dallas_minutes_year1['Text'][0], 'EVENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition with Tabular Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's apply the spacy *nlp* command to everything in the 'Text' column. The formula that we used above was \n",
    "\n",
    "    nlp(sentence)\n",
    "\n",
    "This time, however, we want to apply *nlp()* to every row in a pandas dataframe.   Here's one way of writing such a command:\n",
    "\n",
    "    dallas_minutes['NLP'] = [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text'][:5]]\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the code hung for a moment, even though we limited the sample to the first five reports of the city council minutes using [:5\n",
    "\n",
    "\n",
    "  * Spacy is an intensive software package, and applying nlp() to many documents can be slow going. Remember how a similarly linguistically-intense command, wn.morphy(), slowed us down elsewhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test how slow your code is on a tiny segment of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Many coders like to keep track of how fast different approaches are so that they can choose the speediest approach when they move from small data to big data. Let's do that.  We'll import the *time* module and call\n",
    "\n",
    "    time.time() \n",
    "    \n",
    "to get the time in milliseconds.  Then we run the same line of code, and call time.time() again afterwards, and subtract start time from finish."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use datetime.time() to take the time before and after the operation to see how quick or slow each operation is.\n",
    "\n",
    "Here's the same code you just ran again, with timing instructions around it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "sample = [ner_finder(sentence, 'LAW') for sentence in dallas_minutes['Text'][:5]]\n",
    "\n",
    "finish = time.time()\n",
    "\n",
    "print(sample)\n",
    "print()\n",
    "\n",
    "finish-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try a speedier approach.  Let's use our parallelized ner_finder to search for mentions of laws in just one year. \n",
    "\n",
    "Again, we'll run the sample code on a tiny sample.  Again, we'll keep track of how long it takes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Speeding things up with .apply()\n",
    "\n",
    "To speed things up, we can try calling upon \"parallel\" processing, which causes every node within a computer system to run the same command simultaneously.  \n",
    "\n",
    "We'll use a 'lambda' function, which allows us to take the function following \"lambda x\" and efficiently \"apply\" it to every row in the dataframe. Lambda functions run in parallel.\n",
    "\n",
    "Note these two elements of the grammar.\n",
    "\n",
    "    .apply()\n",
    "    lambda x: [function to be applied]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a tutorial about using .apply()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/smPLY_5gVv4\" title=\"YouTube video player\"\"></iframe>')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's some code using .apply() with ner_finder() to search for all the laws mentioned in the Dallas Minutes.\n",
    "\n",
    "Note that we are also using *time.time()* to take the time in milliseconds before and after running the function, so that we can compare how fast the .apply() method is to similar code using list comprehension above.\n",
    "\n",
    "**This may still take a minute.** But apply is potentially much, much faster than if you had run the same command wihtout parallel processing.  \n",
    "\n",
    "*Note: You will see a pink warning label. It isn't an error, and the data is still running.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "sample2 = dallas_minutes['Text'][:5].apply(lambda x: ner_finder(x, 'LAW'))\n",
    "\n",
    "finish = time.time()\n",
    "\n",
    "print(sample2)\n",
    "print()\n",
    "\n",
    "finish-start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The winner is... the .apply() method in parallel -- faster by a hair! (*NOTE: Your mileage may vary*)\n",
    "\n",
    "Let's run it on a slightly larger sample of text -- the whole year 2019.  \n",
    "\n",
    "***We chose the faster method on purpose, but NER is a slow process. This process clocks at 30 m on my session. Get a cup of tea.***\n",
    "\n",
    "*You can also limit the amount of text you're working with by using square brackets, e.g. dallas_minutes_year1['Text'][:100]*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "\n",
    "dallas_minutes_year1['Laws'] = dallas_minutes_year1['Text'].apply(lambda x: ner_finder(x, 'LAW'))\n",
    "\n",
    "finish = time.time()\n",
    "print(finish-start)\n",
    "\n",
    "\n",
    "dallas_minutes_year1[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's the code for applying nlp to the entire archive of Dallas City Council minutes, not just one year.  \n",
    "#dallas_minutes['Laws'] = dallas_minutes['Text'].apply(lambda x: ner_finder(x, 'LAW'))\n",
    "#dallas_minutes                                                                  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's filter for JUST the NLP findings that are events."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the most frequently mentioned laws."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the laws listed in the 'Laws' column of *dallas_minutes_year1* are organized as a *list* -- they're inside square brackets, seaprated by commas. \n",
    "\n",
    "In order to count items in a list, we *explode* the list so that each list item gets its own row.  \n",
    "\n",
    "Then we can use *.value_counts()* to count how many times each appears.\n",
    "\n",
    "We can use *.nlargest()* to get the most frequent items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use 'explode,' 'groupby,' and 'count' to find out how many events were referenced per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = dallas_minutes_year1['Laws'].explode().dropna() \n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_twenty_laws = pd.Series(list).value_counts().nlargest(20)\n",
    "top_twenty_laws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Groupby() to Count Laws Mentioned by Month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use *.explode()* and *.groupby()* to tell pandas how to organize the data before using *.count()* to tell us how many laws are mentioned in each grouping.\n",
    "\n",
    "  * Let's \"explode\" the data so that every law gets its own row. \n",
    "  * Let's group by *Month-Year* and *Laws* so that we bundle under each month-year each mention of a law mentioned in that time period.  \n",
    "  * We'll use square brackets around 'Laws' once more to tell pandas that we're interested in just working with the *Laws* column. \n",
    "  * We'll apply *.count()* to count ***how many laws are mentioned per Month-Year.*** This is what we really want to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, 'explode' the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas_minutes_year1 = dallas_minutes_year1.explode('Laws').dropna()\n",
    "dallas_minutes_year1[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, groupby() 'month-year' and 'law', counting how frequently each law shows up for each unit of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws_per_month = dallas_minutes_year1.groupby(['Month-Year', 'Laws'])['Laws'].count()\n",
    "laws_per_month[:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some minor reformatting can turn the resulting Series object into a DataFrame object with properly labeled columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "laws_per_month = pd.DataFrame(laws_per_month) # convert from Series to DataFrame\n",
    "laws_per_month.columns = ['Count'] # label the one column \"count\"\n",
    "laws_per_month = laws_per_month.reset_index() # turn the multi-index into columns 'year' and 'vocab'\n",
    "laws_per_month[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In how many unique months was each law referenced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we're interested in laws that are talked about over time, it might be useful to know in how many different months each law appears.\n",
    "\n",
    "We can also find out in how many months each law was referenced.\n",
    "\n",
    "  * Let's group by 'Laws' so that we treat each instance of mentioning a law as a grouping.\n",
    "  * After grouping by 'Laws', let's just look at the column 'Month-Year'.\n",
    "  * We want to count how many unique month-year combinations there are for each Law.  So let's use *.unique()*.\n",
    "  * We want to count the number of unique month-year combinations. So then we'll *.count()*\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_months_referenced = laws_per_month.groupby(['Laws'])['Month-Year'].unique()\n",
    "number_of_months_referenced "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the laws referenced over the highest number of unique months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would be useful to know which laws were talked about over more than 3 months. \n",
    "\n",
    "\n",
    "We will use *.str.len()* to ask Python for the *lengths* of each list of months is (len(monthlist))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_months_referenced = pd.DataFrame(number_of_months_referenced)\n",
    "number_of_months_referenced['unique_months'] = number_of_months_referenced['Month-Year'].str.len()\n",
    "number_of_months_referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll ask in the code below for just those laws where the number of unique_months is greater or equal than 3.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequently_referenced = number_of_months_referenced[number_of_months_referenced['unique_months']>=3].reset_index()\n",
    "frequently_referenced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is great. But to graph the data, we need one month-year per row.  \n",
    "\n",
    "There are multiple ways of going about the next step, but we'll show one.  Let's connect this data about the most talked-about laws to our data about dates, in the dataframe laws_per_month.\n",
    "\n",
    "Notice the use of .index, .isin(), and [] to filter for a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_laws_per_month = laws_per_month[laws_per_month['Laws'].isin(frequently_referenced['Laws'])]\n",
    "top_laws_per_month[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do some minor reformatting of the Month-Year from string into datetime() data so that the graph will display correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_laws_per_month['Month-Year'] = pd.to_datetime(top_laws_per_month['Month-Year'], format='%m-%Y', errors='coerce')\n",
    "top_laws_per_month[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting a Graph of Named Entities Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "plt.style.use('seaborn-darkgrid') # this gives us a grid with a dark background.  you can play with this to change the style.\n",
    "palette = plt.get_cmap('tab10') # this tells matplotlib what colors to use.  you can play with this to change the colors.\n",
    "num=0\n",
    "\n",
    "for entity in set(top_laws_per_month['Laws']):\n",
    "    num+=1 # for each new word, the counter 'num' changes \n",
    "    x = top_laws_per_month.loc[top_laws_per_month['Laws'] == entity, 'Month-Year']\n",
    "    y = top_laws_per_month.loc[top_laws_per_month['Laws'] == entity, 'Count']\n",
    "    x2, y2 = zip(*sorted(zip(x, y))) # get everything in the right order\n",
    "    plt.plot(x2, # x axis \n",
    "             y2,  # y axis\n",
    "             '-o', # make dots with lines\n",
    "             color=palette(num), alpha=0.7, label=entity) # num tells the plot to choose a different color this time\n",
    "    y3 = max(y2) # label lines at their highest point\n",
    "    x3 = random.choice(top_laws_per_month[top_laws_per_month['Count'] == 1]['Month-Year'].tolist()) # more for finding the highest point\n",
    "    plt.text(x3, y3, entity, color = palette(num), size = 10) # this is the code to supply a label for each line\n",
    "\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) # move the legend\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Figure 1: Top Laws in the Dallas City Council Minutes\", loc='left', fontsize=2, fontweight=5, color='Blue')\n",
    "plt.xlabel(\"Month\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the purposes of clarifying what the graph is showing, I've used \"plt.text()\" to create a label for each line, and assigned that label the same color as each line.  Notice that I've also used the argument \"bbox_to_anchor\" for the legend to move the legend to the left of the chart.\n",
    "\n",
    "*The output is far from ideal.*  I would much prefer to see a graph where the labels aren't overlapping (there is a module for this called adjust_text, which we'll explore later)\n",
    "\n",
    "But the point of this exercise is to show one possible strategy for making charts clearer and more intuitive.\n",
    "\n",
    "It will be the responsibility of each student group to decide on the appropriate strategy for each data set.  An appropriate strategy is one that shows you pertinent information and which is legible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do the same thing with some other categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd probably like to search for multipe categories.  \n",
    "\n",
    "Let's create a function that will apply named entity recognition for a single category, count the stuff over months, find the stuff mentioned over multiple months, and plot it in a line chart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's just run the named entity recognizer one more time over our data to get a list of all possible named entities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ***This may take some time. Get a cup of tea.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "named_entities1 = [nlp(doc) for doc in dallas_minutes_year1['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### alternative syntax\n",
    "# named_entities = dallas_minutes_year1['Text'].apply(lambda x: nlp(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function, ner_mapper.\n",
    "\n",
    "The point of our creating a function here is to show that we can recapitulate all of the code in this notebook, starting with the original dataframe, in such a way that we can efficiently mine the text for multiple kinds of named entity (ORG, GPE, PERSON, etc), thus allowing us to make multiple graphs.\n",
    "\n",
    "Following the line beginning with \"def\", all of the code *inside* the function is code that you have seen before.  \n",
    "\n",
    "Here is what happens.\n",
    "\n",
    "In the line beginning with \"def,\" we define a new function, naming it, ner_mapper.  We also tell Python that our new function will take three objects:\n",
    "\n",
    "   * dataframe1 - a dataframe with column \"Month-Year\"\n",
    "   * column1 - a parsed column of named entities, corresponding to the \"text\" column of dataframe1\n",
    "   * label1 - a label of Spacy named entity categories, for example 'GPE' or 'ORG'\n",
    "   \n",
    "Inside the function:\n",
    "\n",
    "   * We filter just for the named entities that match label1.\n",
    "   * We group those named entities by month-year and count how many there are.\n",
    "   * We find the named entities that are spoken about over 3 months or more.\n",
    "   * We plot a line graph showing those named entities over time.\n",
    "   * We save the figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_mapper(dataframe1, column1, label1):\n",
    "    \n",
    "    print('*****************')\n",
    "    print('LOOKING FOR ', label1)\n",
    "    print('**')\n",
    "    \n",
    "    # filter only for the named entities matching label1 and convert Spacy data to a list of strings\n",
    "    new_column = [[str(ent) for ent in doc.ents if ent.label_ == label1] for doc in column1]\n",
    "    \n",
    "    # count overall instances of named entities\n",
    "    stuffcount = []\n",
    "    for row in new_column:\n",
    "        for item in row:\n",
    "            stuffcount.append(str(item))\n",
    "    stuffcount[:5]\n",
    "    top_stuff = pd.Series.value_counts(stuffcount).nlargest(10)\n",
    "    top_stuff = pd.DataFrame(top_stuff).reset_index()\n",
    "    top_stuff.columns = ['Stuff', 'Count']\n",
    "    top_stuff = top_stuff[top_stuff['Count'] > 1]\n",
    "\n",
    "    print(\"The top instances of \" + label1 + \" are:\")\n",
    "    print(top_stuff[:5])\n",
    "\n",
    "    # add named entities back to dataframe, explode data, count entities by Month-Year\n",
    "    dataframe1['Stuff'] = new_column\n",
    "    dataframe1 = dataframe1.explode('Stuff').dropna()\n",
    "    stuff_per_month = dataframe1.groupby(['Month-Year', 'Stuff'])['Stuff'].count()\n",
    "    stuff_per_month = pd.DataFrame(stuff_per_month) # convert from Series to DataFrame\n",
    "    stuff_per_month.columns = ['Count'] # label the one column \"count\"\n",
    "    stuff_per_month = stuff_per_month.reset_index() # turn the multi-index into columns 'year' and 'vocab'\n",
    "    stuff_per_month[:5]\n",
    "    \n",
    "    # count in how many months each named entity appears, take overall most frequent and most popular\n",
    "    number_of_months_referenced = stuff_per_month.groupby(['Stuff'])['Month-Year'].count().sort_values(ascending = False)  # count in how many months stuff is talked about \n",
    "    number_of_months_referenced = pd.DataFrame(number_of_months_referenced)\n",
    "    number_of_months_referenced.columns = ['Count'] # label the one column \"count\"\n",
    "    frequently_referenced = number_of_months_referenced[:10].reset_index()\n",
    "    top_per_month = stuff_per_month[stuff_per_month['Stuff'].isin(frequently_referenced['Stuff'])]\n",
    "\n",
    "    tosearch = top_per_month['Stuff'].tolist().append(top_stuff['Stuff'].tolist())\n",
    "\n",
    "    top_over_time = stuff_per_month[stuff_per_month['Stuff'].isin(top_per_month['Stuff'].tolist() + top_stuff['Stuff'].tolist())] # get the occurrences per month of the stuff talked about in multiple monhts\n",
    "    #top_over_time = top_over_time[top_over_time['Count'] > 1] # Get things mentioned more than once per period\n",
    "    \n",
    "    top_over_time['Month-Year'] = pd.to_datetime(top_over_time['Month-Year'], format='%m-%Y', errors='coerce')\n",
    "\n",
    "    # Graph entities \n",
    "    plt.clf() # clear last output\n",
    "    figure(figsize=(8, 6), dpi=300)\n",
    "    \n",
    "    plt.style.use('seaborn-darkgrid') # this gives us a grid with a dark background.  you can play with this to change the style.\n",
    "    palette = plt.get_cmap('tab20b') # this tells matplotlib what colors to use.  you can play with this to change the colors.\n",
    "    num=0\n",
    "    \n",
    "    \n",
    "    for entity in set(top_over_time['Stuff']): # one loop for each color/line\n",
    "        \n",
    "        # get points\n",
    "        num+=1 # for each new word, the counter 'num' changes \n",
    "        x = top_over_time.loc[top_over_time['Stuff'] == entity, 'Month-Year'] # x points\n",
    "        y = top_over_time.loc[top_over_time['Stuff'] == entity, 'Count'] # y points\n",
    "        x2, y2 = zip(*sorted(zip(x, y))) # get everything in the right order\n",
    "        \n",
    "        # make a line\n",
    "        plt.plot(x2, # x axis \n",
    "             y2,  # y axis\n",
    "             '-o', # make dots with lines\n",
    "             color=palette(num), alpha=0.7, label=entity) # num tells the plot to choose a different color this time\n",
    "        \n",
    "        # make labels\n",
    "        y3 = max(y2) # label lines at their highest point\n",
    "        x3 = random.choice(top_over_time[top_over_time['Count'] == 1]['Month-Year'].tolist()) # more for finding the highest point\n",
    "        plt.text(x3, y3, entity, color = palette(num), size = 10) # this is the code to supply a label for each line\n",
    "\n",
    "    legd = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) # move the legend\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Figure: Top \" + label1 + \"s in the Dallas City Council Minutes\", loc='left', fontsize=25, fontweight=5, color='Blue')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplots_adjust(right=0.9) # add padding to the right so it doesn't get cut off\n",
    "    plt.savefig('dallas-city-council-2019-ner-for-' + label1 + '-.jpg', bbox_inches='tight') # save - with extra commands so it doesn't get cut off \n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out our new function. It should run smoothly because we've already gathered the time-intensive NER information in a separate dataframe, named_entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_mapper(dallas_minutes_year1, named_entities1, 'EVENT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write a for loop to cycle through multiple categories.  ***This will also take time. You're welcome to skip it. Just notice that we can compare lots of analyses if we reduce our complex cold to a function and loop through multiple aspects of the data.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['LOC', 'PERSON', 'GPE', 'EVENT', 'ORG']:\n",
    "    ner_mapper(dallas_minutes_year1, named_entities1, cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do all the minutes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're probably already hating the process of waiting for Spacy to run nlp() to parse the named entities.  That's right -- it's time consuming and slow, and that's for the Dallas City Council Minutes, which are one of our smallest datasets. \n",
    "\n",
    "If or when you decide to run Spacy's NER on your own dataset, you'll probably have to deal with a lot of waiting. However, you can follow some of the processes in this notebook to help you along:\n",
    "\n",
    "  * You can try different modes of doing the same analysis. You can use timing code to keep track of which process is fastest.\n",
    "  * You can write functions that work on a small subset of data. \n",
    "  * Once you have a working prototype for your code, you can apply your functions to a larger set of data and leave the computer running all night -- or all week or month if you have to.  \n",
    "\n",
    "If you're running code for a full day or even week, there are extra options that will allow you to start your Python code and walk away, downloading the results when everything is finished. Check in with the M2 staff if you need more guidance about running slow Python code.\n",
    "\n",
    "For THIS data, however, we've already run the boring part for you!  We ran the Spacy parser on the full Dallas dataset overnight, and stored the results in the city_council folder below.  The new function that follows, ner_mapper_all_dallas, reads in the stored datasets for each possible NER label.  mYou just have to run the function, loading saved information for 15 years of Dallas minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Here's the code to extract the named entities from the entire dallas minutes\n",
    "#named_entities = [nlp(doc) for doc in dallas_minutes['Text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /scratch/group/history/hist_3368-jguldi/city_council"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ner_mapper_all_dallas(label1):\n",
    "\n",
    "    stuff_per_month = pd.read_csv('named-entities-per-month-for-dallas-' + label1 + \".csv\") # save a copy of the data\n",
    "\n",
    "    # count in how many months each named entity appears, take overall most frequent and most popular\n",
    "    number_of_months_referenced = stuff_per_month.groupby(['Stuff'])['Month-Year'].count().sort_values(ascending = False)  # count in how many months stuff is talked about \n",
    "    number_of_months_referenced = pd.DataFrame(number_of_months_referenced)\n",
    "    number_of_months_referenced.columns = ['Count'] # label the one column \"count\"\n",
    "    frequently_referenced = number_of_months_referenced[:10].reset_index()\n",
    "    top_per_month = stuff_per_month[stuff_per_month['Stuff'].isin(frequently_referenced['Stuff'])]\n",
    "\n",
    "\n",
    "    tosearch = top_per_month['Stuff'].tolist() \n",
    "\n",
    "    top_over_time = stuff_per_month[stuff_per_month['Stuff'].isin(top_per_month['Stuff'].tolist() + top_stuff['Stuff'].tolist())] # get the occurrences per month of the stuff talked about in multiple monhts\n",
    "    #top_over_time = top_over_time[top_over_time['Count'] > 1] # Get things mentioned more than once per period\n",
    "    \n",
    "    top_over_time['Month-Year'] = pd.to_datetime(top_over_time['Month-Year'], format='%m-%Y', errors='coerce')\n",
    "\n",
    "    # Graph entities \n",
    "    plt.clf() # clear last output\n",
    "    figure(figsize=(8, 6), dpi=300)\n",
    "    \n",
    "    plt.style.use('seaborn-darkgrid') # this gives us a grid with a dark background.  you can play with this to change the style.\n",
    "    palette = plt.get_cmap('tab20b') # this tells matplotlib what colors to use.  you can play with this to change the colors.\n",
    "    num=0\n",
    "    \n",
    "    \n",
    "    for entity in set(top_over_time['Stuff']): # one loop for each color/line\n",
    "        \n",
    "        # get points\n",
    "        num+=1 # for each new word, the counter 'num' changes \n",
    "        x = top_over_time.loc[top_over_time['Stuff'] == entity, 'Month-Year'] # x points\n",
    "        y = top_over_time.loc[top_over_time['Stuff'] == entity, 'Count'] # y points\n",
    "        x2, y2 = zip(*sorted(zip(x, y))) # get everything in the right order\n",
    "        \n",
    "        # make a line\n",
    "        plt.plot(x2, # x axis \n",
    "             y2,  # y axis\n",
    "             '-o', # make dots with lines\n",
    "             color=palette(num), alpha=0.7, label=entity) # num tells the plot to choose a different color this time\n",
    "        \n",
    "        # make labels\n",
    "        y3 = max(y2) # label lines at their highest point\n",
    "        x3 = random.choice(top_over_time[top_over_time['Count'] == 1]['Month-Year'].tolist()) # more for finding the highest point\n",
    "        plt.text(x3, y3, entity, color = palette(num), size = 10) # this is the code to supply a label for each line\n",
    "\n",
    "    legd = plt.legend(loc='center left', bbox_to_anchor=(1, 0.5)) # move the legend\n",
    "    \n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title(\"Figure: Top \" + label1 + \"s in the Dallas City Council Minutes\", loc='left', fontsize=25, fontweight=5, color='Blue')\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.subplots_adjust(right=0.9) # add padding to the right so it doesn't get cut off\n",
    "    plt.savefig('dallas-city-council-2019-ner-for-' + label1 + '-.jpg', bbox_inches='tight') # save - with extra commands so it doesn't get cut off \n",
    "    plt.figure()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat in ['LAW', 'LOC', 'PERSON', 'GPE', 'EVENT', 'ORG', 'LANGUAGE']:\n",
    "    ner_mapper_all_dallas(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment\n",
    "\n",
    "While our work with named entities is coming along, there's still a lot of bad information -- which distorts the graphs -- and clutter -- which makes the graphs hard to read.\n",
    "\n",
    "*Thinking About the Results*\n",
    "\n",
    " * In the code above, we saw TWO ways to decide which entities were most important.  \n",
    "    * One is to count the total number of months where an entity was mentioned -- a process that favors entities that were talked about regularly. \n",
    "    * The other is to count the overall presence of each entity, choosing the entities that were mentioned the most overall. \n",
    "    * We could imagine lots of other ways of deciding which entities deserve to be graphed.  What about counting the monthly high of each entity, then choosing the entities with the highest count for any month. \n",
    "    \n",
    "    *In a paragraph of at least three sentences, please answer:*\n",
    "    \n",
    "    * What are the differences between these three approaches?  \n",
    "    * Which approach should you use if you want to understand change over time -- how some months are different than others?\n",
    "    \n",
    "*Coding Exercise*\n",
    "\n",
    "Change the code to adjust for the following criteria:\n",
    "\n",
    " * Remove any points that represent a count of less than 3.\n",
    " \n",
    " * Create a stopwords list for three of the entities (e.g., \"GPE,\" \"LOCs,\" and \"Person\") and get rid of ambiguous terms (such as \"South,\" which could mean \"South of 635\" rather than \"South Dallas), terms that are clearly misclassified, or terms that convey no information, for instance, rejecting the classification of Salazar (a person) as a city, or combining multiple ways of referring to Dallas as \"DALLAS,\" \"the City,\" \"The City of Dallas,\" etc. \n",
    " \n",
    " * Change the code so the entities graphed match the measurement of significance that you describe above. You'll need to make a list that matches your judgment -- for instance, using *max()* or *.nlargest()* to grab the data that corresponds to the entities that have the largest monthly counts in any given month.  You will then need to tweak the code that produces the variable *top_over_time*, which is what we ultimately graph. You'll want to use your new list of most important entities with  *.isin()*  to save the data that matches your criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
