{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Mini Notebook: Searching Bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some Jane Austen Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, numpy, re, matplotlib# , num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 13] Permission denied: '/scratch/group/history/hist_3368-jguldi'\n",
      "/users/dbalut/digital-history/hist3368-week3-controlled-vocab\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/group/history/hist_3368-jguldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'senseandsensibility.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-cba6822ec780>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#download some data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'senseandsensibility.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0msas_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmyfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n\\n\"I suppose you know, ma\\'am, that Mr. Ferrars is married\"\\n\\nIt _was_ Edward\\n\\n\"Everything in such respectable condition\"\\n\\n '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'THE END'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'senseandsensibility.txt'"
     ]
    }
   ],
   "source": [
    "#download some data\n",
    "\n",
    "with open('senseandsensibility.txt', 'r') as myfile:\n",
    "    sas_data = myfile.read().split('\\n\\n\"I suppose you know, ma\\'am, that Mr. Ferrars is married\"\\n\\nIt _was_ Edward\\n\\n\"Everything in such respectable condition\"\\n\\n ')[1].split('THE END')[0].strip()\n",
    "\n",
    "with open('emma.txt', 'r') as myfile:\n",
    "    emma_data = myfile.read().split('CHAPTER I')[1].split('FINIS')[0].strip()\n",
    "\n",
    "with open('prideandprejudice.txt', 'r') as myfile:\n",
    "    pap_data = myfile.read().split('CHAPTER I')[1].split('End of the Project Gutenberg EBook of Pride and Prejudice, by Jane Austen')[0].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your data matches what you think it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sas_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-181464b13693>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# printing only first 2000 characters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msas_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sas_data' is not defined"
     ]
    }
   ],
   "source": [
    "# printing only first 2000 characters.\n",
    "sas_data[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "Isn't it getting tired, retyping the same command for each novel? Let's throw them all into one data set so we can loop through them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sas_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-aa265e50e317>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0msas_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memma_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpap_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sas_data' is not defined"
     ]
    }
   ],
   "source": [
    "data = [sas_data, emma_data, pap_data]\n",
    "data[0][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still appear to be some errors where spaces have been replaced by \"\\n\".  We'll get rid of those in a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*       *       *       *       *     CHAPTER I   The family of Dashwood had long been settled in Sussex. Their estate was large, and their residence was at Norland Park, in the centre of their property, where, for many generations, they had lived in so respectable a manner as to engage the general good opinion of their surrounding acquaintance. The late owner of this estate was a single man, who lived to a very advanced age, and who for many years of his life, had a constant companion and housekeeper in his sister. But her death, which happened ten years before his own, produced a great alteration in his home; for to supply her loss, he invited and received into his house the family of his nephew Mr. Henry Dashwood, the legal inheritor of the Norland estate, and the person to whom he intended to bequeath it. In the society of his nephew and niece, and their children, the old Gentleman's days were comfortably spent. His attachment to them all increased. The constant attention of Mr. and Mrs. Henry Dashwood to his wishes, which proceeded not merely from interest, but from goodness of heart, gave him every degree of solid comfort which his age could receive; and the cheerfulness of the children added a relish to his existence.  By a former marriage, Mr. Henry Dashwood had one son: by his present lady, three daughters. The son, a steady respectable young man, was amply provided for by the fortune of his mother, which had been large, and half of which devolved on him on his coming of age. By his own marriage, likewise, which happened soon afterwards, he added to his wealth. To him therefore the succession to the Norland estate was not so really important as to his sisters; for their fortune, independent of what might arise to them from their father's inheriting that property, could be but small. Their mother had nothing, and their father only seven thousand pounds in his own disposal; for the remaining moiety of his first wife's fortune was also secured to her child, an\""
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace('\\n', ' ')\n",
    "data[0][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the text into words and print the first word of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '*', '*', '*', '*', 'CHAPTER', 'I', 'The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex.', 'Their', 'estate', 'was']\n",
      "['Emma', 'Woodhouse,', 'handsome,', 'clever,', 'and', 'rich,', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition,', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best']\n",
      "['.', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged,', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune,', 'must', 'be', 'in']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    print(words[:20]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Novels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's lowercase the text and get rid of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # data[i] is the current novel\n",
    "    data[i] = data[i].lower() # force to lowercase\n",
    "    data[i] = re.sub('[\\\",.;:?([)\\]_*]', '', data[i]) # remove punctuation and special characters with regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of removing suffices, like \"ed\" or \"ing\".\n",
    "\n",
    "We will use another standard NLTK package, PorterStemmer, to do the stemming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'i',\n",
       " 'the',\n",
       " 'famili',\n",
       " 'of',\n",
       " 'dashwood',\n",
       " 'had',\n",
       " 'long',\n",
       " 'been',\n",
       " 'settl',\n",
       " 'in',\n",
       " 'sussex',\n",
       " 'their',\n",
       " 'estat',\n",
       " 'wa',\n",
       " 'larg',\n",
       " 'and',\n",
       " 'their',\n",
       " 'resid',\n",
       " 'wa']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "st = PorterStemmer()\n",
    "\n",
    "stemmed_list = []\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    for word in words:\n",
    "        stemmed = st.stem(word)\n",
    "        stemmed_list.append(stemmed)\n",
    "        \n",
    "stemmed_list[:20] # i have changed this so you print just the first words\n",
    "# printing all the words is actually way more computer intensive than it may seem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, \"universal\" becomes \"univers\" (which means that \"universally\" will be counted with \"universal\" and \"universe\") and \"single\" becomes \"singl\" (which means it would be counted with \"singled\").  But \"acknowledged\" has been left as it is.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the word counts look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-25bb87c018e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mnovel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnovel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    count = Counter(words)\n",
    "    print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Words and N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the word counts look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4092), ('to', 4090), ('of', 3573), ('and', 3419), ('her', 2522), ('a', 2048), ('i', 1948), ('in', 1937), ('was', 1848), ('it', 1701)]\n",
      "[('and', 107), ('to', 102), ('a', 92), ('of', 90), ('the', 81), ('her', 61), ('i', 49), ('you', 48), ('it', 45), ('in', 43)]\n",
      "[('you', 31), ('of', 29), ('to', 22), ('a', 21), ('the', 18), ('and', 17), ('i', 17), ('that', 15), ('it', 14), ('is', 14)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    count = Counter(words)\n",
    "    print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to look for multi-word phrases instead of individual words.  For example, if we're researching the living spaces of Jane Austen's England, we definitely want to know whether she refers to \"dining rooms\" or \"bed-rooms\" (which our punctuation clean-up might have turned into separate words, depending on what we did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['chapter', 'i', 'the']),\n",
       " WordList(['i', 'the', 'family']),\n",
       " WordList(['the', 'family', 'of']),\n",
       " WordList(['family', 'of', 'dashwood']),\n",
       " WordList(['of', 'dashwood', 'had']),\n",
       " WordList(['dashwood', 'had', 'long']),\n",
       " WordList(['had', 'long', 'been']),\n",
       " WordList(['long', 'been', 'settled']),\n",
       " WordList(['been', 'settled', 'in']),\n",
       " WordList(['settled', 'in', 'sussex']),\n",
       " WordList(['in', 'sussex', 'their']),\n",
       " WordList(['sussex', 'their', 'estate']),\n",
       " WordList(['their', 'estate', 'wa']),\n",
       " WordList(['estate', 'wa', 'large']),\n",
       " WordList(['wa', 'large', 'and']),\n",
       " WordList(['large', 'and', 'their']),\n",
       " WordList(['and', 'their', 'residence']),\n",
       " WordList(['their', 'residence', 'wa']),\n",
       " WordList(['emma', 'woodhouse', 'handsome']),\n",
       " WordList(['woodhouse', 'handsome', 'clever'])]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "three_grams_list = []\n",
    "\n",
    "for novel in data:\n",
    "    # Get the first 20 words of the novel.\n",
    "    words = novel.split(maxsplit=20)\n",
    "    \n",
    "    # Delete the last entry of the list as it contains the rest of the novel's text.\n",
    "    del words[-1]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    # Join the lemmatized words back into text.\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    # Collect the n-grams and extend it to our list of n grams\n",
    "    three_grams = TextBlob(text).ngrams(n=3)\n",
    "    three_grams_list.extend(three_grams)\n",
    "\n",
    "three_grams_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to be', 436), ('of the', 431), ('in the', 359), ('it was', 281), ('of her', 277), ('to the', 242), ('mrs jennings', 237), ('to her', 231), ('i am', 224), ('she was', 209)]\n",
      "[('” “', 29), ('miss taylor', 23), ('mr knightley', 13), ('of her', 12), ('mr weston', 12), ('of the', 10), ('to have', 9), ('it was', 9), ('her father', 9), ('she had', 9)]\n",
      "[('my dear', 8), ('that he', 6), ('mr bennet', 6), ('you must', 5), ('of them', 5), ('it is', 4), ('do not', 4), ('how can', 4), ('will be', 4), ('of the', 3)]\n"
     ]
    }
   ],
   "source": [
    "for novel in data:\n",
    "    bigrams = TextBlob(novel).ngrams(n=2)\n",
    "    bigram_counter = Counter()\n",
    "    for bigram in bigrams:\n",
    "        # Join the bigram into a string as it is a WordList object.\n",
    "        bigram_text = ' '.join(bigram)\n",
    "        # Update the count.\n",
    "        bigram_counter.update([bigram_text])\n",
    "\n",
    "    print(bigram_counter.most_common(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the for loop outputs three lists of ten -- the top ten bigrams for each novel.  The output is a 'dictionary' type.  What if you wanted it as a simple list of bigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter i',\n",
       " 'i the',\n",
       " 'the family',\n",
       " 'family of',\n",
       " 'of dashwood',\n",
       " 'dashwood had',\n",
       " 'had long',\n",
       " 'long been',\n",
       " 'been settled',\n",
       " 'settled in',\n",
       " 'in sussex',\n",
       " 'sussex their',\n",
       " 'their estate',\n",
       " 'estate was',\n",
       " 'was large',\n",
       " 'large and',\n",
       " 'and their',\n",
       " 'their residence',\n",
       " 'residence was',\n",
       " 'was at']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_as_text = []\n",
    "\n",
    "for novel in data:\n",
    "    bigrams = TextBlob(novel).ngrams(n=2)\n",
    "    for bigram in bigrams:\n",
    "        bigram_text = ' '.join(bigram)\n",
    "        bigrams_as_text.append(bigram_text)\n",
    "\n",
    "bigrams_as_text[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the bigrams in one list, we can also count the overall top bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('to be', 448),\n",
       " ('of the', 444),\n",
       " ('in the', 369),\n",
       " ('of her', 291),\n",
       " ('it was', 290),\n",
       " ('to the', 244),\n",
       " ('mrs jennings', 237),\n",
       " ('to her', 234),\n",
       " ('i am', 234),\n",
       " ('she was', 213),\n",
       " ('of his', 209),\n",
       " ('i have', 203),\n",
       " ('it is', 194),\n",
       " ('she had', 193),\n",
       " ('could not', 167),\n",
       " ('on the', 161),\n",
       " ('have been', 161),\n",
       " ('in a', 161),\n",
       " ('and the', 160),\n",
       " ('at the', 160)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_count = Counter(bigrams_as_text)\n",
    "top_twenty_bigrams = bigram_count.most_common(20)\n",
    "top_twenty_bigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we only want the bigrams that include the word \"she\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('she was', 213),\n",
       " ('she had', 193),\n",
       " ('that she', 122),\n",
       " ('as she', 116),\n",
       " ('she could', 108)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "she_bigrams = []\n",
    "\n",
    "for bigram in bigrams_as_text:\n",
    "    if \"she\" in bigram: # notice the space after she.  It\n",
    "        she_bigrams.append(bigram)\n",
    "        \n",
    "Counter(she_bigrams).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, the above code won't work for 'he,' because it will pick up other words that contain 'he.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('of the', 444),\n",
       " ('in the', 369),\n",
       " ('of her', 291),\n",
       " ('to the', 244),\n",
       " ('to her', 234)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_bigrams = []\n",
    "\n",
    "for bigram in bigrams_as_text:\n",
    "    if \"he\" in bigram: # notice the space after she.  It\n",
    "        he_bigrams.append(bigram)\n",
    "        \n",
    "Counter(he_bigrams).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution is to use \"regular expressions,\" which are ways of coding the details of language.  You can communicate to the computer about such needs as detecting the beginning or end of a word by using two backslashes (an \"escape\" to tell the computer not to take the next letter literally) and \"b\" for \"boundary.\"  If you tell the computer to find a \"boundary\" in this way, it will look for both spaces and for the end of strings.\n",
    "\n",
    "Notice how I use two \"\\\\\\b\"'s below to tell the computer to look for the word \"he\" but not \"her\" or \"the.\" Python use the 're' package to detect regular expressions, and the .compile() and .match() commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern = re.compile(\"\\\\bhe\\\\b\") #  notice the .compile() and the \"escapes\"+b to signify \"word boundary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('he was', 126),\n",
       " ('he had', 113),\n",
       " ('he is', 75),\n",
       " ('he has', 49),\n",
       " ('he did', 37)]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_bigrams = []\n",
    "\n",
    "for bigram in bigrams_as_text:\n",
    "    if pattern.match(bigram): # notice the use of .match()\n",
    "        he_bigrams.append(bigram)\n",
    "        \n",
    "Counter(he_bigrams).most_common(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a for loop that looks for bigrams, trigrams, up to nine-grams, and counts how many there are in Jane Austen.  What are the longest phrases that are repeated more than 3 times across her corpus? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
