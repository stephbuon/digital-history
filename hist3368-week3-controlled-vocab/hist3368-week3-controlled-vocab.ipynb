{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For citation information, please see the \"Source Information\" section listed in the associated README file: https://github.com/stephbuon/digital-history/tree/master/hist3368-week3-controlled-vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3 Assignment: For Loops, Expert Vocabulary, and Revising With a Thesaurus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this week's assignment, you'll learn how to loop over lists of data.  You'll also start the process of thinking critically about which words matter to you for the purposes of text mining, and how to use a thesaurus and the powers of reason to expand your expert vocabulary and divide it into categories of information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterating over lists with for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[based on Lauren Klein's Lists and Loops https://github.com/laurenfklein/emory-qtm340/tree/0c3d0935ecd0a7920e331a8efd78240c49997606/notebooks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list comprehension syntax discussed earlier is very powerful: it allows you to succinctly transform one list into another list by thinking in terms of filtering and modification. But sometimes your primary goal isn't to make a new list, but simply to perform a set of operations on an existing list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say that you want to print every string in a list. Here's a short text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"it was the best of times, it was the worst of times\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make a list of all the words in the text by splitting on whitespace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can see what's in the list simply by evaluating the variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'best',\n",
       " 'of',\n",
       " 'times,',\n",
       " 'it',\n",
       " 'was',\n",
       " 'the',\n",
       " 'worst',\n",
       " 'of',\n",
       " 'times']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's say that we want to print out each word on a separate line, without any of Python's weird punctuation. In other words, I want the output to look like:\n",
    "\n",
    "\n",
    "    it\n",
    "    was\n",
    "    the\n",
    "    best\n",
    "    of\n",
    "    times,\n",
    "    it\n",
    "    was\n",
    "    the\n",
    "    worst\n",
    "    of\n",
    "    times\n",
    "\n",
    "But how can this be accomplished? We know that the print() function can display an individual string in this manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what we need, clearly, is a way to call the print() function with every item of the list. We could do this by writing a series of print() statements, one for every item in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "was\n",
      "the\n",
      "best\n",
      "of\n",
      "times,\n",
      "it\n",
      "was\n",
      "the\n",
      "worst\n",
      "of\n",
      "times\n"
     ]
    }
   ],
   "source": [
    "print(words[0])\n",
    "print(words[1])\n",
    "print(words[2])\n",
    "print(words[3])\n",
    "print(words[4])\n",
    "print(words[5])\n",
    "print(words[6])\n",
    "print(words[7])\n",
    "print(words[8])\n",
    "print(words[9])\n",
    "print(words[10])\n",
    "print(words[11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Nice, but there are some problems with this approach:\n",
    "\n",
    "- It's kind of verbose---we're doing exactly the same thing multiple times, only with slightly different expressions. Surely there's an easier way to tell the computer to do this?\n",
    "- It doesn't scale. What if we wrote a program that we want to produce hundreds or thousands of lines. Would we really need to write a print statement for each of those expressions?\n",
    "- It requires us to know how many items are going to end up in the list to begin with.\n",
    "\n",
    "Things are looking grim! But there's hope. Performing the same operation on all items of a list is an extremely common task in computer programming. So common, that Python has some built-in syntax to make the task easy: the for loop.\n",
    "\n",
    "Here's how a for loop looks:\n",
    "\n",
    "for tempvar in sourcelist:\n",
    "    statements\n",
    "\n",
    "The words for and in just have to be there---that's how Python knows it's a for loop. Here's what each of those parts mean.\n",
    "\n",
    "    tempvar: A name for a variable. Inside of the for loop, this variable will contain the current item of the list.\n",
    "    sourcelist: This can be any Python expression that evaluates to a list---a variable that contains a list, or a list slice, or even a list literal that you just type right in!\n",
    "    statements: One or more Python statements. Everything tabbed over underneath the for will be executed once for each item in the list. The statements tabbed over underneath the for line are called the body of the loop.\n",
    "\n",
    "Here's what the for loop for printing out every item in a list might look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it\n",
      "was\n",
      "the\n",
      "best\n",
      "of\n",
      "times,\n",
      "it\n",
      "was\n",
      "the\n",
      "worst\n",
      "of\n",
      "times\n"
     ]
    }
   ],
   "source": [
    "for item in words:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable name item is arbitrary. You can pick whatever variable name you like, as long as you're consistent about using the same variable name in the body of the loop. If you wrote out this loop in a long-hand fashion, it might look like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    item = words[0]\n",
    "    print(item)\n",
    "    item = words[1]\n",
    "    print(item)\n",
    "    item = words[2]\n",
    "    print(item)\n",
    "    item = words[3]\n",
    "    print(item)\n",
    "    # etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "    it\n",
    "    was\n",
    "    the\n",
    "    best\n",
    "    \n",
    "Of course, the body of the loop can have more than one statement, and you can assign values to variables inside the loop:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT\n",
      "WAS\n",
      "THE\n",
      "BEST\n",
      "OF\n",
      "TIMES,\n",
      "IT\n",
      "WAS\n",
      "THE\n",
      "WORST\n",
      "OF\n",
      "TIMES\n"
     ]
    }
   ],
   "source": [
    "for item in words:\n",
    "    yelling = item.upper()\n",
    "    print(yelling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also include other kinds of nested statements inside the for loop, like if/else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IT\n",
      "   was\n",
      "   the\n",
      "best\n",
      "OF\n",
      "times,\n",
      "IT\n",
      "   was\n",
      "   the\n",
      "worst\n",
      "OF\n",
      "times\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for item in words:\n",
    "    if len(item) == 2:\n",
    "        print(item.upper())\n",
    "    elif len(item) == 3:\n",
    "        print(\"   \" + item)\n",
    "    else:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This structure is called a \"loop\" because when Python reaches the end of the statements in the body, it \"loops\" back to the beginning of the body, and executes the same statements again (this time with the next item in the list).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python programmers tend to use for loops most often when the problem would otherwise be too tricky or complicated to solve using a list comprehension. It's easy to paraphrase any list comprehension in for loop syntax. For example, this list comprehension, which evaluates to a list of the squares of even integers from 1 to 25:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x * x for x in range(1, 26) if x % 2 == 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can rewrite this list comprehesion as a for loop by starting out with an empty list, then appending an item to the list inside the loop. The source list remains the same:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 16, 36, 64, 100, 144, 196, 256, 324, 400, 484, 576]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = []\n",
    "for x in range(1, 26):\n",
    "    if x % 2 == 0:\n",
    "        result.append(x * x)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join: Making strings from lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've created a list of words, it's a common task to want to take that list and \"glue\" it back together, so it's a single string again, instead of a list. So, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hydrogen, and helium, and lithium, and beryllium, and boron'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "element_list = [\"hydrogen\", \"helium\", \"lithium\", \"beryllium\", \"boron\"]\n",
    "glue = \", and \"\n",
    "glue.join(element_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .join() method needs a \"glue\" string to the left of it---this is the string that will be placed in between the list elements. In the parentheses to the right, you need to put an expression that evaluates to a list. Very frequently with .join(), programmers don't bother to assign the \"glue\" string to a variable first, so you end up with code that looks like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [\"this\", \"is\", \"a\", \"test\"]\n",
    "\" \".join(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "When we're working with .split() and .join(), our workflow usually looks something like this:\n",
    "\n",
    "    Split a string to get a list of units (usually words).\n",
    "    Use some of the list operations discussed above to modify or slice the list.\n",
    "    Join that list back together into a string.\n",
    "    Do something with that string (e.g., print it out).\n",
    "\n",
    "With this in mind, here's a program that splits a string into words, randomizes the order of the words, then prints out the results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this a test is'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to make this block work:\n",
    "\n",
    "# add `import random`, the module `shuffle()` belongs to. \n",
    "\n",
    "# remove `split()` bc the `shuffle()` method only works on lists, not string objects (and `split()` transforms items to string objects)\n",
    "\n",
    "# if you want to keep demonstrating `.split()` with `shuffle()` you could transform the str objects to lists, but that step might be hard to follow logically \n",
    "\n",
    "# alterantively you could use `sort()` instead of suffle (see below)\n",
    "\n",
    "import random\n",
    "\n",
    "text = \"it was a dark and stormy night\"\n",
    "# words = text.split() \n",
    "random.shuffle(words)\n",
    "' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "and\n",
      "dark\n",
      "it\n",
      "night\n",
      "stormy\n",
      "was\n"
     ]
    }
   ],
   "source": [
    "# sort option w str split\n",
    "\n",
    "text = \"it was a dark and stormy night\"\n",
    "words = text.split()\n",
    "words.sort()\n",
    "for word in words:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXERCISE: Write a Python command-line program that prints out the lines of a text file in random order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested For Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, I want to use multiple for loops to do my business.  This usually happens when data is 'structured,' that is, when the data exists in multiple separate lists, dictionaries, or dataframes, to each of which we want to apply separate conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make a set of lists.  Each list contains a set of strings that correspond to the names of novels written by three novelists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dickens = ['oliver twist', 'bleak house', 'a tale of two cities']\n",
    "austen = ['sense and sensibility', 'emma', 'pride and prejudice']\n",
    "trollope = ['doctor thorne', 'barchester towers', 'the land leaguers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "austen[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a list of the novelists' names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "novelists = ['dickens', 'austen', 'trollope']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, I can now call up novels by the strings in the variable 'novelists.'  Here are two ways of getting Dickens' novels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oliver twist', 'bleak house', 'a tale of two cities']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()['dickens'] # this looks for variables called 'dickens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['oliver twist', 'bleak house', 'a tale of two cities']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[novelists[0]] # this looks for variables that correspond to the first item in the list, 'novelists'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's put that into a for loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dickens\n",
      "['oliver twist', 'bleak house', 'a tale of two cities']\n",
      "austen\n",
      "['sense and sensibility', 'emma', 'pride and prejudice']\n",
      "trollope\n",
      "['doctor thorne', 'barchester towers', 'the land leaguers']\n"
     ]
    }
   ],
   "source": [
    "for novelist in novelists: # cycle through each novelist\n",
    "    their_novels = globals()[novelist] # for each novelist, pull up the list that corresponds to their name -- thus for 'dickens,' call up the variable called 'dickens'\n",
    "    print(novelist)\n",
    "    print(their_novels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's nicely formatted output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But say we want to do something which each of the novel names -- like creating a new dataset where each novel name is accurately annotate it with the name of its author.  How do I glue them together, when what I want to glue changes for each novel but also for each novelist? \n",
    "\n",
    "To do that, we'll need a *double* for loop, or a \"nested\" for loop.  \n",
    "\n",
    "The outside for loop cycles through each novelist and calls up their list of novels in the variable 'their_novels'.\n",
    "\n",
    "The inner for loop cycles through each of the items in 'their_novels.\"\n",
    "\n",
    "I can use these nested for loops to output a really nicely formatted list of authors and novels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oliver twist\n",
      "dickens\n",
      "\n",
      "bleak house\n",
      "dickens\n",
      "\n",
      "a tale of two cities\n",
      "dickens\n",
      "\n",
      "------------------\n",
      "sense and sensibility\n",
      "austen\n",
      "\n",
      "emma\n",
      "austen\n",
      "\n",
      "pride and prejudice\n",
      "austen\n",
      "\n",
      "------------------\n",
      "doctor thorne\n",
      "trollope\n",
      "\n",
      "barchester towers\n",
      "trollope\n",
      "\n",
      "the land leaguers\n",
      "trollope\n",
      "\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for novelist in novelists: # for each novelist,\n",
    "    their_novels = globals()[novelist] \n",
    "    for novel in their_novels: # cycle through each of the novels for that novelist. for each novel of each novelist:\n",
    "        print(novel)\n",
    "        print(novelist)\n",
    "        print(\"\")\n",
    "    print(\"------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a nicely formatted print-out.  \n",
    "\n",
    "But what I really want is a dataset where every entry is the name of a novelist and the novel they wrote.  Can I tweak the double for loop to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dickens-oliver twist',\n",
       " 'dickens-bleak house',\n",
       " 'dickens-a tale of two cities',\n",
       " 'austen-sense and sensibility',\n",
       " 'austen-emma',\n",
       " 'austen-pride and prejudice',\n",
       " 'trollope-doctor thorne',\n",
       " 'trollope-barchester towers',\n",
       " 'trollope-the land leaguers']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novels_with_authors = []\n",
    "\n",
    "for novelist in novelists: # for each novelist,\n",
    "    their_novels = globals()[novelist] \n",
    "    for novel in their_novels: # cycle through each of the novels for that novelist. for each novel of each novelist:\n",
    "        new_entry = novelist + \"-\" + novel # create a dummy variable, 'new_entry', which lists the novelist and novel\n",
    "        novels_with_authors.append(new_entry) # add the dummy variable to my master list, novels_with_authors\n",
    "        \n",
    "novels_with_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I produced this output -- and the output above -- with a \"double for loop\" or a \"nested for loop.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first \"for loop\" iterates through the novelists, one at a time:\n",
    "\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"for novelist in novelists:\"\n",
    "\n",
    "\n",
    "The second \"for loop\" takes each novelist, and iterates through the list of their novels:\n",
    "       \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; \"for novel in their_novels\"\n",
    "\n",
    "\n",
    "Because the loops are nested, I'm not randomly applying Trollope or Austen's names to random novel titles; I'm creating a list where each author's name corresponds to the right novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dickens-oliver twist',\n",
       " 'dickens-bleak house',\n",
       " 'dickens-a tale of two cities',\n",
       " 'austen-sense and sensibility',\n",
       " 'austen-emma',\n",
       " 'austen-pride and prejudice',\n",
       " 'trollope-doctor thorne',\n",
       " 'trollope-barchester towers',\n",
       " 'trollope-the land leaguers']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "novels_with_authors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with a Controlled Vocabulary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by tutorials by Paige McKenzie - https://p-mckenzie.github.io/2018/01/11/Jane-Austen/\n",
    "William Scott - https://towardsdatascience.com/tf-idf-for-document-ranking-from-scratch-in-python-on-real-world-dataset-796d339a4089"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of this notebook, we'll be working with a 'controlled vocabulary,' which is to say, expert-defined words that help to limit our pursuit of wordcount to words that share a certain semantic valence.  Controlled Vocabularies have been used in digital history to examine the history of words used by Victorian people to describe the way that strangers walked down the street, and to show that novelists in the nineteenth century described the urban landscape with increasing detail.  \n",
    "\n",
    "First, we'll download some novels by Jane Austen to try our vocabulary on.  Then, we'll talk about how to clean the text using stemming and lemmatization.  \n",
    "\n",
    "Next, we'll use a controlled vocabulary to limit the count to words that are interesting to us.  Then, we'll expand that controlled vocabulary using the 'hyponym' feature of the WordNet package, which consults with dictionaries of the English language organized by linguists at Princeton.  \n",
    "\n",
    "Finally, we'll visualize our findings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download some Jane Austen Novels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, numpy, re, matplotlib# , num2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/group/history/hist_3368-jguldi\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/group/history/hist_3368-jguldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download some data\n",
    "\n",
    "with open('senseandsensibility.txt', 'r') as myfile:\n",
    "    sas_data = myfile.read().split('\\n\\n\"I suppose you know, ma\\'am, that Mr. Ferrars is married\"\\n\\nIt _was_ Edward\\n\\n\"Everything in such respectable condition\"\\n\\n ')[1].split('THE END')[0].strip()\n",
    "\n",
    "with open('emma.txt', 'r') as myfile:\n",
    "    emma_data = myfile.read().split('CHAPTER I')[1].split('FINIS')[0].strip()\n",
    "\n",
    "with open('prideandprejudice.txt', 'r') as myfile:\n",
    "    pap_data = myfile.read().split('CHAPTER I')[1].split('End of the Project Gutenberg EBook of Pride and Prejudice, by Jane Austen')[0].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that your data matches what you think it should."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*       *       *       *       *\\n\\n\\n\\n\\nCHAPTER I\\n\\n\\nThe family of Dashwood had long been settled in Sussex. Their estate\\nwas large, and their residence was at Norland Park, in the centre of\\ntheir property, where, for many generations, they had lived in so\\nrespectable a manner as to engage the general good opinion of their\\nsurrounding acquaintance. The late owner of this estate was a single\\nman, who lived to a very advanced age, and who for many years of his\\nlife, had a constant companion and housekeeper in his sister. But her\\ndeath, which happened ten years before his own, produced a great\\nalteration in his home; for to supply her loss, he invited and\\nreceived into his house the family of his nephew Mr. Henry Dashwood,\\nthe legal inheritor of the Norland estate, and the person to whom he\\nintended to bequeath it. In the society of his nephew and niece, and\\ntheir children, the old Gentleman's days were comfortably spent. His\\nattachment to them all increased. The constant attention of Mr. and\\nMrs. Henry Dashwood to his wishes, which proceeded not merely from\\ninterest, but from goodness of heart, gave him every degree of solid\\ncomfort which his age could receive; and the cheerfulness of the\\nchildren added a relish to his existence.\\n\\nBy a former marriage, Mr. Henry Dashwood had one son: by his present\\nlady, three daughters. The son, a steady respectable young man, was\\namply provided for by the fortune of his mother, which had been large,\\nand half of which devolved on him on his coming of age. By his own\\nmarriage, likewise, which happened soon afterwards, he added to his\\nwealth. To him therefore the succession to the Norland estate was not\\nso really important as to his sisters; for their fortune, independent\\nof what might arise to them from their father's inheriting that\\nproperty, could be but small. Their mother had nothing, and their\\nfather only seven thousand pounds in his own disposal; for the\\nremaining moiety of his first wife's fortune was also secured to her\\nchild, an\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing only first 2000 characters.\n",
    "sas_data[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good!\n",
    "\n",
    "Isn't it getting tired, retyping the same command for each novel? Let's throw them all into one data set so we can loop through them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*       *       *       *       *\\n\\n\\n\\n\\nCHAPTER I\\n\\n\\nThe family of Dashwood had long been settled in Sussex. Their estate\\nwas large, and their residence was at Norland Park, in the centre of\\ntheir property, where, for many generations, they had lived in so\\nrespectable a manner as to engage the general good opinion of their\\nsurrounding acquaintance. The late owner of this estate was a single\\nman, who lived to a very advanced age, and who for many years of his\\nlife, had a constant companion and housekeeper in his sister. But her\\ndeath, which happened ten years before his own, produced a great\\nalteration in his home; for to supply her loss, he invited and\\nreceived into his house the family of his nephew Mr. Henry Dashwood,\\nthe legal inheritor of the Norland estate, and the person to whom he\\nintended to bequeath it. In the society of his nephew and niece, and\\ntheir children, the old Gentleman's days were comfortably spent. His\\nattachment to them all increased. The constant attention of Mr. and\\nMrs. Henry Dashwood to his wishes, which proceeded not merely from\\ninterest, but from goodness of heart, gave him every degree of solid\\ncomfort which his age could receive; and the cheerfulness of the\\nchildren added a relish to his existence.\\n\\nBy a former marriage, Mr. Henry Dashwood had one son: by his present\\nlady, three daughters. The son, a steady respectable young man, was\\namply provided for by the fortune of his mother, which had been large,\\nand half of which devolved on him on his coming of age. By his own\\nmarriage, likewise, which happened soon afterwards, he added to his\\nwealth. To him therefore the succession to the Norland estate was not\\nso really important as to his sisters; for their fortune, independent\\nof what might arise to them from their father's inheriting that\\nproperty, could be but small. Their mother had nothing, and their\\nfather only seven thousand pounds in his own disposal; for the\\nremaining moiety of his first wife's fortune was also secured to her\\nchild, an\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = [sas_data, emma_data, pap_data]\n",
    "data[0][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There still appear to be some errors where spaces have been replaced by \"\\n\".  We'll get rid of those in a second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"*       *       *       *       *     CHAPTER I   The family of Dashwood had long been settled in Sussex. Their estate was large, and their residence was at Norland Park, in the centre of their property, where, for many generations, they had lived in so respectable a manner as to engage the general good opinion of their surrounding acquaintance. The late owner of this estate was a single man, who lived to a very advanced age, and who for many years of his life, had a constant companion and housekeeper in his sister. But her death, which happened ten years before his own, produced a great alteration in his home; for to supply her loss, he invited and received into his house the family of his nephew Mr. Henry Dashwood, the legal inheritor of the Norland estate, and the person to whom he intended to bequeath it. In the society of his nephew and niece, and their children, the old Gentleman's days were comfortably spent. His attachment to them all increased. The constant attention of Mr. and Mrs. Henry Dashwood to his wishes, which proceeded not merely from interest, but from goodness of heart, gave him every degree of solid comfort which his age could receive; and the cheerfulness of the children added a relish to his existence.  By a former marriage, Mr. Henry Dashwood had one son: by his present lady, three daughters. The son, a steady respectable young man, was amply provided for by the fortune of his mother, which had been large, and half of which devolved on him on his coming of age. By his own marriage, likewise, which happened soon afterwards, he added to his wealth. To him therefore the succession to the Norland estate was not so really important as to his sisters; for their fortune, independent of what might arise to them from their father's inheriting that property, could be but small. Their mother had nothing, and their father only seven thousand pounds in his own disposal; for the remaining moiety of his first wife's fortune was also secured to her child, an\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    data[i] = data[i].replace('\\n', ' ')\n",
    "data[0][:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's split the text into words and print the first word of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['*', '*', '*', '*', '*', 'CHAPTER', 'I', 'The', 'family', 'of', 'Dashwood', 'had', 'long', 'been', 'settled', 'in', 'Sussex.', 'Their', 'estate', 'was']\n",
      "['Emma', 'Woodhouse,', 'handsome,', 'clever,', 'and', 'rich,', 'with', 'a', 'comfortable', 'home', 'and', 'happy', 'disposition,', 'seemed', 'to', 'unite', 'some', 'of', 'the', 'best']\n",
      "['.', 'It', 'is', 'a', 'truth', 'universally', 'acknowledged,', 'that', 'a', 'single', 'man', 'in', 'possession', 'of', 'a', 'good', 'fortune,', 'must', 'be', 'in']\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    print(words[:20]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning the Novels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's lowercase the text and get rid of punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for i in range(len(data)):\n",
    "    # data[i] is the current novel\n",
    "    data[i] = data[i].lower() # force to lowercase\n",
    "    data[i] = re.sub('[\\\",.;:?([)\\]_*]', '', data[i]) # remove punctuation and special characters with regular expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is the process of removing suffices, like \"ed\" or \"ing\".\n",
    "\n",
    "We will use another standard NLTK package, PorterStemmer, to do the stemming.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'i',\n",
       " 'the',\n",
       " 'famili',\n",
       " 'of',\n",
       " 'dashwood',\n",
       " 'had',\n",
       " 'long',\n",
       " 'been',\n",
       " 'settl',\n",
       " 'in',\n",
       " 'sussex',\n",
       " 'their',\n",
       " 'estat',\n",
       " 'wa',\n",
       " 'larg',\n",
       " 'and',\n",
       " 'their',\n",
       " 'resid',\n",
       " 'wa']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "st = PorterStemmer()\n",
    "\n",
    "stemmed_list = []\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    for word in words:\n",
    "        stemmed = st.stem(word)\n",
    "        stemmed_list.append(stemmed)\n",
    "        \n",
    "stemmed_list[:20] # i have changed this so you print just the first words\n",
    "# printing all the words is actually way more computer intensive than it may seem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, \"universal\" becomes \"univers\" (which means that \"universally\" will be counted with \"universal\" and \"universe\") and \"single\" becomes \"singl\" (which means it would be counted with \"singled\").  But \"acknowledged\" has been left as it is.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick up another term -- lemmatization -- which is extremely memory intensive, but far more accurate.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aardwolf'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "wn.morphy('aardwolves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'i',\n",
       " 'family',\n",
       " 'have',\n",
       " 'long',\n",
       " 'be',\n",
       " 'settle',\n",
       " 'in',\n",
       " 'sussex',\n",
       " 'estate',\n",
       " 'wa',\n",
       " 'large',\n",
       " 'residence',\n",
       " 'wa',\n",
       " 'at',\n",
       " 'park',\n",
       " 'in',\n",
       " 'centre',\n",
       " 'property',\n",
       " 'many']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma_list = []\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    for word in words:\n",
    "        lemma = wn.morphy(word)\n",
    "        if not lemma:\n",
    "            # word is not a valid english word so skip it\n",
    "            continue\n",
    "        lemma_list.append(lemma)\n",
    "\n",
    "lemma_list[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization is often a more useful approach than stemming because it leverages an understanding of the word itself to convert the word back to its root word. \"Acknowledged\"  becomes \"acknowledge,\" and \"daughters\" becomes \"daughter.\"  \n",
    "\n",
    "Note some important oddities -- words such as \"that\" are replaced by \"None,\" so if we count lemmas to graph them we will want to eliminate this noise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming and lemmatization are important because they matter for how we count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 4214), ('the', 4191), ('of', 3692), ('and', 3543), ('her', 2598), ('a', 2161), ('i', 2014), ('in', 1992), ('wa', 1896), ('it', 1896), ('she', 1629), ('be', 1501), ('that', 1403), ('for', 1282), ('not', 1281), ('as', 1247), ('you', 1239), ('he', 1125), ('hi', 1048), ('had', 1032), ('with', 1010), ('have', 920), ('but', 862), ('at', 848), ('is', 780), ('by', 761), ('mr', 758), ('on', 703), ('all', 674), ('so', 661), ('him', 649), ('my', 638), ('elinor', 614), ('which', 600), ('could', 588), ('no', 570), ('from', 554), ('would', 527), ('veri', 525), ('they', 524), ('their', 506), ('mariann', 486), ('them', 484), ('been', 454), ('were', 451), ('what', 443), ('thi', 442), ('me', 429), ('more', 414), ('ani', 409), ('your', 407), ('said', 393), ('everi', 388), ('will', 385), ('such', 373), ('than', 372), ('do', 368), ('or', 360), ('an', 347), ('one', 333), ('when', 317), ('must', 305), ('if', 303), ('much', 301), ('onli', 299), ('own', 284), ('know', 282), ('who', 276), ('time', 264), ('sister', 262), ('herself', 257), ('dashwood', 255), ('did', 250), ('other', 249), ('think', 249), ('are', 247), ('am', 246), ('now', 241), ('miss', 241), ('how', 240), ('should', 240), ('there', 236), ('we', 229), ('see', 225), ('some', 222), ('ha', 222), ('though', 220), ('might', 218), ('say', 214), ('can', 212), ('thing', 211), ('well', 210), ('after', 207), ('edward', 207), ('mother', 205), ('befor', 201), ('jen', 200), ('never', 197), ('day', 185), ('noth', 182)]\n"
     ]
    }
   ],
   "source": [
    "count = Counter(stemmed_list)\n",
    "print(count.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting Words and N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the word counts look like now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4092), ('to', 4090), ('of', 3573), ('and', 3419), ('her', 2522), ('a', 2048), ('i', 1948), ('in', 1937), ('was', 1848), ('it', 1701)]\n",
      "[('and', 107), ('to', 102), ('a', 92), ('of', 90), ('the', 81), ('her', 61), ('i', 49), ('you', 48), ('it', 45), ('in', 43)]\n",
      "[('you', 31), ('of', 29), ('to', 22), ('a', 21), ('the', 18), ('and', 17), ('i', 17), ('that', 15), ('it', 14), ('is', 14)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "for novel in data:\n",
    "    words = novel.split()\n",
    "    count = Counter(words)\n",
    "    print(count.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes we want to look for multi-word phrases instead of individual words.  For example, if we're researching the living spaces of Jane Austen's England, we definitely want to know whether she refers to \"dining rooms\" or \"bed-rooms\" (which our punctuation clean-up might have turned into separate words, depending on what we did)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WordList(['chapter', 'i', 'the']),\n",
       " WordList(['i', 'the', 'family']),\n",
       " WordList(['the', 'family', 'of']),\n",
       " WordList(['family', 'of', 'dashwood']),\n",
       " WordList(['of', 'dashwood', 'had']),\n",
       " WordList(['dashwood', 'had', 'long']),\n",
       " WordList(['had', 'long', 'been']),\n",
       " WordList(['long', 'been', 'settled']),\n",
       " WordList(['been', 'settled', 'in']),\n",
       " WordList(['settled', 'in', 'sussex']),\n",
       " WordList(['in', 'sussex', 'their']),\n",
       " WordList(['sussex', 'their', 'estate']),\n",
       " WordList(['their', 'estate', 'wa']),\n",
       " WordList(['estate', 'wa', 'large']),\n",
       " WordList(['wa', 'large', 'and']),\n",
       " WordList(['large', 'and', 'their']),\n",
       " WordList(['and', 'their', 'residence']),\n",
       " WordList(['their', 'residence', 'wa']),\n",
       " WordList(['emma', 'woodhouse', 'handsome']),\n",
       " WordList(['woodhouse', 'handsome', 'clever'])]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "three_grams_list = []\n",
    "\n",
    "for novel in data:\n",
    "    # Get the first 20 words of the novel.\n",
    "    words = novel.split(maxsplit=20)\n",
    "    \n",
    "    # Delete the last entry of the list as it contains the rest of the novel's text.\n",
    "    del words[-1]\n",
    "    \n",
    "    # Lemmatize\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        lemmatized_words.append(lemmatizer.lemmatize(word))\n",
    "    \n",
    "    # Join the lemmatized words back into text.\n",
    "    text = ' '.join(lemmatized_words)\n",
    "    \n",
    "    # Collect the n-grams and extend it to our list of n grams\n",
    "    three_grams = TextBlob(text).ngrams(n=3)\n",
    "    three_grams_list.extend(three_grams)\n",
    "\n",
    "three_grams_list[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to be', 436), ('of the', 431), ('in the', 359), ('it was', 281), ('of her', 277), ('to the', 242), ('mrs jennings', 237), ('to her', 231), ('i am', 224), ('she was', 209)]\n",
      "[('” “', 29), ('miss taylor', 23), ('mr knightley', 13), ('of her', 12), ('mr weston', 12), ('of the', 10), ('to have', 9), ('it was', 9), ('her father', 9), ('she had', 9)]\n",
      "[('my dear', 8), ('that he', 6), ('mr bennet', 6), ('you must', 5), ('of them', 5), ('it is', 4), ('do not', 4), ('how can', 4), ('will be', 4), ('of the', 3)]\n"
     ]
    }
   ],
   "source": [
    "for novel in data:\n",
    "    bigrams = TextBlob(novel).ngrams(n=2)\n",
    "    bigram_counter = Counter()\n",
    "    \n",
    "    for bigram in bigrams:\n",
    "        # Join the bigram into a string as it is a WordList object.\n",
    "        bigram_text = ' '.join(bigram)\n",
    "        # Update the count.\n",
    "        bigram_counter.update([bigram_text])\n",
    "\n",
    "    print(bigram_counter.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Controlled Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look for what scholars call a \"controlled vocabulary\" -- a list of words that we know to be meaningful. For right now, let's pretend that we're researching the buildings, landscape, and furniture of nineteenth-century England.  I'm curious about what kinds of spaces are described in Austen, and I'd like to begin by counting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "controlled_vocab = [\n",
    "    \"garden\",\n",
    "    \"room\", \n",
    "    \"estate\",\n",
    "    \"manor\", \n",
    "    \"hedge\", \n",
    "    \"residence\",\n",
    "    \"park\",\n",
    "    \"lane\",\n",
    "    \"chair\",\n",
    "    \"sofa\",\n",
    "    \"settee\",\n",
    "    \"bed\",\n",
    "    \"bedroom\",\n",
    "    \"chaise\",\n",
    "    \"table\",\n",
    "    \"rug\",\n",
    "    \"carpet\",\n",
    "    \"candelabra\",\n",
    "    \"shed\",\n",
    "    \"cottage\",\n",
    "    \"fence\",\n",
    "    \"turret\",\n",
    "    \"castle\",\n",
    "    \"palace\",\n",
    "    \"hut\",\n",
    "    \"dwelling\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'estate': 19,\n",
       "         'residence': 7,\n",
       "         'park': 51,\n",
       "         'dwelling': 6,\n",
       "         'room': 97,\n",
       "         'cottage': 56,\n",
       "         'garden': 11,\n",
       "         'shed': 3,\n",
       "         'table': 23,\n",
       "         'manor': 1,\n",
       "         'chair': 9,\n",
       "         'bed': 25,\n",
       "         'lane': 3,\n",
       "         'chaise': 6,\n",
       "         'rug': 1,\n",
       "         'bedroom': 1,\n",
       "         'sofa': 1})"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlled_words = []\n",
    "\n",
    "\n",
    "words = data[0].split()\n",
    "\n",
    "for w in words:\n",
    "    if w in controlled_vocab:\n",
    "        controlled_words.append(w)\n",
    "\n",
    "Counter(controlled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not a very good return.  It also occurs to me that I might not be thinking clearly about all the kinds of furniture, buildings, and other structures that might make up the Georgian landscape.  Fortunately, linguists have compiled many dictionaries that can help us to navigate the semantic universe with greater position.  One of these dictionaries is \"Wordnet,\" the fruit of a long-term research undertaking at Princeton. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expanded Controlled Vocabulary with Wordnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'get_synsets' command in Wordnet unlocks the thesaurus/dictionary in its full potential.  We won't go into the full power of the \"synsets,\" but suffice it to say that Wordnet knows that a \"house\" when used as a noun can mean a \"firm,\" a \"sign of the zodiac,\" a \"family,\" or a \"theater.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('house.n.01'), Synset('firm.n.01'), Synset('house.n.03'), Synset('house.n.04'), Synset('house.n.05'), Synset('house.n.06'), Synset('house.n.07'), Synset('sign_of_the_zodiac.n.01'), Synset('house.n.09'), Synset('family.n.01'), Synset('theater.n.01'), Synset('house.n.12')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import Word\n",
    "\n",
    "from textblob.wordnet import NOUN\n",
    "\n",
    "w1 = Word(\"house\")\n",
    "w1.synsets\n",
    "syns = w1.get_synsets(pos=NOUN)\n",
    "print(syns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, wordnet knows that the word \"building\" can refer to different kinds of construction (as a noun), but it can also be a verb form used with many different senses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('building.n.01'),\n",
       " Synset('construction.n.01'),\n",
       " Synset('construction.n.07'),\n",
       " Synset('building.n.04'),\n",
       " Synset('construct.v.01'),\n",
       " Synset('build_up.v.02'),\n",
       " Synset('build.v.03'),\n",
       " Synset('build.v.04'),\n",
       " Synset('build.v.05'),\n",
       " Synset('build.v.06'),\n",
       " Synset('build.v.07'),\n",
       " Synset('build.v.08'),\n",
       " Synset('build_up.v.04'),\n",
       " Synset('build.v.10')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('building')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *hyponym* is a word that is a more specific version of another word.  So if we want to know the many different types of houses in the dictionary, we can use wordnet's .hyponyms() command to navigate these lists, and we can generate another controlled vocabulary from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('beach_house.n.01'),\n",
       " Synset('boarding_house.n.01'),\n",
       " Synset('bungalow.n.01'),\n",
       " Synset('cabin.n.02'),\n",
       " Synset('chalet.n.01'),\n",
       " Synset('chapterhouse.n.02'),\n",
       " Synset('country_house.n.01'),\n",
       " Synset('detached_house.n.01'),\n",
       " Synset('dollhouse.n.01'),\n",
       " Synset('duplex_house.n.01'),\n",
       " Synset('farmhouse.n.01'),\n",
       " Synset('gatehouse.n.01'),\n",
       " Synset('guesthouse.n.01'),\n",
       " Synset('hacienda.n.02'),\n",
       " Synset('lodge.n.04'),\n",
       " Synset('lodging_house.n.01'),\n",
       " Synset('maisonette.n.02'),\n",
       " Synset('mansion.n.02'),\n",
       " Synset('ranch_house.n.01'),\n",
       " Synset('residence.n.02'),\n",
       " Synset('row_house.n.01'),\n",
       " Synset('safe_house.n.01'),\n",
       " Synset('saltbox.n.01'),\n",
       " Synset('sod_house.n.01'),\n",
       " Synset('solar_house.n.01'),\n",
       " Synset('tract_house.n.01'),\n",
       " Synset('villa.n.02')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synlist = wn.synset('house.n.01').hyponyms()\n",
    "synlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wordnet's 'lemmas()' function gives us access to the base lemma associated with any of these categories.  Let's use the 'append' function and the 'lemmas' function to create a vocabulary list stripped of the Wordnet apparatus.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['beach_house', 'boarding_house', 'boardinghouse', 'bungalow', 'cottage', 'cabin', 'chalet', 'chapterhouse', 'fraternity_house', 'frat_house', 'country_house', 'detached_house', 'single_dwelling', 'dollhouse', \"doll's_house\", 'duplex_house', 'duplex', 'semidetached_house', 'farmhouse', 'gatehouse', 'guesthouse', 'hacienda', 'lodge', 'hunting_lodge', 'lodging_house', 'rooming_house', 'maisonette', 'maisonnette', 'mansion', 'mansion_house', 'manse', 'hall', 'residence', 'ranch_house', 'residence', 'row_house', 'town_house', 'safe_house', 'saltbox', 'sod_house', 'soddy', 'adobe_house', 'solar_house', 'tract_house', 'villa']\n"
     ]
    }
   ],
   "source": [
    "new_vocab = []\n",
    "\n",
    "for syn in synlist:\n",
    "    for lemma in syn.lemmas():\n",
    "        new_vocab.append(str(lemma.name()))\n",
    "        \n",
    "print(new_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bear in mind: we don't have to stop here.  We can keep drilling down within each of these catergories to get an even finer-grain list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Lemma('beach_house.n.01.beach_house')]\n",
      "[Lemma('boarding_house.n.01.boarding_house'), Lemma('boarding_house.n.01.boardinghouse')]\n",
      "[Lemma('bungalow.n.01.bungalow'), Lemma('bungalow.n.01.cottage')]\n",
      "[Lemma('cabin.n.02.cabin')]\n",
      "[Lemma('chalet.n.01.chalet')]\n",
      "[Lemma('chapterhouse.n.02.chapterhouse'), Lemma('chapterhouse.n.02.fraternity_house'), Lemma('chapterhouse.n.02.frat_house')]\n",
      "[Lemma('country_house.n.01.country_house')]\n",
      "[Lemma('detached_house.n.01.detached_house'), Lemma('detached_house.n.01.single_dwelling')]\n",
      "[Lemma('dollhouse.n.01.dollhouse'), Lemma('dollhouse.n.01.doll's_house')]\n",
      "[Lemma('duplex_house.n.01.duplex_house'), Lemma('duplex_house.n.01.duplex'), Lemma('duplex_house.n.01.semidetached_house')]\n",
      "[Lemma('farmhouse.n.01.farmhouse')]\n",
      "[Lemma('gatehouse.n.01.gatehouse')]\n",
      "[Lemma('guesthouse.n.01.guesthouse')]\n",
      "[Lemma('hacienda.n.02.hacienda')]\n",
      "[Lemma('lodge.n.04.lodge'), Lemma('lodge.n.04.hunting_lodge')]\n",
      "[Lemma('lodging_house.n.01.lodging_house'), Lemma('lodging_house.n.01.rooming_house')]\n",
      "[Lemma('maisonette.n.02.maisonette'), Lemma('maisonette.n.02.maisonnette')]\n",
      "[Lemma('mansion.n.02.mansion'), Lemma('mansion.n.02.mansion_house'), Lemma('mansion.n.02.manse'), Lemma('mansion.n.02.hall'), Lemma('mansion.n.02.residence')]\n",
      "[Lemma('ranch_house.n.01.ranch_house')]\n",
      "[Lemma('residence.n.02.residence')]\n",
      "[Lemma('row_house.n.01.row_house'), Lemma('row_house.n.01.town_house')]\n",
      "[Lemma('safe_house.n.01.safe_house')]\n",
      "[Lemma('saltbox.n.01.saltbox')]\n",
      "[Lemma('sod_house.n.01.sod_house'), Lemma('sod_house.n.01.soddy'), Lemma('sod_house.n.01.adobe_house')]\n",
      "[Lemma('solar_house.n.01.solar_house')]\n",
      "[Lemma('tract_house.n.01.tract_house')]\n",
      "[Lemma('villa.n.02.villa')]\n"
     ]
    }
   ],
   "source": [
    "for syn in synlist:\n",
    "    print(syn.lemmas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('bed_and_breakfast.n.01'), Synset('log_cabin.n.01'), Synset('chateau.n.01'), Synset('dacha.n.01'), Synset('shooting_lodge.n.01'), Synset('summer_house.n.01'), Synset('villa.n.03'), Synset('villa.n.04'), Synset('lodge.n.03'), Synset('flophouse.n.01'), Synset('manor.n.01'), Synset('palace.n.01'), Synset('stately_home.n.01'), Synset('court.n.09'), Synset('deanery.n.01'), Synset('manse.n.02'), Synset('palace.n.04'), Synset('parsonage.n.01'), Synset('religious_residence.n.01'), Synset('brownstone.n.02'), Synset('terraced_house.n.01')]\n"
     ]
    }
   ],
   "source": [
    "finer_syns = []\n",
    "\n",
    "for syn in synlist:\n",
    "    hypo = syn.hyponyms()\n",
    "    for h in hypo:\n",
    "        finer_syns.append(h)\n",
    " #   print(syn.hyponyms())\n",
    "  \n",
    "print(finer_syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bed_and_breakfast',\n",
       " 'bed-and-breakfast',\n",
       " 'log_cabin',\n",
       " 'chateau',\n",
       " 'dacha',\n",
       " 'shooting_lodge',\n",
       " 'shooting_box',\n",
       " 'summer_house',\n",
       " 'villa',\n",
       " 'villa',\n",
       " 'lodge',\n",
       " 'flophouse',\n",
       " 'dosshouse',\n",
       " 'manor',\n",
       " 'manor_house',\n",
       " 'palace',\n",
       " 'castle',\n",
       " 'stately_home',\n",
       " 'court',\n",
       " 'deanery',\n",
       " 'manse',\n",
       " 'palace',\n",
       " 'parsonage',\n",
       " 'vicarage',\n",
       " 'rectory',\n",
       " 'religious_residence',\n",
       " 'cloister',\n",
       " 'brownstone',\n",
       " 'terraced_house']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_vocab_finer = []\n",
    "\n",
    "for syn in finer_syns:\n",
    "    for subsyn in syn.lemmas():\n",
    "          new_vocab_finer.append(str(subsyn.name()))\n",
    "\n",
    "new_vocab_finer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['garden',\n",
       " 'room',\n",
       " 'estate',\n",
       " 'manor',\n",
       " 'hedge',\n",
       " 'residence',\n",
       " 'park',\n",
       " 'lane',\n",
       " 'chair',\n",
       " 'sofa',\n",
       " 'settee',\n",
       " 'bed',\n",
       " 'bedroom',\n",
       " 'chaise',\n",
       " 'table',\n",
       " 'rug',\n",
       " 'carpet',\n",
       " 'candelabra',\n",
       " 'shed',\n",
       " 'cottage',\n",
       " 'fence',\n",
       " 'turret',\n",
       " 'castle',\n",
       " 'palace',\n",
       " 'hut',\n",
       " 'dwelling',\n",
       " ['bed_and_breakfast',\n",
       "  'bed-and-breakfast',\n",
       "  'log_cabin',\n",
       "  'chateau',\n",
       "  'dacha',\n",
       "  'shooting_lodge',\n",
       "  'shooting_box',\n",
       "  'summer_house',\n",
       "  'villa',\n",
       "  'villa',\n",
       "  'lodge',\n",
       "  'flophouse',\n",
       "  'dosshouse',\n",
       "  'manor',\n",
       "  'manor_house',\n",
       "  'palace',\n",
       "  'castle',\n",
       "  'stately_home',\n",
       "  'court',\n",
       "  'deanery',\n",
       "  'manse',\n",
       "  'palace',\n",
       "  'parsonage',\n",
       "  'vicarage',\n",
       "  'rectory',\n",
       "  'religious_residence',\n",
       "  'cloister',\n",
       "  'brownstone',\n",
       "  'terraced_house'],\n",
       " ['beach_house',\n",
       "  'boarding_house',\n",
       "  'boardinghouse',\n",
       "  'bungalow',\n",
       "  'cottage',\n",
       "  'cabin',\n",
       "  'chalet',\n",
       "  'chapterhouse',\n",
       "  'fraternity_house',\n",
       "  'frat_house',\n",
       "  'country_house',\n",
       "  'detached_house',\n",
       "  'single_dwelling',\n",
       "  'dollhouse',\n",
       "  \"doll's_house\",\n",
       "  'duplex_house',\n",
       "  'duplex',\n",
       "  'semidetached_house',\n",
       "  'farmhouse',\n",
       "  'gatehouse',\n",
       "  'guesthouse',\n",
       "  'hacienda',\n",
       "  'lodge',\n",
       "  'hunting_lodge',\n",
       "  'lodging_house',\n",
       "  'rooming_house',\n",
       "  'maisonette',\n",
       "  'maisonnette',\n",
       "  'mansion',\n",
       "  'mansion_house',\n",
       "  'manse',\n",
       "  'hall',\n",
       "  'residence',\n",
       "  'ranch_house',\n",
       "  'residence',\n",
       "  'row_house',\n",
       "  'town_house',\n",
       "  'safe_house',\n",
       "  'saltbox',\n",
       "  'sod_house',\n",
       "  'soddy',\n",
       "  'adobe_house',\n",
       "  'solar_house',\n",
       "  'tract_house',\n",
       "  'villa']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlled_vocab.append(new_vocab_finer)\n",
    "controlled_vocab.append(new_vocab)\n",
    "controlled_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'estate': 19,\n",
       "         'residence': 7,\n",
       "         'park': 51,\n",
       "         'dwelling': 6,\n",
       "         'room': 97,\n",
       "         'cottage': 56,\n",
       "         'garden': 11,\n",
       "         'shed': 3,\n",
       "         'table': 23,\n",
       "         'manor': 1,\n",
       "         'chair': 9,\n",
       "         'bed': 25,\n",
       "         'lane': 3,\n",
       "         'chaise': 6,\n",
       "         'rug': 1,\n",
       "         'bedroom': 1,\n",
       "         'sofa': 1})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlled_words = []\n",
    "\n",
    "for w in words:\n",
    "    if w.lower() in controlled_vocab:\n",
    "        controlled_words.append(w)\n",
    "\n",
    "Counter(controlled_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To be turned in on Canvas*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Expand the variable \"controlled words\" by looping through the words in the original \"controlled_vocab\" variables, finding their noun hyponyms, and creating a list of lemmas that you can use to search Jane Austen.\n",
    "\n",
    "2) Next, find the bigrams (two-word phrases) in Jane Austen that contain any of these words.  Sort the phrases by descending frequency, and paste the top twenty in Canvas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Write an interpretive paragraph of at least five sentences making some observations about the build landscape of England at the time of Jane Austen.  Offset phrases and words found in the text with quotation marks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
