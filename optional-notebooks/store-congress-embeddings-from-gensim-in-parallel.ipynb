{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallelization with Word Context Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Jo Guldi - 11/2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Workhorse script to download the Congressional testimony, produce a 5-yr word2vec model for showing change over time, run in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startdate = 1870\n",
    "enddate = 2010\n",
    "n = multiprocessing.cpu_count()\n",
    "print(multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_operation(df, func, n_cores = n):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LEMMA_INDEX'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-8f3b328039a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpprint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLemmatizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLEMMA_INDEX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_EXC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLEMMA_RULES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'LEMMA_INDEX'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk, numpy, re, matplotlib# , num2words\n",
    "from nltk.corpus import wordnet as wn\n",
    "import gensim \n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.spatial.distance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "#!pip install wordsegment --user\n",
    "from wordsegment import load, segment, clean\n",
    "import string\n",
    "load()\n",
    "import re\n",
    "import gensim, pprint\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines load some data from Congress. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qCg0mXrtOD1",
    "outputId": "4ecca950-9419-4b8d-96fc-aa5fbb1426f5"
   },
   "outputs": [],
   "source": [
    "all_speech_files = glob.glob('/scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_*.txt')\n",
    "CONGRESS_MIN_THRESHOLD = 1\n",
    "CONGRESS_MAX_THRESHOLD = 115\n",
    "\n",
    "speech_files = []\n",
    "\n",
    "for fn in all_speech_files:\n",
    "    number = int(fn.rsplit('_', 1)[-1].split('.')[0])\n",
    "    if CONGRESS_MIN_THRESHOLD <= number <= CONGRESS_MAX_THRESHOLD:\n",
    "        speech_files.append(fn)\n",
    "\n",
    "speech_files.sort()\n",
    "        \n",
    "def parse_one(fn):\n",
    "    print(f'Reading {fn}...')\n",
    "    return pd.read_csv(fn, sep='|', encoding=\"ISO-8859-1\", error_bad_lines=False, warn_bad_lines=False, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "speeches_df = pd.concat((parse_one(fn) for fn in speech_files))\n",
    "speeches_df.dropna(how='any', inplace=True)\n",
    "\n",
    "all_description_files = glob.glob('/scratch/group/oit_research_data/stanford_congress/hein-bound/descr_*.txt')\n",
    "                                  \n",
    "description_files = []\n",
    "\n",
    "for fn in all_description_files:\n",
    "    number = int(fn.rsplit('_', 1)[-1].split('.')[0])\n",
    "    if CONGRESS_MIN_THRESHOLD <= number <= CONGRESS_MAX_THRESHOLD:\n",
    "        description_files.append(fn)\n",
    "        description_files.sort()\n",
    "        \n",
    "description_df = pd.concat((parse_one(fn) for fn in description_files))\n",
    "\n",
    "all_data = pd.merge(speeches_df, description_df, on = 'speech_id')\n",
    "all_data.fillna(0, inplace=True)\n",
    "all_data = all_data.drop(['chamber', 'speech_id', 'number_within_file', 'speaker', 'first_name'], 1)\n",
    "all_data = all_data.drop(['last_name', 'state', 'gender', 'line_start', 'line_end', 'file', 'char_count', 'word_count'], 1)\n",
    "all_data['date']=pd.to_datetime(all_data['date'],format='%Y%m%d')\n",
    "all_data['year'] = pd.to_datetime(all_data['date']).dt.year\n",
    "all_data['5yrperiod'] = np.floor(all_data['year'] / 5) * 5 # round each year to the nearest 5 -- by dividing by 5 and \"flooring\" to the lowest integer\n",
    "all_data = all_data.drop(['date', 'year'], 1)\n",
    "all_data['index'] = np.arange(len(all_data)) # create an 'index' column\n",
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_l = all_data.sample(50000)\n",
    "sample_m = sample_l.sample(5000)\n",
    "sample = sample_m.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create function for cleaning & structuring the data in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section and the next, we will create a function, then launch that function with parallelize_operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_strings_into_sentences(data): # WORKING 4-16\n",
    "   \n",
    "    df = data\n",
    "    \n",
    "    # Getting s as pandas series which has split on full stop and new sentence a new line\n",
    "    s = df['speech'].str.split('.').apply(pd.Series,1).stack()\n",
    "    s.index = s.index.droplevel(-1) # to line up with df's index\n",
    "    s.name = 'sentence' # needs a name to join\n",
    "\n",
    "    del df['speech']\n",
    "    df = df.join(s)\n",
    "    del df['index']\n",
    "\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences_into_words(data): #  works 11-12-21\n",
    "    \n",
    "    new_column = [row.split() for row in data['sentence']]\n",
    "    data['sentence'] = new_column\n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING 4-17\n",
    "def cleanup(df):\n",
    "\n",
    "    df = sentences_df2.reset_index()\n",
    "    df2 = df  \n",
    "    \n",
    "    # To remove punctuation:\n",
    "    for i, sentence in enumerate(df['sentence']):\n",
    "        sentence2 = []\n",
    "        for word in sentence:\n",
    "            word2 = re.sub('\\W', '', word).lower()\n",
    "            if len(word2)>0:\n",
    "                sentence2.append(word2)\n",
    "        #df2['sentence'][index] = sentence2 #<---- ERROR HERE\n",
    "        #df2.at[index, 'sentence'] = sentence2 \n",
    "        df2.at[df2.index[i],'sentence'] = sentence2 \n",
    "        #df2['5yrperiod'][index] = df['5yrperiod'][index]\n",
    "\n",
    "                \n",
    "    return(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data_old(period_data):\n",
    "    sentences_df = parallelize_operation(period_data, split_strings_into_sentences) #  split speech into sentences\n",
    "    sentences_df2 = parallelize_operation(sentences_df, split_sentences_into_words) # split sentences into words\n",
    "    sentences_df3 = cleanup(sentences_df2) # cleanup punctuation and empty lines\n",
    "\n",
    "    return(sentences_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data(sentences, lemma, stopwords, stemmed):\n",
    "\n",
    "    # tokenize documents with gensim's tokenize() function\n",
    "    token_list = [list(gensim.utils.tokenize(sent, lower=True)) for sent in sentences]\n",
    "    \n",
    "    # build bigram model\n",
    "    bigram_mdl = gensim.models.phrases.Phrases(token_list, min_count=1, threshold=2)\n",
    "    \n",
    "    # apply bigram model to list\n",
    "    bigrams = [bigram_mdl[item] for item in token_list]\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    if lemma == True:\n",
    "        pool = multiprocessing.Pool()\n",
    "        bigrams =  pool.map(lemmatize_column, bigrams) #[[wn.morphy(item) for item in list] for list in token_list] \n",
    "        bigrams = [[item for item in list if item is not None] for list in bigrams] \n",
    "        bigrams  = [[item for item in list if len(item) > 2] for list in bigrams] \n",
    "    bigrams[0][:15]\n",
    "\n",
    "    # remove stopwords and/or do stemming\n",
    "    from gensim.parsing.preprocessing import preprocess_string, remove_stopwords#, #stem_text\n",
    "    CUSTOM_FILTERS = []\n",
    "    if stopwords == True:\n",
    "        CUSTOM_FILTERS.append(remove_stopwords)\n",
    "    if stemmed == True:\n",
    "        CUSTOM_FILTERS.append(stem_text)\n",
    "    bigrams = [preprocess_string(\" \".join(doc), CUSTOM_FILTERS) for doc in bigrams]\n",
    "    \n",
    "    return(bigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_column(token_list):    \n",
    "    \n",
    "    token_list = [wn.morphy(item) for item in token_list]#[[wn.morphy(item) for item in list] for list in token_list]\n",
    "    return(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making GENSIM Word Embeddings for every 5yr period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodnames = all_data['5yrperiod'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#periodnames = [period for period in periodnames if int(period) > 1970]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1870.0,\n",
       " 1875.0,\n",
       " 1880.0,\n",
       " 1885.0,\n",
       " 1890.0,\n",
       " 1895.0,\n",
       " 1900.0,\n",
       " 1905.0,\n",
       " 1910.0,\n",
       " 1915.0,\n",
       " 1920.0,\n",
       " 1925.0,\n",
       " 1930.0,\n",
       " 1935.0,\n",
       " 1940.0,\n",
       " 1945.0,\n",
       " 1950.0,\n",
       " 1955.0,\n",
       " 1960.0,\n",
       " 1965.0,\n",
       " 1970.0,\n",
       " 1975.0,\n",
       " 1980.0,\n",
       " 1985.0,\n",
       " 1990.0,\n",
       " 1995.0,\n",
       " 2000.0,\n",
       " 2005.0,\n",
       " 2010.0]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periodnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'lemmatized-stopworded-bigrammed-congress-model-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/group/history/hist_3368-jguldi/congress-embeddings\n"
     ]
    }
   ],
   "source": [
    "cd '/scratch/group/history/hist_3368-jguldi/congress-embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context = [] # create an empty dummy variable\n",
    "\n",
    "for period1 in periodnames:\n",
    "    \n",
    "    # get just the data in the period in question\n",
    "    period_data = all_data[all_data['5yrperiod'] == period1]\n",
    "\n",
    "\n",
    "    #  split speech into sentences, split sentences into words, cleanup punctuation and empty lines\n",
    "    structured_data = structure_data(period_data['speech'], True, True, False) \n",
    "    structured_data\n",
    "\n",
    "    # make a gensim model for that data\n",
    "    period_model = gensim.models.Word2Vec( \n",
    "        sentences = structured_data,\n",
    "        workers= n,\n",
    "        iter = 15,\n",
    "        min_count = 20, \n",
    "        size = 100)  \n",
    "    \n",
    "    # save the model with the name of the period\n",
    "    period_model.save(filename + str(period1)) \n",
    "    \n",
    "    # load model for each 5 yr period - one period per cycle of the for loop\n",
    "    #period_model = gensim.models.Word2Vec.load('model-' + str(period1)) # to load a saved model\n",
    "\n",
    "    # append each period to a larger model of all congress\n",
    "    if period1 == periodnames[0]:\n",
    "        congress_model = period_model # for the first time, save period_model as congress model\n",
    "    else:    \n",
    "        congress_model.build_vocab(sentences_df3['sentence'], # after the first period, add new period data to the congress model\n",
    "                               update = True)\n",
    "        congress_model.train(sentences_df3['sentence'], total_examples=period_model.corpus_count, epochs=period_model.epochs) \n",
    "\n",
    "    # store the model with the name of the period\n",
    "    congress_model.save(filename + str(startdate) + '-' + str(period1))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making GENSIM Word Embeddings for all Congress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/scratch/group/history/hist_3368-jguldi/congress-embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split speech into sentences, split sentences into words, cleanup punctuation and empty lines\n",
    "structured_data = structure_data(all_data) \n",
    "    \n",
    "# make a gensim model for that data\n",
    "congress_model = gensim.models.Word2Vec( \n",
    "        sentences = structured_data['sentence'],\n",
    "        workers= n,\n",
    "        iter = 15,\n",
    "        min_count = 20, \n",
    "        size = 100)  \n",
    "    \n",
    "# save the model with the name of the period\n",
    "congress_model.save('stopworded-bigrammed-congress_model-1870-2010') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
