{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For citation information, please see the \"Source Information\" section listed in the associated README file: https://github.com/stephbuon/digital-history/tree/master/hist3368-week12-word-context-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist 3368 - Week 12: Word Context Vectors with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Jo Guldi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Word Vectors vs. Word Embeddings \n",
    "\n",
    "Wordcount vectors are just what we’ve looked at: a simple count of words, with one integer per every word.  Wordcount embeddings are similar. But they typically add one more row of data or more per document.  That might mean that there’s a count of how many nouns, verbs, or adjectives there are per document. That might mean that there’s a count of bigrams, trigrams, fourgrams, or more – or multi-word phrases, plus or minus a word, called a “skipgram.”  These “hidden layers” in word embedding models mean an even richer model of which documents are like other documents. Because they factor in grammar and sentence structure as well as lexicon, they produce models that are very good at matching rhetorical style in text, and getting at the nuances of grammatical meaning. That is to say, they’re good at noticing when you mean “apple” the fruit (which you might eat or make into pie) or “apple” the computer (which you might turn on or off).  \n",
    "\n",
    "Functionally, you use word embeddings just the way you use wordcount vectors. You can measure the distance between them, just like we did in our notebook this week.  You can subtract them, just as we did, to get a litmus test of what’s different between two periods of time, or which words are used to signify masculinity and femininity.  \n",
    "\n",
    "*Previously, you've made word count vectors by brute force -- by 'grouping' your data by each word, then creating a word vector for each word, and using log likelihood to measure the most distinctive collocates of each word.\n",
    "\n",
    "*This time, however, we'll use the GENSIM package of word embeddings to work on a larger-scale sample of debates. We'll use GENSIM's pre-built tools to do analysis comparative to what you did with cosine distance and vector subtraction:\n",
    "\n",
    "     wv.similar_by_vector() -- which allows you to search for by vector. it thus allows you to find the words that commonly appear in the same context as a given keyword.  \n",
    "\n",
    "#### Skill Building for Historical Analysis\n",
    "\n",
    "By the end of this notebook, you'll know how to replicate most of the fancy work with vectors in the reading.  You'll be able to:\n",
    "* use word context vectors to analyze the intellectual history of concept words like \"freedom,\" \"gay\", or \"woman,\" detecting how their context changed from moment to moment\n",
    "* visualize changes to word concepts as a dendrogram\n",
    "* use GENSIM's \"most_similar()\" to generate a list of the words most similar to any concept (for instance \"freedom\") at different moments over time\n",
    "* visualize changes to the context of an individual word over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load programs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/b3/668ace2f0517b7fb01f780f93a75cb0592754d6365d808d2adccb2a94b92/gensim-4.1.2-cp36-cp36m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1MB)\n",
      "\u001b[K     |████████████████████████████████| 24.1MB 11.6MB/s eta 0:00:01    |██████▏                         | 4.7MB 11.6MB/s eta 0:00:02     |███████████████████████████████▍| 23.6MB 11.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting smart-open>=1.8.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/11/05f68ea934c24ade38e95ac30a38407767787c4e3db1776eae4886ad8c95/smart_open-5.2.1-py3-none-any.whl (58kB)\n",
      "\u001b[K     |████████████████████████████████| 61kB 1.5MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.18.1 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from gensim) (1.2.1)\n",
      "Collecting dataclasses; python_version < \"3.7\"\n",
      "  Downloading https://files.pythonhosted.org/packages/fe/ca/75fac5856ab5cfa51bbbcefa250182e50441074fdc3f803f6e76451fab43/dataclasses-0.8-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17.0 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from gensim) (1.17.3)\n",
      "Installing collected packages: smart-open, dataclasses, gensim\n",
      "Successfully installed dataclasses-0.8 gensim-4.1.2 smart-open-5.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import string\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.spatial.distance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(sentence):\n",
    "    result = [wn.morphy(item) for item in sentence]\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data(sentences, lemma, stopwords, stemmed):\n",
    "   # smoosh everything together\n",
    "    one_string = ' '.join(sentences)\n",
    "     \n",
    "    # break it into sentences \n",
    "    sentences =  sent_tokenize(one_string) \n",
    "    \n",
    "    # remove punctuation\n",
    "    sentences = [''.join(c for c in sentence if not c in string.punctuation) for sentence in sentences]\n",
    "\n",
    "    # lowercase\n",
    "    sentences = [sent.lower() for sent in sentences]\n",
    "\n",
    "    # tokenize documents with gensim's tokenize() function\n",
    "    sentences_in_words = [sent.split() for sent in sentences]\n",
    "    \n",
    "    # build bigram model\n",
    "    bigram_mdl = gensim.models.phrases.Phrases(sentences_in_words, min_count=1, threshold=2)\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    if lemma == True:\n",
    "        pool = multiprocessing.Pool()\n",
    "        sentences_in_words =  pool.map(lemmatize_list, sentences_in_words) #[[wn.morphy(item) for item in list] for list in token_list] \n",
    "        sentences_in_words = [[item for item in sentence if item is not None] for sentence in sentences_in_words] \n",
    "    sentences_in_words[0][:15]\n",
    "\n",
    "    # remove stopwords and/or do stemming\n",
    "    from gensim.parsing.preprocessing import preprocess_string#, remove_stopwords#, #stem_text\n",
    "    CUSTOM_FILTERS = []\n",
    "    if stopwords == True:\n",
    "        from gensim.parsing.preprocessing import remove_stopwords\n",
    "        CUSTOM_FILTERS.append(remove_stopwords)\n",
    "    if stemmed == True:\n",
    "        from gensim.parsing.preprocessing import stem_text\n",
    "        CUSTOM_FILTERS.append(stem_text)\n",
    "        \n",
    "    processed = [preprocess_string(\" \".join(sentence), CUSTOM_FILTERS) for sentence in sentences_in_words]\n",
    "    #processed = [[item for item in list if item] for list in processed]\n",
    "\n",
    "    # apply bigram model to list\n",
    "    result = [bigram_mdl[item] for item in processed]\n",
    "        \n",
    "    return(result)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a Function for Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because Gensim likes data to be organized by sentence, we'll want to break a string of text up into a list of strings, each of which is a sentence.\n",
    "\n",
    "We'll also want to have identified any bigrams in advance. \n",
    "\n",
    "Gensim makes it super easy to do both of these things with simple built-in commands:\n",
    "\n",
    "    gensim.utils.tokenize(doc, lower=True) -- to break sentences into words\n",
    "    gensim.models.phrases.Phrases(tokens, min_count=1, threshold=2) -- to find frequent bigrams in token strings.\n",
    "    \n",
    "We can package tokenization and bigrams into a function that we'll create called 'structure_data:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/group/history/hist_3368-jguldi\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/group/history/hist_3368-jguldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_csv(\"congress1967-2010.csv\")\n",
    "all_data = congress[congress['year'] >= 1967]\n",
    "all_data = congress[congress['year'] <= 1983]\n",
    "#congress = pd.read_csv(\"eighties_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>speech</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Those who do not enjoy the privilege of the fl...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>16</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr. President. on the basis of an agreement re...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>Mr. MANSFIELD</td>\n",
       "      <td>35</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>The Members of the Senate have heard the remar...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>40</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>The Chair lays before the Senate the following...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>151</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Secretary of State.</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>Mrs. AGNES BAGGETT</td>\n",
       "      <td>3</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                              speech        date  \\\n",
       "0  Those who do not enjoy the privilege of the fl...  1967-01-10   \n",
       "1  Mr. President. on the basis of an agreement re...  1967-01-10   \n",
       "2  The Members of the Senate have heard the remar...  1967-01-10   \n",
       "3  The Chair lays before the Senate the following...  1967-01-10   \n",
       "4                                Secretary of State.  1967-01-10   \n",
       "\n",
       "              speaker  word_count  year  month  month_year  \n",
       "0  The VICE PRESIDENT          16  1967      1  1967-01-01  \n",
       "1       Mr. MANSFIELD          35  1967      1  1967-01-01  \n",
       "2  The VICE PRESIDENT          40  1967      1  1967-01-01  \n",
       "3  The VICE PRESIDENT         151  1967      1  1967-01-01  \n",
       "4  Mrs. AGNES BAGGETT           3  1967      1  1967-01-01  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'year'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'year'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-750ea97eb32c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'5yrperiod'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;31m# round each year to the nearest 5 -- by dividing by 5 and \"flooring\" to the lowest integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mall_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'year'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'speaker'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unnamed: 0.1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_count'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'month'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/applications/anaconda/3/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'year'"
     ]
    }
   ],
   "source": [
    "all_data['5yrperiod'] = np.floor(all_data['year'] / 5) * 5 # round each year to the nearest 5 -- by dividing by 5 and \"flooring\" to the lowest integer\n",
    "all_data = all_data.drop(['date', 'year', 'speaker','Unnamed: 0', 'Unnamed: 0.1', 'word_count', 'month'], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If you see a warning above, it isn't an error.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['index'] = np.arange(len(all_data)) # create an 'index' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "atQkz5HhtOFo",
    "outputId": "cff96380-af39-4402-a9c2-a5c97c547fc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>month_year</th>\n",
       "      <th>5yrperiod</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those who do not enjoy the privilege of the fl...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. President. on the basis of an agreement re...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Members of the Senate have heard the remar...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Chair lays before the Senate the following...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Secretary of State.</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speech  month_year  5yrperiod  \\\n",
       "0  Those who do not enjoy the privilege of the fl...  1967-01-01     1965.0   \n",
       "1  Mr. President. on the basis of an agreement re...  1967-01-01     1965.0   \n",
       "2  The Members of the Senate have heard the remar...  1967-01-01     1965.0   \n",
       "3  The Chair lays before the Senate the following...  1967-01-01     1965.0   \n",
       "4                                Secretary of State.  1967-01-01     1965.0   \n",
       "\n",
       "   index  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, the first pass, we're going to do some memory-intensive work on the computer by creating word context vectors 'by hand' -- i.e., using onyl SKLEARN's CountVectorizer() and .fit_transform + cosine distance and subtraction.  \n",
    "\n",
    "Doing it this way is slower than loading some other packages that have been built specifically for working with large-scale wordcount vectors, where the code is packaged with high-dimensional math designed to make the comparisons run faster.\n",
    "\n",
    "We're doing it this way, however, so that you can really see for yourself how a word vector is built and what's inside it at every moment.   \n",
    "\n",
    "When we structure the data, build the vectors, subtract and measure the distance between vectors, we'll be able to inspect what's in the vector at every turn. You'd be able to do the math yourself if you looked more carefully.\n",
    "\n",
    "Later in the notebook, we'll return to a 'word embedding' software package that uses high-dimensional math and hidden layers to make whip-fast vectors.\n",
    "\n",
    "However, as we're doing old-fashioned vectors by hand, it'll go best if we \"downsample\" the data, taking a random sample of 5000 sentences spoken in Congress between 1985-2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some downsamples so we don't break the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_l = all_data.sample(500000)\n",
    "sample_m = sample_l.sample(50000)\n",
    "sample = sample_m.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Gensim, a Tool for Studying Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the code, we're shifting from word vectors made with SKLEARN to word \"embeddings\" made with the GENSIM package.\n",
    "\n",
    "GENSIM uses higher-level math to condense the matrices, meaning that we'll be able to deal with more information than the downsized sample above. Word embeddings like GENSIM also typically have a \"hidden layer\" of modeling which includes information about word order and part-of-speech, designed to make the word vectors more accurate models of the way that words are used in sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim wants to work with a dataset of texts where each row is a sentence, organized as a list of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break the data into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to break our dataframe of speeches into sentences.\n",
    "\n",
    "NOTE: the lines below may take a while. Splitting sentences and words can be intensive on a dataset of this scale. If it's not working for you, try *sample_l*, *sample_m* or *sample* where you see *all_data* below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = structure_data(all_data['speech'], lemma = False, stopwords = True, stemmed = True) # <---- switch out sample_l to all_data, sample_s or sample_m here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['enjoy', 'privilege', 'floor', 'retire', 'chamber'],\n",
       " ['mr_president'],\n",
       " ['basis', 'agreement', 'reach'],\n",
       " ['suggest', 'chamber', 'clear', 'attache'],\n",
       " ['absolutely', 'important', 'business', 'attend', 'chamber']]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to model our Congress data with the help of GENSIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up GENSIM\n",
    "\n",
    "The first step is to \"train\" the GENSIM model with the function `gensim.models.Word2Vec()`. This function has a couple dozen parameters, some of which are more important than others.\n",
    "\n",
    "Here are a few major ones. Only two are MANDATORY: these are marked with an asterisk:\n",
    "\n",
    "1. `sentences*`: This is where you provide your data. It must be in a format of iterable of iterables.\n",
    "2. `sg`: Your choice of training algorithm. There are two standard ways of training W2V vectors -- 'skipgram' and 'CBOW'. If you enter 1 here the skip-gram is applied; otherwise, the default is CBOW.\n",
    "3. `size*`: This is the length of your resulting word vectors. If you have a large corpus (>few billion tokens) you can go up to 100-300 dimensions. Generally word vectors with more dimensions give better results.\n",
    "4. `window`: This is the window of context words you are training on. In other words, how many words come before and after your given word. A good number is 4 here but this can vary depending on what you are interested in. For instance, if you are more interested in embeddings that embody semantic meaning, smaller window sizes work better. \n",
    "5. `alpha`: The learning rate of your model. If you are interested in machine learning experimentation with your vectors you may experiment with this parameter.\n",
    "6. `seed` (int): This is the random seed for your random initialization. All deep learning models initialize the weights with random floats before training. This is a useful field if you want to replicate your experiments because giving this a seed will initialize 'randomly' deterministically.\n",
    "7. `min_count`: This is the minimum frequency threshold. If a given word appears with lower frequency than provided it will be ignored. This is here because words with very low frequency are hard to train.\n",
    "8. `iter`: This is the number of iterations(entire run) over the corpus, also known as epochs. Usually anything between 1-10 is ok. The trade offs are that if you have higher iterations, it will take longer to train and the model may overfit on your dataset. However, longer training will allow your vectors to perform better on tasks relevant to your dataset.\n",
    "\n",
    "Most of these settings will not concern us. As you'll see below, we are only going to use four arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model = gensim.models.Word2Vec(\n",
    "    sentences = sentences,\n",
    "    workers = 30, # if you have more computing power available\n",
    "    min_count = 10 # remove words stated only once\n",
    "    ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also save our model in case we want to use it again in a later session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change this filename to reflect whatever you are doing now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'congress_model-1967-2010-full-lemmatized-stopworded-bigrammed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can load a model in the same way (remember this from our topic model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model = gensim.models.Word2Vec.load(filename) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `wv.index_to_key` allows us to see the words in our model (but careful! congress_model.wv.key_to_index will print out every word in the corpus -- a very long list!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['senat',\n",
       " 'state',\n",
       " 'amend',\n",
       " 'committe',\n",
       " 'time',\n",
       " 'mr_presid',\n",
       " 'year',\n",
       " 'nation',\n",
       " 'program',\n",
       " 'hous',\n",
       " 'gentleman',\n",
       " 'mr_speaker',\n",
       " 'congress',\n",
       " 'feder',\n",
       " 'presid',\n",
       " 'peopl',\n",
       " 'legisl',\n",
       " 'govern',\n",
       " 'provid',\n",
       " 'ask',\n",
       " 'act',\n",
       " 'member',\n",
       " 'need',\n",
       " 'servic',\n",
       " 'countri']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model.wv.index_to_key[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model itself is -- like the SKLEARN CountVectors model -- a matrix of vectors. Every row corresponds to the counts for one word. We can call the entire matrix or call up one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.4754717e+00, -4.4011908e+00, -4.6082503e-01, ...,\n",
       "        -4.2563915e-01, -3.5345390e-01,  1.9237659e+00],\n",
       "       [-2.4642570e+00, -2.7159767e+00, -1.7856932e-01, ...,\n",
       "        -1.3992382e+00,  1.2022231e+00, -2.0550833e+00],\n",
       "       [-1.0655633e+00, -2.8813925e-01, -7.5352246e-01, ...,\n",
       "        -1.5756048e+00,  2.4891226e-01,  7.7211171e-01],\n",
       "       ...,\n",
       "       [ 1.9001320e-02, -2.9346248e-02, -3.4601636e-02, ...,\n",
       "        -1.9130716e-02, -3.1579260e-02,  1.5718418e-03],\n",
       "       [-1.1716643e-03, -4.0329710e-02, -7.0127897e-02, ...,\n",
       "        -1.5042054e-02, -1.2530140e-02, -4.7811344e-02],\n",
       "       [-1.8255347e-02, -2.0252684e-02, -4.9519010e-02, ...,\n",
       "         7.4810158e-03, -2.2701774e-02, -3.9323349e-02]], dtype=float32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the fourth row of the model, represented as a word and as a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'committe'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = congress_model.wv.index_to_key[3]\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9128954 , -4.3240027 , -0.44421792,  0.17803319, -1.6887195 ,\n",
       "       -1.8417858 , -2.382663  , -0.54376405, -0.14213592,  2.4068887 ,\n",
       "       -0.8008058 , -2.0471299 ,  1.2257737 , -0.813561  ,  0.26949748,\n",
       "        2.1343286 , -1.7320954 , -2.3215182 , -1.7046514 ,  1.5707619 ,\n",
       "       -1.5329007 , -1.7563577 , -0.2608846 ,  0.6554528 , -0.6724577 ,\n",
       "        1.6333065 , -1.309933  ,  0.7698269 ,  3.6186852 ,  1.5208755 ,\n",
       "       -1.013507  , -0.18529823, -0.35002238,  2.0540264 , -1.26692   ,\n",
       "        1.6140392 , -0.40102732, -0.7102088 , -1.7623308 , -2.326815  ,\n",
       "       -3.711199  , -1.7033322 , -0.8530236 ,  3.6067219 ,  0.46784082,\n",
       "       -0.15662834, -1.1279367 , -0.13018279, -1.5130849 , -1.2077358 ,\n",
       "        0.1416394 ,  2.8089206 , -2.4010386 , -1.738008  , -0.50861233,\n",
       "       -2.4322774 , -0.30739444,  0.15793154,  2.0183172 ,  0.8630274 ,\n",
       "        0.94616544,  1.2690661 , -0.70399517, -2.5525053 , -0.80706865,\n",
       "        0.2644899 , -0.5465852 , -2.489552  ,  0.8314482 , -0.19818251,\n",
       "       -1.3867577 ,  0.25821123, -0.13773668,  1.0722232 ,  1.6042336 ,\n",
       "        3.895327  , -0.19772802, -0.7008293 ,  0.2722903 , -2.457045  ,\n",
       "       -1.3304869 ,  0.50170493,  0.28275594,  3.2120883 ,  4.6979303 ,\n",
       "        1.9955121 , -1.0781655 , -0.6645737 ,  1.1977693 ,  2.018111  ,\n",
       "        2.4449813 , -0.1650891 , -0.38315365,  1.8062367 , -0.14791669,\n",
       "        0.6114342 , -0.05139104,  0.7507459 , -2.5508335 ,  1.7907822 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.9128954 , -4.3240027 , -0.44421792,  0.17803319, -1.6887195 ,\n",
       "       -1.8417858 , -2.382663  , -0.54376405, -0.14213592,  2.4068887 ,\n",
       "       -0.8008058 , -2.0471299 ,  1.2257737 , -0.813561  ,  0.26949748,\n",
       "        2.1343286 , -1.7320954 , -2.3215182 , -1.7046514 ,  1.5707619 ,\n",
       "       -1.5329007 , -1.7563577 , -0.2608846 ,  0.6554528 , -0.6724577 ,\n",
       "        1.6333065 , -1.309933  ,  0.7698269 ,  3.6186852 ,  1.5208755 ,\n",
       "       -1.013507  , -0.18529823, -0.35002238,  2.0540264 , -1.26692   ,\n",
       "        1.6140392 , -0.40102732, -0.7102088 , -1.7623308 , -2.326815  ,\n",
       "       -3.711199  , -1.7033322 , -0.8530236 ,  3.6067219 ,  0.46784082,\n",
       "       -0.15662834, -1.1279367 , -0.13018279, -1.5130849 , -1.2077358 ,\n",
       "        0.1416394 ,  2.8089206 , -2.4010386 , -1.738008  , -0.50861233,\n",
       "       -2.4322774 , -0.30739444,  0.15793154,  2.0183172 ,  0.8630274 ,\n",
       "        0.94616544,  1.2690661 , -0.70399517, -2.5525053 , -0.80706865,\n",
       "        0.2644899 , -0.5465852 , -2.489552  ,  0.8314482 , -0.19818251,\n",
       "       -1.3867577 ,  0.25821123, -0.13773668,  1.0722232 ,  1.6042336 ,\n",
       "        3.895327  , -0.19772802, -0.7008293 ,  0.2722903 , -2.457045  ,\n",
       "       -1.3304869 ,  0.50170493,  0.28275594,  3.2120883 ,  4.6979303 ,\n",
       "        1.9955121 , -1.0781655 , -0.6645737 ,  1.1977693 ,  2.018111  ,\n",
       "        2.4449813 , -0.1650891 , -0.38315365,  1.8062367 , -0.14791669,\n",
       "        0.6114342 , -0.05139104,  0.7507459 , -2.5508335 ,  1.7907822 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model.wv.vectors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting Word Context with the GENSIM model, one word at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GENSIM model has all sorts of tools built in for navigating and inspecting vectors.  We will make use of the\n",
    "\n",
    "    most_similar()\n",
    "\n",
    "command, which calls up all the words used in the same context as a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('femal', 0.7471427321434021),\n",
       " ('negro', 0.7277549505233765),\n",
       " ('young_men', 0.7235952615737915),\n",
       " ('men_women', 0.708111584186554),\n",
       " ('teenag', 0.6955069303512573),\n",
       " ('youth', 0.6834402680397034),\n",
       " ('wive', 0.6701415777206421),\n",
       " ('mexicanamerican', 0.6671103239059448),\n",
       " ('male', 0.6652931571006775),\n",
       " ('religi', 0.6620873212814331),\n",
       " ('sex', 0.6553176045417786),\n",
       " ('depriv', 0.6506348848342896),\n",
       " ('young_women', 0.650577962398529),\n",
       " ('group', 0.6495906710624695),\n",
       " ('hispan', 0.6455756425857544),\n",
       " ('black', 0.6396685838699341),\n",
       " ('older_worker', 0.638055682182312),\n",
       " ('youngster', 0.6380534768104553),\n",
       " ('adult', 0.6349009871482849),\n",
       " ('parent', 0.6277905702590942)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model.wv.most_similar(\"women\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('american_soldier', 0.8551445007324219),\n",
       " ('wound', 0.8450905680656433),\n",
       " ('shot', 0.8175531625747681),\n",
       " ('maim', 0.8090488314628601),\n",
       " ('dy', 0.808813214302063),\n",
       " ('dead', 0.8079124093055725),\n",
       " ('men', 0.804377555847168),\n",
       " ('brutal', 0.8016231060028076),\n",
       " ('kill', 0.7979767322540283),\n",
       " ('beirut', 0.7927286028862),\n",
       " ('vietcong', 0.7925238609313965),\n",
       " ('battlefield', 0.7914227247238159),\n",
       " ('captur', 0.7881661653518677),\n",
       " ('hitler', 0.7857232689857483),\n",
       " ('crew', 0.7825039029121399),\n",
       " ('fighter', 0.7777116298675537),\n",
       " ('enemi', 0.7770015597343445),\n",
       " ('their', 0.7758663296699524),\n",
       " ('gallant', 0.7749981880187988),\n",
       " ('brave_men', 0.7722310423851013)]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model.wv.most_similar(\"soldier\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('knew', 0.8173918724060059),\n",
       " ('humor', 0.7942655086517334),\n",
       " ('honesti', 0.787624716758728),\n",
       " ('love', 0.768494188785553),\n",
       " ('god', 0.7611181735992432),\n",
       " ('public_servant', 0.7589733600616455),\n",
       " ('humbl', 0.7589018940925598),\n",
       " ('legaci', 0.7477531433105469),\n",
       " ('admir', 0.7477036118507385),\n",
       " ('courag', 0.7426941990852356),\n",
       " ('reput', 0.731036365032196),\n",
       " ('displai', 0.7307682037353516),\n",
       " ('warm', 0.7305134534835815),\n",
       " ('underdog', 0.7238001823425293),\n",
       " ('inspir', 0.719451367855072),\n",
       " ('charact', 0.7173877358436584),\n",
       " ('vision', 0.7162234783172607),\n",
       " ('gentl', 0.7155197262763977),\n",
       " ('fellow_man', 0.7104278802871704),\n",
       " ('woman', 0.7098684906959534)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "congress_model.wv.most_similar(\"man\", topn = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting vector similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the word context vectors that are most similar to 'men'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"men\", topn = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that men are spoken about almost in entirely the same context as women. But if women are spoken about in the same context as children, men are spoken about slightly more often in the same context as their homes. (what you see may vary with a different sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: everything the model knows it knows from our corpus. What we're learning are assumptions *immanent* to the corpus.  These aren't FACTS about women or men -- these are data about how women and men were spoken about in Congress, 1985-2005.\n",
    "\n",
    "Both `word2vec` and our model have limitations.\n",
    "\n",
    "Additionally, our training set is selective and small (just a subset of some debates about the environment). Therefore, our analogies can return some wild cards. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "america_vector = congress_model.wv['america']\n",
    "congress_model.wv.similar_by_vector(america_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this is pretty straight forward -- America is talked about in terms of Americans, the world, and prosperity.  Nothing to see here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our other method gives the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"america\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But when we look for other words that are spoken about with the same language as America -- the answers are quite telling.\n",
    "\n",
    "Wow. America is spoken about a few other places in the world.  Some versions of the output suggest that we speak of America with the same language in which we invoke democracy, drugs, and the interests of different peoples, especially workers. (what you see may vary with a different sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own hand at interpreting these outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"iraq\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret these similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"britain\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtracting Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll recall that we've used vector subtraction before.  Subtracting the context for \"woman\" from the context for \"man\" produces a vector of high scores for the words that only appear around \"man\" but not woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['man'] - congress_model.wv['woman']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['woman'] - congress_model.wv['man']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Vectors to Find Substitutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"women\", topn = 1000)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Store just the words as a vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "women_context = [word[0] for word in congress_model.wv.most_similar(\"women\", topn = 100)]\n",
    "women_context[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the context vectors of each word and add them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = congress_model.wv[women_context[0]] \n",
    "\n",
    "for word in women_context[1:len(women_context)]:\n",
    "    next_vector = congress_model.wv[word] \n",
    "    sum = sum + next_vector\n",
    "    \n",
    "    \n",
    "congress_model.wv.similar_by_vector(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the output of this process is a different list than we started with.\n",
    "\n",
    "What have we done? We've taken the word context vectors for the 100 words that most commonly co-occur with the word 'women.'  We've added those word contet vectors together.\n",
    "\n",
    "In essence, we've taken the context for 'women' and asked, 'what other words might substitute for the word 'women,' given the same context?'\n",
    "\n",
    "Our final list is essentially the *functional synonyms* for the word 'women.'  The words in this list could functionally be substituted for 'women' in most sentences in which the word 'women' is used, with the same meaning -- at least, from the point of view of the speakers.  \n",
    "\n",
    "We are looking here at a powerful tool for understanding stereotypes.  Digital scholar Richard Jean So has used a similar process to show that 'homeless' was functionally a synonym for 'black' in the novels written by white people of the twentieth century."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance and Similarity with Vectors in GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make *quantitative* measurements about how close or far about any two vectors are based on their usage.\n",
    "\n",
    "'Similarity' in this case is a mathematical statistic, calculated as the cosine similarity between any two vectors -- it's 1 minus cosine distance.  You've used cosine distance before -- you're a whiz with cosine distance already. \n",
    "\n",
    "With similarity, the higher the number, the more alike two terms are in the context in which they are used. \n",
    "\n",
    "When we used cosine distance before, we were doing it one vector at a time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('women', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('soldier', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('women', 'individu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the similarities\n",
    "\n",
    "Some researchers have used these similarity 'scores' to study how ideas were related in the past.  Scholars have sometimes produced a \"dendrogram\" of words related to other words, which we learned was created on the basis of cosine distance scores between word vectors.\n",
    "\n",
    "This dendrogram was used to compare the meaning of \"freedom\" in the seventeenth century (when the word was nearest in meaning to \"friendship\") to the meaning of \"freedom\" in the eighteenth century (when the word became associated with nations and patriotism).\n",
    "\n",
    "Let's see if we can make a dendrogram of words for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \n",
    "\n",
    "    linkage()\n",
    "    \n",
    "command performs hierarchical clustering -- in other words, it takes the Euclidean similarity score between any two vectors, and then ranks them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['dream', 'war', 'fight', 'racism', 'wealth', 'delight',# 'today', \n",
    "            'tomorrow', 'past', 'present', 'futur',\n",
    "            'america', 'ireland', 'britain', 'iraq', 'china', 'democrat', #'dictator',\n",
    "            'totalitarian', #'democracy', \n",
    "            #'charity', 'socialism','communism', \n",
    "            'russia', 'congress', 'riot','protest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: if you get an error because any of the words above aren't in your sample corpus, edit the list and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_vectors = congress_model.wv[keywords]\n",
    "keyword_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "links = linkage(keyword_vectors, method='complete', metric='seuclidean')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ranking gives us a read of which vectors are closest to which vectors.  We can visualize it using matplotlib and the \"dendrogram\" command from SKLEARN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "l = links\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.ylabel('word')\n",
    "plt.xlabel('distance')\n",
    "\n",
    "dendrogram(\n",
    "    l,\n",
    "    leaf_rotation=0,  # rotates the x axis labels\n",
    "    leaf_font_size=16,  # font size for the x axis labels\n",
    "    orientation='left',\n",
    "    leaf_label_func=lambda v: str(keywords[v])\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little tweaking, you can create a list of only those vectors for the words most of interest to you, using GENSIM to visualize their similarity to each other in the corpus.\n",
    "\n",
    "You could even -- like Connell's blog entry indicates -- create a separate dendrogram for 1985 and another for 2005, to see how these terms have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Abstract Relatedness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity scores can also be used to visualize words as points in space where each word represents a single point.\n",
    "\n",
    "These points represent words' relationships with one-another.\n",
    "\n",
    "The code that follows is borrowed from digital humanist Dan Sinykin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.key_to_index.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.key_to_index ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_scatterplot(congress_model.wv, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth be told, I don't love this visualization; it's visualizing abstract relationships that show the conceptual distance between different entities in the model. I present it to you as a cute toy, not as an approved visualization that i'd like to see in your work. \n",
    "\n",
    "Please use PCA analysis with care; it's almost impossible to get back to what it actually *means* -- at least without pairing it with other visualizations and measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Studying Change Over Time with GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to go further and use GENSIM as a tool for studying change over time, we'll want to organize the data by the features of the data that we care about.\n",
    "\n",
    "In this case, we want to be able to investigate keywords, but also 5-year-periods.  We have a column called '5yrperiod,' and we want to make sure that this column is part of our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We'll want to organize the data by the features of the data that we care about -- the period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the names of the unique periods in this dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1970.0, 1965.0, 1975.0, 1980.0]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "periodnames = sample_m['5yrperiod'].unique().tolist()\n",
    "periodnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of periods, we can tell Gensim to make a 'model' for each period. And we can use these models to compare how the usage of each word changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***The following line of code might take a while, since we're creating 6 different gensim models. Fortunately, we're saving all of them, so if you want to go back and run this for a different word later, you can just load the old data rather than running the whole thing again.  If you want to rerun the code, follow the directions in the code below to hashtag out the gensim command and instead use the period_model = genseim-models-Word2Vec.load() command to load the old data -- it will be much less time consuming.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'sample-m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/group/history/hist_3368-jguldi/congress-embeddings\n"
     ]
    }
   ],
   "source": [
    "cd '/scratch/group/history/hist_3368-jguldi/congress-embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on  1970.0\n",
      "working on  1965.0\n",
      "working on  1975.0\n",
      "working on  1980.0\n"
     ]
    }
   ],
   "source": [
    "for period1 in periodnames:\n",
    "    print('working on ', period1)\n",
    "\n",
    "    # grab the data from period1\n",
    "    period_data = sample_m[sample_m['5yrperiod'] == period1] # select one period at a time\n",
    "    \n",
    "    # structure the data for Gensim\n",
    "    period_sentences = structure_data(period_data['speech'], lemma = False, stopwords = True, stemmed = True)\n",
    "    \n",
    "    # make the Gensim model\n",
    "    period_model = gensim.models.Word2Vec( # make a gensim model for that data\n",
    "        sentences = period_sentences,\n",
    "        min_count = 2)\n",
    "    \n",
    "    # save it\n",
    "    period_model.save(dataname + '-model-' + str(period1)) # save the model with the name of the period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a result of this code, we have saved in the congress-embeddings folder a \"period model\" labeled with the name of your data and each period.  We can call up each period one at a time to get information about how any individual words were talked about.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search the Period Models for a Keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The period models aren't very interesting in themselves, but they allow us to efficiently search for how the context of a keyword changes over time.\n",
    "\n",
    "Let's search each 5-yr-period for a keyword and save the results as the variable *keyword_context*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword1 = 'women'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'sample-congress-model-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/group/history/hist_3368-jguldi/congress-embeddings\n"
     ]
    }
   ],
   "source": [
    "cd '/scratch/group/history/hist_3368-jguldi/congress-embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on  1970.0\n",
      "found  women\n",
      "working on  1965.0\n",
      "found  women\n",
      "working on  1975.0\n",
      "found  women\n",
      "working on  1980.0\n",
      "found  women\n"
     ]
    }
   ],
   "source": [
    "#########  after the first run, use this line to call the old data without generating it again\n",
    "keyword_context = []\n",
    "dates_found = []\n",
    "\n",
    "# cycle through each period\n",
    "for period1 in periodnames:\n",
    "    print('working on ', period1)\n",
    "    \n",
    "    # load the model from period1\n",
    "    period_model = gensim.models.Word2Vec.load(dataname + '-model-' + str(period1)) # to load a saved model\n",
    "\n",
    "    ## is the keyword found?\n",
    "    if keyword1 in period_model.wv.key_to_index:\n",
    "        print('found ', keyword1)\n",
    "        \n",
    "        # get the context vector for keyword1\n",
    "        keyword_context_period = period_model.wv.most_similar(keyword1, topn = 5000) \n",
    "        \n",
    "        # save it for later\n",
    "        keyword_context.append(keyword_context_period) # save the context of how women were talked about for later\n",
    "        dates_found.append(period1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable *keyword_context* is a list of vectors, each of which corresponds to a period.\n",
    "\n",
    "   * keyword_context[0] is a vector of the words that most frequently occurred with keyword1 in 1965\n",
    "   * keyword_context[1] is a vector of the words that most frequently occurred with keyword1 in 1970\n",
    "   * keyword_context[2] is a vector of the words that most frequently occurred with keyword1 in 1975\n",
    "   * ...and so on\n",
    "   \n",
    "We can use this list of vectors to study how the context of 'woman' was changing from period to period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context[0][0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can also grab just the names from the keyword vectors this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item[0] for item in keyword_context[1]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can grab just the numbers for any given year (in this case, the second period -- 1990 -- [1]) this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item[1] for item in keyword_context[1]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to abstract only unique values while keeping the list in the same order -- the order of first appearance\n",
    "def unique2(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a flattened list of all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in range(len(dates_found)):\n",
    "    words = [item[0] for item in keyword_context[i]][:10]\n",
    "    all_words.append(words)\n",
    "\n",
    "all_words2 = []\n",
    "for list in all_words:\n",
    "    for word in list:\n",
    "        all_words2.append(word)\n",
    "\n",
    "numwords = 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import linspace\n",
    "from matplotlib import cm\n",
    "colors = [ cm.jet(x) for x in linspace(.5, 2, 50) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "from adjustText import adjust_text\n",
    "from numpy import linspace\n",
    "from matplotlib import cm\n",
    "\n",
    "colors = [ cm.viridis(x) for x in linspace(0, 1, len(unique2(all_words2))+10) ]\n",
    "\n",
    "# change the figure's size here\n",
    "plt.figure(figsize=(10,10), dpi = 200)\n",
    "\n",
    "texts = []\n",
    "\n",
    "# plt.annotate only plots one label per iteration, so we have to use a for loop \n",
    "for i in range(len(dates_found)):    # cycle through the period names\n",
    "    \n",
    "    #yyy = int(keyword_per_year[keyword_per_year['5yrperiod'] == int(xx)]['count'])   # how many times was the keyword used that year?\n",
    "                     \n",
    "    for j in range(10):     # cycle through the first ten words (you can change this variable)\n",
    "        \n",
    "        xx = dates_found[i]        # on the x axis, plot the period name\n",
    "        yy = [item[1] for item in keyword_context[i]][j]         # on the y axis, plot the distance -- how closely the word is related to the keyword\n",
    "        txt = [item[0] for item in keyword_context[i]][j]        # grab the name of each collocated word\n",
    "        colorindex = unique2(all_words2).index(txt)   # this command keeps all dots for the same word the same color\n",
    "        \n",
    "        plt.scatter(                                             # plot dots\n",
    "            xx, #x axis\n",
    "            yy, # y axis\n",
    "            linewidth=1, \n",
    "            color = colors[colorindex],\n",
    "            edgecolors = 'darkgray',\n",
    "            s = 100, # dot size\n",
    "            alpha=0.8)  # dot transparency\n",
    "\n",
    "        # make a label for each word\n",
    "        texts.append(plt.text(xx, yy, txt))\n",
    "\n",
    "# Code to help with overlapping labels -- may take a minute to run\n",
    "adjust_text(texts, force_points=0.2, force_text=.7, \n",
    "                    expand_points=(1, 1), expand_text=(1, 1),\n",
    "                    arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"What words were used in the same contex as ''\" + keyword1 + \"' in Congress?\", fontsize=20, fontweight=0, color='Red')\n",
    "plt.xlabel(\"period\")\n",
    "plt.ylabel(\"normalized probability score\")\n",
    "\n",
    "\n",
    "filename2 = 'words-used-in-context-of-' + keyword1 + '-' + filename\n",
    "plt.savefig(filename2 + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This week, you may choose a coding-intensive exercise OR an interpretive research question.\n",
    "\n",
    "You do not have to do both.\n",
    "\n",
    "#### Coding-intensive exercise\n",
    "   * Create a list of keywords that you think would be particularly relevant for Congress during this time -- something that might demonstrate historical change in ideas.\n",
    "        * Using the code above, create a GENSIM model for 1985, 1995, and 2005\n",
    "        * Using the code above, create an array of vectors for your words for each time period\n",
    "        * Using the code above, draw a dendrogram of keyword relatedness for the three time periods.\n",
    "   * The code for the final visualization above shows the most common words used in the context of a keyword.  Tweak the code so that instead of showing *the keyword's context*, the visualization shows the *words that share the same context as the keyword in question.* This tweak should require about two lines of code.\n",
    "     \n",
    "        * Use the final visualization that you created above and its variation as the basis for an interpretive essay of one page.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Interpretive Research Question\n",
    "\n",
    "In the code above, we learned how to streamline code to make it run over the complete data set.\n",
    "\n",
    "Now that you can process *all* the data, you're ready for a more serious engagement with interpretation of historical questions, like:\n",
    "\n",
    "* What are some of the ideas that changed in Congress over this time period? For instance, historians of this period frequently talk about the rise of a free-market ideology, a critique of the welfare state, arguments about the nature of democracy and America's role abroad. Can you support an argument about intellectual change on the basis of the changing context in which words were discussed?\n",
    "* What groups of people were talked about in this period, and did the way they were spoken about change? Consider the role of women, minorities, the gay movement, and individuals who identify as religious in your answer.\n",
    "* How did America's relationship with other nations change during this time period?  A historian might consider, for starters, the fall of the Berlin Wall in 1989 and the disintegration of the former USSR; the rise of terrorism, and the identification of Iraq, Iran, and Afghanistan as a frontier for US pacification; the border with Mexico and issues of immigration. Can you find systematic evidence of when and how one or more of these conversations changed in the data?\n",
    "\n",
    "Choose one of the above questions. Iterate through a series of keyword queries and data results that would support a robust answer. Formulate an answer with at least one visualization and a page of writing, single-spaced, which analyze historical change in detail.\n",
    "\n",
    "Turn in your work on Canvas. Do not turn in an ipynb. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
