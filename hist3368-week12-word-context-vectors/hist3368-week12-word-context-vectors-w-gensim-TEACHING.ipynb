{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For citation information, please see the \"Source Information\" section listed in the associated README file: https://github.com/stephbuon/digital-history/tree/master/hist3368-week12-word-context-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist 3368 - Week 12: Word Context Vectors with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teaching Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Jo Guldi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "import string\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.spatial.distance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_list(sentence):\n",
    "    result = [wn.morphy(item) for item in sentence]\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_data(sentences, lemma, stopwords, stemmed):\n",
    "   # smoosh everything together\n",
    "    one_string = ' '.join(sentences)\n",
    "     \n",
    "    # break it into sentences \n",
    "    sentences =  sent_tokenize(one_string) \n",
    "    \n",
    "    # remove punctuation\n",
    "    sentences = [''.join(c for c in sentence if not c in string.punctuation) for sentence in sentences]\n",
    "\n",
    "    # lowercase\n",
    "    sentences = [sent.lower() for sent in sentences]\n",
    "\n",
    "    # tokenize documents with gensim's tokenize() function\n",
    "    sentences_in_words = [sent.split() for sent in sentences]\n",
    "    \n",
    "    # build bigram model\n",
    "    bigram_mdl = gensim.models.phrases.Phrases(sentences_in_words, min_count=1, threshold=2)\n",
    "\n",
    "    # lemmatize the tokens\n",
    "    if lemma == True:\n",
    "        pool = multiprocessing.Pool()\n",
    "        sentences_in_words =  pool.map(lemmatize_list, sentences_in_words) #[[wn.morphy(item) for item in list] for list in token_list] \n",
    "        sentences_in_words = [[item for item in sentence if item is not None] for sentence in sentences_in_words] \n",
    "    sentences_in_words[0][:15]\n",
    "\n",
    "    # remove stopwords and/or do stemming\n",
    "    from gensim.parsing.preprocessing import preprocess_string#, remove_stopwords#, #stem_text\n",
    "    CUSTOM_FILTERS = []\n",
    "    if stopwords == True:\n",
    "        from gensim.parsing.preprocessing import remove_stopwords\n",
    "        CUSTOM_FILTERS.append(remove_stopwords)\n",
    "    if stemmed == True:\n",
    "        from gensim.parsing.preprocessing import stem_text\n",
    "        CUSTOM_FILTERS.append(stem_text)\n",
    "        \n",
    "    processed = [preprocess_string(\" \".join(sentence), CUSTOM_FILTERS) for sentence in sentences_in_words]\n",
    "    #processed = [[item for item in list if item] for list in processed]\n",
    "\n",
    "    # apply bigram model to list\n",
    "    result = [bigram_mdl[item] for item in processed]\n",
    "        \n",
    "    return(result)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallelize_operation(df, func, n_cores = n):\n",
    "    df_split = np.array_split(df, n_cores)\n",
    "    pool = Pool(n)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load some Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/group/history/hist_3368-jguldi\n"
     ]
    }
   ],
   "source": [
    "cd /scratch/group/history/hist_3368-jguldi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress = pd.read_csv(\"congress1967-2010.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = congress.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>speech</th>\n",
       "      <th>date</th>\n",
       "      <th>speaker</th>\n",
       "      <th>word_count</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>month_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Those who do not enjoy the privilege of the fl...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>16</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Mr. President. on the basis of an agreement re...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>Mr. MANSFIELD</td>\n",
       "      <td>35</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>The Members of the Senate have heard the remar...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>40</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>The Chair lays before the Senate the following...</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>The VICE PRESIDENT</td>\n",
       "      <td>151</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Secretary of State.</td>\n",
       "      <td>1967-01-10</td>\n",
       "      <td>Mrs. AGNES BAGGETT</td>\n",
       "      <td>3</td>\n",
       "      <td>1967</td>\n",
       "      <td>1</td>\n",
       "      <td>1967-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  \\\n",
       "0           0             0   \n",
       "1           1             1   \n",
       "2           2             2   \n",
       "3           3             3   \n",
       "4           4             4   \n",
       "\n",
       "                                              speech        date  \\\n",
       "0  Those who do not enjoy the privilege of the fl...  1967-01-10   \n",
       "1  Mr. President. on the basis of an agreement re...  1967-01-10   \n",
       "2  The Members of the Senate have heard the remar...  1967-01-10   \n",
       "3  The Chair lays before the Senate the following...  1967-01-10   \n",
       "4                                Secretary of State.  1967-01-10   \n",
       "\n",
       "              speaker  word_count  year  month  month_year  \n",
       "0  The VICE PRESIDENT          16  1967      1  1967-01-01  \n",
       "1       Mr. MANSFIELD          35  1967      1  1967-01-01  \n",
       "2  The VICE PRESIDENT          40  1967      1  1967-01-01  \n",
       "3  The VICE PRESIDENT         151  1967      1  1967-01-01  \n",
       "4  Mrs. AGNES BAGGETT           3  1967      1  1967-01-01  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['5yrperiod'] = np.floor(all_data['year'] / 5) * 5 # round each year to the nearest 5 -- by dividing by 5 and \"flooring\" to the lowest integer\n",
    "all_data = all_data.drop(['date', 'year', 'speaker','Unnamed: 0', 'Unnamed: 0.1', 'word_count', 'month'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['index'] = np.arange(len(all_data)) # create an 'index' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "atQkz5HhtOFo",
    "outputId": "cff96380-af39-4402-a9c2-a5c97c547fc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>month_year</th>\n",
       "      <th>5yrperiod</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Those who do not enjoy the privilege of the fl...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mr. President. on the basis of an agreement re...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Members of the Senate have heard the remar...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Chair lays before the Senate the following...</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Secretary of State.</td>\n",
       "      <td>1967-01-01</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speech  month_year  5yrperiod  \\\n",
       "0  Those who do not enjoy the privilege of the fl...  1967-01-01     1965.0   \n",
       "1  Mr. President. on the basis of an agreement re...  1967-01-01     1965.0   \n",
       "2  The Members of the Senate have heard the remar...  1967-01-01     1965.0   \n",
       "3  The Chair lays before the Senate the following...  1967-01-01     1965.0   \n",
       "4                                Secretary of State.  1967-01-01     1965.0   \n",
       "\n",
       "   index  \n",
       "0      0  \n",
       "1      1  \n",
       "2      2  \n",
       "3      3  \n",
       "4      4  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_l = all_data.sample(500000)\n",
    "sample_m = sample_l.sample(50000)\n",
    "sample = sample_m.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing Gensim, a Tool for Studying Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing your data into a list of sentences, each sentence a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = structure_data(all_data['speech'], lemma = False, stopwords = True, stemmed = True) # <---- switch out sample_l to all_data, sample_s or sample_m here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use Gensim to create a vector model from sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model = gensim.models.Word2Vec(\n",
    "    sentences = sentences,\n",
    "    workers = n # if you have more computing power available\n",
    "    min_count = 10 # remove words stated only once\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the lines below to save the model for later use or to load a saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'congress-1967-2010-full-stopworded-bigrammed-stemmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.save(filename) #### save the model you just made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'congress-1967-2010-full-stopworded-bigrammed-stemmed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model = gensim.models.Word2Vec.load(filename) #### load a saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the Contents of Your Vector Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the CONTEXT for One Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_vector = congress_model.wv['man']\n",
    "congress_model.wv.similar_by_vector(man_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_vector = congress_model.wv['woman']\n",
    "congress_model.wv.similar_by_vector(woman_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_vector = congress_model.wv['person']\n",
    "congress_model.wv.similar_by_vector(individual_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soldier_vector = congress_model.wv['soldier']\n",
    "congress_model.wv.similar_by_vector(soldier_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"women\", topn = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting vector similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own hand at interpreting these outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret these similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"iraq\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"america\", topn = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"britain\", topn = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtracting Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['man'] - congress_model.wv['woman']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['woman'] - congress_model.wv['boy']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['peopl'] - congress_model.wv['person']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['person'] - congress_model.wv['peopl']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['think'] - congress_model.wv['heart']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['feel'] - congress_model.wv['think']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding vectors to find synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context = [word[0] for word in congress_model.wv.most_similar(\"women\", topn = 100)]\n",
    "\n",
    "sum = congress_model.wv[keyword_context[0]] \n",
    "\n",
    "for word in keyword_context[1:len(keyword_context)]:\n",
    "    next_vector = congress_model.wv[word] \n",
    "    sum = sum + next_vector\n",
    "    \n",
    "congress_model.wv.similar_by_vector(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context = [word[0] for word in congress_model.wv.most_similar(\"soldier\", topn = 100)]\n",
    "sum = congress_model.wv[keyword_context[0]] \n",
    "for word in keyword_context[1:len(keyword_context)]:\n",
    "    next_vector = congress_model.wv[word] \n",
    "    sum = sum + next_vector\n",
    "congress_model.wv.similar_by_vector(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context = [word[0] for word in congress_model.wv.most_similar(\"happi\", topn = 100)]\n",
    "sum = congress_model.wv[keyword_context[0]] \n",
    "for word in keyword_context[1:len(keyword_context)]:\n",
    "    next_vector = congress_model.wv[word] \n",
    "    sum = sum + next_vector\n",
    "congress_model.wv.similar_by_vector(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context = [word[0] for word in congress_model.wv.most_similar(\"american\", topn = 100)]\n",
    "sum = congress_model.wv[keyword_context[0]] \n",
    "for word in keyword_context[1:len(keyword_context)]:\n",
    "    next_vector = congress_model.wv[word] \n",
    "    sum = sum + next_vector\n",
    "congress_model.wv.similar_by_vector(sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance and Similarity with Vectors in GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With similarity, the higher the number, the more alike two terms are in the context in which they are used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('women', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('soldier', 'men')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('women', 'person')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the similarities as a Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['dream',  'war',  'wealth', 'happi',  'tomorrow', 'past', 'present', 'futur', 'america',  'britain', 'china', 'democrat', 'welfar', 'commun', 'russia', 'congress', 'protest']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_vectors = congress_model.wv[keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "links = linkage(keyword_vectors, method='complete', metric='seuclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "l = links\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.ylabel('word')\n",
    "plt.xlabel('distance')\n",
    "\n",
    "dendrogram(\n",
    "    l,\n",
    "    leaf_rotation=0,  # rotates the x axis labels\n",
    "    leaf_font_size=16,  # font size for the x axis labels\n",
    "    orientation='left',\n",
    "    leaf_label_func=lambda v: str(keywords[v])\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note: if you get an error above, delete any words from the list.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Abstract Relatedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.key_to_index.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.key_to_index ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_scatterplot(congress_model.wv, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research the Changing Context of One Keyword Over Time with GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a word embedding model per period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataname = 'sample-m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodnames = all_data['5yrperiod'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd '/scratch/group/history/hist_3368-jguldi/congress-embeddings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for period1 in periodnames:\n",
    "    print('working on ', period1)\n",
    "\n",
    "    # grab the data from period1\n",
    "    period_data = sample_m[sample_m['5yrperiod'] == period1] # select one period at a time\n",
    "    \n",
    "    # structure the data for Gensim\n",
    "    period_sentences = structure_data(period_data['speech'], lemma = False, stopwords = True, stemmed = True)\n",
    "    \n",
    "    # make the Gensim model\n",
    "    period_model = gensim.models.Word2Vec( # make a gensim model for that data\n",
    "        sentences = period_sentences,\n",
    "        min_count = 2, \n",
    "        size = 100)\n",
    "    \n",
    "    # save it\n",
    "    period_model.save(dataname + '-model-' + str(period1)) # save the model with the name of the period\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search each 5-year model for a keyword."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword1 = 'black'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########  after the first run, use this line to call the old data without generating it again\n",
    "keyword_context = []\n",
    "dates_found = []\n",
    "\n",
    "# cycle through each period\n",
    "for period1 in periodnames:\n",
    "    print('working on ', period1)\n",
    "    \n",
    "    # load the model from period1\n",
    "    period_model = gensim.models.Word2Vec.load(dataname + '-model-' + str(period1)) # to load a saved model\n",
    "\n",
    "    ## is the keyword found?\n",
    "    if keyword1 in period_model.wv.key_to_index:\n",
    "        print('found ', keyword1)\n",
    "        \n",
    "        # get the context vector for keyword1\n",
    "        keyword_context_period = period_model.wv.most_similar(keyword1, topn = 5000) \n",
    "        \n",
    "        # save it for later\n",
    "        keyword_context.append(keyword_context_period) # save the context of how women were talked about for later\n",
    "        dates_found.append(period1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to abstract only unique values while keeping the list in the same order -- the order of first appearance\n",
    "def unique2(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in range(len(dates_found)):\n",
    "    words = [item[0] for item in keyword_context[i]][:10]\n",
    "    all_words.append(words)\n",
    "\n",
    "all_words2 = []\n",
    "for list in all_words:\n",
    "    for word in list:\n",
    "        all_words2.append(word)\n",
    "\n",
    "numwords = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "from adjustText import adjust_text\n",
    "from numpy import linspace\n",
    "from matplotlib import cm\n",
    "\n",
    "colors = [ cm.viridis(x) for x in linspace(0, 1, len(unique2(all_words2))+10) ]\n",
    "\n",
    "# change the figure's size here\n",
    "plt.figure(figsize=(10,10), dpi = 200)\n",
    "\n",
    "texts = []\n",
    "\n",
    "# plt.annotate only plots one label per iteration, so we have to use a for loop \n",
    "for i in range(len(dates_found)):    # cycle through the period names\n",
    "    \n",
    "    #yyy = int(keyword_per_year[keyword_per_year['5yrperiod'] == int(xx)]['count'])   # how many times was the keyword used that year?\n",
    "                     \n",
    "    for j in range(10):     # cycle through the first ten words (you can change this variable)\n",
    "        \n",
    "        xx = dates_found[i]        # on the x axis, plot the period name\n",
    "        yy = [item[1] for item in keyword_context[i]][j]         # on the y axis, plot the distance -- how closely the word is related to the keyword\n",
    "        txt = [item[0] for item in keyword_context[i]][j]        # grab the name of each collocated word\n",
    "        colorindex = unique2(all_words2).index(txt)   # this command keeps all dots for the same word the same color\n",
    "        \n",
    "        plt.scatter(                                             # plot dots\n",
    "            xx, #x axis\n",
    "            yy, # y axis\n",
    "            linewidth=1, \n",
    "            color = colors[colorindex],\n",
    "            edgecolors = 'darkgray',\n",
    "            s = 100, # dot size\n",
    "            alpha=0.8)  # dot transparency\n",
    "\n",
    "        # make a label for each word\n",
    "        texts.append(plt.text(xx, yy, txt))\n",
    "\n",
    "# Code to help with overlapping labels -- may take a minute to run\n",
    "adjust_text(texts, force_points=0.2, force_text=.7, \n",
    "                    expand_points=(1, 1), expand_text=(1, 1),\n",
    "                    arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"What words were most similar to ''\" + keyword1 + \"' in Congress?\", fontsize=20, fontweight=0, color='Red')\n",
    "plt.xlabel(\"period\")\n",
    "plt.ylabel(\"similarity to \" + keyword1)\n",
    "\n",
    "\n",
    "filename = 'words-similar-to-' + keyword1 + '-' + dataname\n",
    "plt.savefig(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
