{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For citation information, please see the \"Source Information\" section listed in the associated README file: https://github.com/stephbuon/digital-history/tree/master/hist3368-week12-word-context-vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hist 3368 - Week 12: Word Context Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Jo Guldi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Word Vectors to Word Context Vectors\n",
    "\n",
    "In previous notebooks, you've used word count vectors to compare the words most distinctive of companies and time periods, and to measure the abstract \"distance\" between different entities.\n",
    "\n",
    "In our reading, we've learned that many scholars applied word vectors to understanding intellectual history. Word vectors can help you to understand the changing profile of every word -- how its \"context\" was different in 1920 than in 1980.  For example, the word \"gay\" meant \"happy\" in 1920, but by 1990 it had come to mean \"homosexual.\"  Word context is the study of the changing words that surrounded \"gay\" in both instances.\n",
    "\n",
    "Wordcount vectors can get you to changing word context. To perform such an investigation, you need to structure the data so that there is one vector for each word in every period of time.  This is called a “word context vector”. \n",
    "\n",
    "In this week's notebook, we'll make word context vectors for some words in Congress. We'll go through the following steps:\n",
    "* First, we'll organize the data so that we have a dataframe of one word per row, where another column gives the sentence in which the word appears in Congress and the date.\n",
    "* Then, we'll \"groupby\" keyword and period, so that we have an index for each keyword and period (for instance, \"woman-1985\") and a \"context\" column with every word next to which \"woman\" appears in the year 1985.\n",
    "* Next, we'll use the word vector tools you already know -- SKLEARN's Countvectorizer(), .fit_transform -- to make vectors from this data.\n",
    "* We'll use a measurement tool you already know -- cosine distance -- to compare context vectors for \"woman\" from 1985 to 1995 and 2005\n",
    "* We'll use a comparison tool you already know -- vector subtraction -- to create a \"gender difference vectors\" whose low scores show words more likely to show up in the context of \"woman\" and whose high scores show words more likely to show up in the context of \"man\"\n",
    "* Later in the notebook, we'll return to a 'word embedding' software package that uses high-dimensional math and hidden layers to make whip-fast vectors.\n",
    "\n",
    "#### Word Vectors vs. Word Embeddings \n",
    "\n",
    "Wordcount vectors are just what we’ve looked at: a simple count of words, with one integer per every word.  Wordcount embeddings are similar. But they typically add one more row of data or more per document.  That might mean that there’s a count of how many nouns, verbs, or adjectives there are per document. That might mean that there’s a count of bigrams, trigrams, fourgrams, or more – or multi-word phrases, plus or minus a word, called a “skipgram.”  These “hidden layers” in word embedding models mean an even richer model of which documents are like other documents. Because they factor in grammar and sentence structure as well as lexicon, they produce models that are very good at matching rhetorical style in text, and getting at the nuances of grammatical meaning. That is to say, they’re good at noticing when you mean “apple” the fruit (which you might eat or make into pie) or “apple” the computer (which you might turn on or off).  \n",
    "\n",
    "Functionally, you use word embeddings just the way you use wordcount vectors. You can measure the distance between them, just like we did in our notebook this week.  You can subtract them, just as we did, to get a litmus test of what’s different between two periods of time, or which words are used to signify masculinity and femininity.  \n",
    "\n",
    "*In the first half of this notebook,* we'll stick with \"word vectors,\" not embeddings.  We'll word context vectors 'by hand' -- i.e., using onyl SKLEARN's CountVectorizer() and .fit_transform + cosine distance and subtraction.  Doing it this way is slower than loading some other packages that have been built specifically for working with large-scale wordcount vectors, where the code is packaged with high-dimensional math designed to make the comparisons run faster. We're doing it this way, however, so that you can really see for yourself how a word vector is built and what's inside it at every moment.   When we structure the data, build the vectors, subtract and measure the distance between vectors, we'll be able to inspect what's in the vector at every turn. You'd be able to do the math yourself if you looked more carefully.\n",
    "\n",
    "*In the second half of the code,* however, we'll use the GENSIM package of word embeddings to work on a larger-scale sample of debates. We'll use GENSIM's pre-built tools to do analysis comparative to what you did with cosine distance and vector subtraction:\n",
    "\n",
    "     wv.vocab - which allows you to inspect the words in a vector \n",
    "     wv.most_similar() - which allows you to call up vectors from your dataset that are most similar to a given word\n",
    "\n",
    "#### Skill Building for Historical Analysis\n",
    "\n",
    "By the end of this notebook, you'll know how to replicate most of the fancy work with vectors in the reading.  You'll be able to:\n",
    "* use word context vectors to analyze the intellectual history of concept words like \"freedom,\" \"gay\", or \"woman,\" detecting how their context changed from moment to moment\n",
    "* visualize changes to word concepts as a dendrogram\n",
    "* use GENSIM's \"most_similar()\" to generate a list of the words most similar to any concept (for instance \"freedom\") at different moments over time\n",
    "* visualize changes to the context of an individual word over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import glob\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy.spatial.distance\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines load some data from Congress. Don't worry too much about the commands within this block; we're more interested in the transformations we'll apply to the data after it's loaded.  If you're curious, the lines below download two separate dataframes --  \"speeches\" and \"descriptions\" -- and then merge them  so that we now have one database of speeches with the date on which they were spoken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6qCg0mXrtOD1",
    "outputId": "4ecca950-9419-4b8d-96fc-aa5fbb1426f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_100.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_101.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_102.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_103.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_104.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_105.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_106.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_107.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_108.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_109.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_110.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_111.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_100.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_101.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_102.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_103.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_104.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_105.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_106.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_107.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_108.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_109.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_110.txt...\n",
      "Reading /scratch/group/oit_research_data/stanford_congress/hein-bound/descr_111.txt...\n"
     ]
    }
   ],
   "source": [
    "all_speech_files = glob.glob('/scratch/group/oit_research_data/stanford_congress/hein-bound/speeches_*.txt')\n",
    "CONGRESS_MIN_THRESHOLD = 100\n",
    "CONGRESS_MAX_THRESHOLD = 115\n",
    "\n",
    "speech_files = []\n",
    "\n",
    "for fn in all_speech_files:\n",
    "    number = int(fn.rsplit('_', 1)[-1].split('.')[0])\n",
    "    if CONGRESS_MIN_THRESHOLD <= number <= CONGRESS_MAX_THRESHOLD:\n",
    "        speech_files.append(fn)\n",
    "\n",
    "speech_files.sort()\n",
    "        \n",
    "def parse_one(fn):\n",
    "    print(f'Reading {fn}...')\n",
    "    return pd.read_csv(fn, sep='|', encoding=\"ISO-8859-1\", error_bad_lines=False, warn_bad_lines=False, quoting=csv.QUOTE_NONE)\n",
    "\n",
    "speeches_df = pd.concat((parse_one(fn) for fn in speech_files))\n",
    "speeches_df.dropna(how='any', inplace=True)\n",
    "\n",
    "all_description_files = glob.glob('/scratch/group/oit_research_data/stanford_congress/hein-bound/descr_*.txt')\n",
    "                                  \n",
    "description_files = []\n",
    "\n",
    "for fn in all_description_files:\n",
    "    number = int(fn.rsplit('_', 1)[-1].split('.')[0])\n",
    "    if CONGRESS_MIN_THRESHOLD <= number <= CONGRESS_MAX_THRESHOLD:\n",
    "        description_files.append(fn)\n",
    "        description_files.sort()\n",
    "        \n",
    "description_df = pd.concat((parse_one(fn) for fn in description_files))\n",
    "\n",
    "all_data = pd.merge(speeches_df, description_df, on = 'speech_id')\n",
    "all_data.fillna(0, inplace=True)\n",
    "all_data = all_data.drop(['chamber', 'speech_id', 'number_within_file', 'speaker', 'first_name'], 1)\n",
    "all_data = all_data.drop(['last_name', 'state', 'gender', 'line_start', 'line_end', 'file', 'char_count', 'word_count'], 1)\n",
    "all_data['date']=pd.to_datetime(all_data['date'],format='%Y%m%d')\n",
    "all_data['year'] = pd.to_datetime(all_data['date']).dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['5yrperiod'] = np.floor(all_data['year'] / 5) * 5 # round each year to the nearest 5 -- by dividing by 5 and \"flooring\" to the lowest integer\n",
    "all_data = all_data.drop(['date', 'year'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data['index'] = np.arange(len(all_data)) # create an 'index' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "atQkz5HhtOFo",
    "outputId": "cff96380-af39-4402-a9c2-a5c97c547fc5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>5yrperiod</th>\n",
       "      <th>index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Chair would also like to state that Repres...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The quor closes that 426 Represe have answered...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Clerk credentials regular in for received ...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The next order of business is the election of ...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              speech  5yrperiod  index\n",
       "0  Representativeselect to the 100th Congress. th...     1985.0      0\n",
       "1  The Chair would also like to state that Repres...     1985.0      1\n",
       "2  The quor closes that 426 Represe have answered...     1985.0      2\n",
       "3  The Clerk credentials regular in for received ...     1985.0      3\n",
       "4  The next order of business is the election of ...     1985.0      4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, the first pass, we're going to do some memory-intensive work on the computer by creating word context vectors 'by hand' -- i.e., using onyl SKLEARN's CountVectorizer() and .fit_transform + cosine distance and subtraction.  \n",
    "\n",
    "Doing it this way is slower than loading some other packages that have been built specifically for working with large-scale wordcount vectors, where the code is packaged with high-dimensional math designed to make the comparisons run faster.\n",
    "\n",
    "We're doing it this way, however, so that you can really see for yourself how a word vector is built and what's inside it at every moment.   \n",
    "\n",
    "When we structure the data, build the vectors, subtract and measure the distance between vectors, we'll be able to inspect what's in the vector at every turn. You'd be able to do the math yourself if you looked more carefully.\n",
    "\n",
    "Later in the notebook, we'll return to a 'word embedding' software package that uses high-dimensional math and hidden layers to make whip-fast vectors.\n",
    "\n",
    "However, as we're doing old-fashioned vectors by hand, it'll go best if we \"downsample\" the data, taking a random sample of 5000 sentences spoken in Congress between 1985-2010."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some downsamples so we don't break the computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_l = all_data.sample(500000)\n",
    "sample_m = sample_l.sample(50000)\n",
    "sample = sample_m.sample(5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make a word context vector dataframe.\n",
    "\n",
    "First, we'll break up the data so that we have one row per every sentence.\n",
    "\n",
    "Then, we'll break up the data so that we have one row per every word -- and a column with the 'sentence' where the word was originally found.\n",
    "\n",
    "This will tell us about the context of the word.\n",
    "\n",
    "We'll retain information about the '5yrperiod' when the word was originally from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break data into sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a handy script for breaking up strings into sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>representativeselect to the 100th congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>this being the day fixed by the 20th amendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>the clerk of the house has prepared the offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>certificates of election covering the 435 sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>and the names of those persons whose credenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260445</th>\n",
       "      <td>2643100</td>\n",
       "      <td>build local infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260446</th>\n",
       "      <td>2643100</td>\n",
       "      <td>and save local property tax dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260447</th>\n",
       "      <td>2643100</td>\n",
       "      <td>i voted against my own pay raise each time it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260448</th>\n",
       "      <td>2643100</td>\n",
       "      <td>and donated my raise to local nonprofit organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260449</th>\n",
       "      <td>2643100</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59260450 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            index                                           sentence\n",
       "0               0         representativeselect to the 100th congress\n",
       "1               0   this being the day fixed by the 20th amendmen...\n",
       "2               0   the clerk of the house has prepared the offic...\n",
       "3               0   certificates of election covering the 435 sea...\n",
       "4               0   and the names of those persons whose credenti...\n",
       "...           ...                                                ...\n",
       "59260445  2643100                         build local infrastructure\n",
       "59260446  2643100                and save local property tax dollars\n",
       "59260447  2643100   i voted against my own pay raise each time it...\n",
       "59260448  2643100   and donated my raise to local nonprofit organ...\n",
       "59260449  2643100                                                   \n",
       "\n",
       "[59260450 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_df = all_data.copy() #.set_index('index')\n",
    "sentences_df =pd.concat([sentences_df['speech'].str.split('.').explode()],axis=1).reset_index() #explode the data \n",
    "sentences_df = sentences_df.rename({'speech' : 'sentence'}, axis = 1) # rename the column \"sentence\" to \"keyword\"\n",
    "sentences_df['sentence'] = sentences_df['sentence'].str.lower().copy()\n",
    "sentences_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's annotate the sentences with the original information about period and speech from all_data -- which we can do by \"merging\" the two dataframes, using the \"index\" column as a common referent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>5yrperiod</th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>0</td>\n",
       "      <td>representativeselect to the 100th congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>0</td>\n",
       "      <td>this being the day fixed by the 20th amendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>0</td>\n",
       "      <td>the clerk of the house has prepared the offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>0</td>\n",
       "      <td>certificates of election covering the 435 sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>0</td>\n",
       "      <td>and the names of those persons whose credenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260445</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2643100</td>\n",
       "      <td>build local infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260446</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2643100</td>\n",
       "      <td>and save local property tax dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260447</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2643100</td>\n",
       "      <td>i voted against my own pay raise each time it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260448</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2643100</td>\n",
       "      <td>and donated my raise to local nonprofit organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260449</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>2643100</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59260450 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     speech  5yrperiod  \\\n",
       "0         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "1         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "2         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "3         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "4         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "...                                                     ...        ...   \n",
       "59260445  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260446  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260447  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260448  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260449  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "\n",
       "            index                                           sentence  \n",
       "0               0         representativeselect to the 100th congress  \n",
       "1               0   this being the day fixed by the 20th amendmen...  \n",
       "2               0   the clerk of the house has prepared the offic...  \n",
       "3               0   certificates of election covering the 435 sea...  \n",
       "4               0   and the names of those persons whose credenti...  \n",
       "...           ...                                                ...  \n",
       "59260445  2643100                         build local infrastructure  \n",
       "59260446  2643100                and save local property tax dollars  \n",
       "59260447  2643100   i voted against my own pay raise each time it...  \n",
       "59260448  2643100   and donated my raise to local nonprofit organ...  \n",
       "59260449  2643100                                                     \n",
       "\n",
       "[59260450 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_context_sentences = pd.merge(all_data, sentences_df, on=\"index\")  # merge \n",
    "word_context_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To work with word vectors, as you'll recall, we need our data broken up into sentences and words in a list of lists.  Above, we have a column that is composed of sentences; now we need to break them up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = sentences_df['sentence'].str.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break sentences into words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some code for breaking up sentences into words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's create a new 'index' column with a number for every unique sentence with \n",
    "    \n",
    "        np.arange(len(index_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speech</th>\n",
       "      <th>5yrperiod</th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>0</td>\n",
       "      <td>representativeselect to the 100th congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>1</td>\n",
       "      <td>this being the day fixed by the 20th amendmen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>2</td>\n",
       "      <td>the clerk of the house has prepared the offic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>3</td>\n",
       "      <td>certificates of election covering the 435 sea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Representativeselect to the 100th Congress. th...</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>4</td>\n",
       "      <td>and the names of those persons whose credenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260445</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>59260445</td>\n",
       "      <td>build local infrastructure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260446</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>59260446</td>\n",
       "      <td>and save local property tax dollars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260447</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>59260447</td>\n",
       "      <td>i voted against my own pay raise each time it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260448</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>59260448</td>\n",
       "      <td>and donated my raise to local nonprofit organ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59260449</th>\n",
       "      <td>Madam Speaker. I wold like to submit the follo...</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>59260449</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59260450 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     speech  5yrperiod  \\\n",
       "0         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "1         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "2         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "3         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "4         Representativeselect to the 100th Congress. th...     1985.0   \n",
       "...                                                     ...        ...   \n",
       "59260445  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260446  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260447  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260448  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "59260449  Madam Speaker. I wold like to submit the follo...     2010.0   \n",
       "\n",
       "             index                                           sentence  \n",
       "0                0         representativeselect to the 100th congress  \n",
       "1                1   this being the day fixed by the 20th amendmen...  \n",
       "2                2   the clerk of the house has prepared the offic...  \n",
       "3                3   certificates of election covering the 435 sea...  \n",
       "4                4   and the names of those persons whose credenti...  \n",
       "...            ...                                                ...  \n",
       "59260445  59260445                         build local infrastructure  \n",
       "59260446  59260446                and save local property tax dollars  \n",
       "59260447  59260447   i voted against my own pay raise each time it...  \n",
       "59260448  59260448   and donated my raise to local nonprofit organ...  \n",
       "59260449  59260449                                                     \n",
       "\n",
       "[59260450 rows x 4 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_context = word_context_sentences.copy()\n",
    "index_context['index'] = np.arange(len(index_context)) # create an 'index' column\n",
    "index_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use str.split() and \n",
    "\n",
    "    .explode()\n",
    "\n",
    "to create a dataframe with one word per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_per_row = index_context.set_index('index')\n",
    "word_per_row =pd.concat([word_per_row['sentence'].str.split(' ').explode()],axis=1).reset_index() #explode the data \n",
    "word_per_row = word_per_row.rename({'sentence' : 'keyword'}, axis = 1) # rename the column \"sentence\" to \"keyword\"\n",
    "word_per_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the two dataframes to create a well-annotated dataframe of every word and its context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_words = pd.merge(word_per_row, index_context, on=\"index\") # merge the two df's\n",
    "word_context_words = word_context_words.drop('index', 1) # get rid of the index column because we don't need it any more\n",
    "word_context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_words['keyword'] = word_context_words['keyword'].str.strip() # strip the whitespace\n",
    "word_context_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use \n",
    "\n",
    "    .size()\n",
    "    \n",
    "with .groupby() to get the word counts per time.  We can also call\n",
    "\n",
    "    .to_frame()\n",
    "\n",
    "to tell pandas what to name the new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_period = word_context_words.groupby(['keyword', '5yrperiod']).size().to_frame('count')\n",
    "words_per_period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groupby Word and Period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new data structure where we group by keyword AND period.\n",
    "\n",
    "If we organize our data this way, we will preserve information about the context for how each word was spoken about, across all companies, in 1994, 2011, etc.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Technically, we could groupby('keyword', '5yrperiod'). However, later on, we're going to want to call vectors of data by an index that references both word and period. So it's better if we just create a new column for the data called 'wpord-period,' and groupby() that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_word_period = word_context_words.copy()\n",
    "word_context_word_period['word-period'] = word_context_word_period['keyword'] + \"-\" + word_context_word_period['5yrperiod'].astype(str)\n",
    "word_context_word_period = word_context_word_period.drop(['5yrperiod', 'keyword'], 1)\n",
    "word_context_word_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_grouped = word_context_word_period.groupby(['word-period']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_grouped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this new output, the 'context' column has all the words from all the sentences that contain the 'keyword' of that row in a given period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_context_grouped.filter(like = 'woman', axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to see how 'woman' changed its meaning in Congress from 1985 to 2005."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Word Context Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=10000, lowercase=True, ngram_range=(1, 1), analyzer = \"word\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we feed the vectorizer the column 'sentence' because we want to model the CONTEXT in which each keyword appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = vectorizer.fit_transform(word_context_grouped['sentence'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the vectors as a dataframe where every column is a word and every row a period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_words = np.array(vectorizer.get_feature_names())\n",
    "context_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_period = list(word_context_grouped.axes[0].to_numpy())\n",
    "word_period[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors_dataframe = pd.DataFrame(vectorized.todense(), # the matrix we saw above is turned into a dataframe\n",
    "                                 columns=context_words,\n",
    "                                 index = word_period\n",
    "                                 )\n",
    "vectors_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matr = vectorized.todense()\n",
    "matr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking measurements of vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our last exercise, we measured how different individual words were from each other.\n",
    "\n",
    "Let's do it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_1985_vector = vectors_dataframe.filter(regex = ('^woman-1985'), axis=0) # the caret (^) means 'begins with'\n",
    "woman_1985_vector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_1990_vector = vectors_dataframe.filter(regex = '^woman-1990', axis=0)\n",
    "woman_1995_vector = vectors_dataframe.filter(regex = '^woman-1995', axis=0)\n",
    "woman_2000_vector = vectors_dataframe.filter(regex = '^woman-2000', axis=0)\n",
    "woman_2005_vector = vectors_dataframe.filter(regex = '^woman-2005', axis=0)\n",
    "woman_1990_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors_dataframe is nice to use because its rows and columns are nicely labeled.  It's easy to call exactly the keyword-period combination you want.  \n",
    "\n",
    "You can use the rows directly pulled from vectors_dataframe as the basis for calculating cosine distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.spatial.distance.cosine(woman_1985_vector, woman_1990_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.spatial.distance.cosine(woman_1985_vector, woman_1995_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.spatial.distance.cosine(woman_1985_vector, woman_2000_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.spatial.distance.cosine(woman_1985_vector, woman_2005_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Subtracting vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we called the rows of vectors_dataframe directly to calculate cosine distances.  \n",
    "\n",
    "We can use these same vectors to execute a subtraction -- with a bit of reformatting.\n",
    "\n",
    "First, we \"transmute\" them from a horizontal row of values to a vertical row of values with \n",
    "\n",
    "    .T\n",
    "\n",
    "\n",
    "Next, we call the columns with the values by name, e.g.: \n",
    "\n",
    "    ['woman-1985.0']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = woman_1995_vector.T['woman-1995.0'] - woman_1985_vector.T['woman-1985.0']\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We \"sort\" the values from small to big using:\n",
    "\n",
    "    .sort_values()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: use \n",
    "    \n",
    "    .dropna() \n",
    "    \n",
    "to get rid of NaN's (not a number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff.dropna().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the words that changed the most in the context of 'woman' between 1985 and 1995."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare words used for 'man' and 'woman'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a vector that contains all the references to women."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = ['woman','women','\\bshe','\\bher','\\bhers','girl']\n",
    "woman_vector = vectors_dataframe.loc[[x for x in vectors_dataframe.index for word in pattern if word in x]]\n",
    "\n",
    "woman_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the data we want, all right. But it'd be more useful as a matrix where all the columns are added together. \n",
    "\n",
    "That's easy to do with \n",
    "\n",
    "    .sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_vector = woman_vector.sum()\n",
    "woman_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect! Now let's look for men"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "pattern2 = ['\\bhe\\b','\\bhim','\\bhis','\\bman\\b','\\bmen\\b','boy\\b','boys']\n",
    "man_vector = vectors_dataframe.loc[[x for x in vectors_dataframe.index for word in pattern2 if word in x]]\n",
    "man_vector = man_vector.sum()\n",
    "man_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_diff_vector = man_vector - woman_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_diff_vector.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of gender_diff_vector.sort_values() is predictive of the words most likely to refer to women (the negatives) and the words most likely to indicate men (the higher positives.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the code, we're shifting from word vectors made with SKLEARN to word \"embeddings\" made with the GENSIM package.\n",
    "\n",
    "GENSIM uses higher-level math to condense the matrices, meaning that we'll be able to deal with more information than the downsized sample above. Word embeddings like GENSIM also typically have a \"hidden layer\" of modeling which includes information about word order and part-of-speech, designed to make the word vectors more accurate models of the way that words are used in sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample the data and create data structure (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a larger sample than we did last time. We'll need to break it into sentences and words again, and group by the features of the data that we care about -- keyword and period -- again.\n",
    "\n",
    "Because you've seen the instructions above, we'll skip them below and just give the code.\n",
    "\n",
    "NOTE: the lines below may take a while. Splitting sentences and words can be intensive on a dataset of this scale. If it's not working for you, try sample_m or sample where you see sample_l in the first line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_m = make_sentences(all_data['speech']).copy() # <---- switch out sample_l to sample_s or sample_m here\n",
    "word_context_sentences_m = pd.concat([pd.DataFrame({'sentence': speech, '5yrperiod': row['5yrperiod']}, index=[0]) \n",
    "           for _, row in sample.iterrows() \n",
    "           for speech in row['speech'].split('.') if speech != ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_m[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to model a larger set of data in Congress from 1985 to 2005 with the help of GENSIM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Break sentences into words (for later use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the code for breaking up sentences into words. We'll need words_per_period later in the code. You've seen the detailed code before, so here's the quick version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_context2 = word_context_sentences_m.copy()\n",
    "index_context2['index'] = np.arange(len(index_context2)) # create an 'index' column\n",
    "word_per_row2 = index_context2.set_index('index')\n",
    "word_per_row2 =pd.concat([word_per_row2['sentence'].str.split(' ').explode()],axis=1).reset_index() #explode the data \n",
    "word_per_row2 = word_per_row2.rename({'sentence' : 'keyword'}, axis = 1) # rename the column \"sentence\" to \"keyword\"\n",
    "word_context_words2 = pd.merge(word_per_row2, index_context2, on=\"index\") # merge the two df's\n",
    "word_context_words2 = word_context_words2.drop('index', 1) # get rid of the index column because we don't need it any more\n",
    "word_context_words2['keyword'] = word_context_words2['keyword'].str.strip() # strip the whitespace\n",
    "words_per_period2 = word_context_words2.groupby(['keyword', '5yrperiod']).size().to_frame('count')\n",
    "words_per_period2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up GENSIM\n",
    "\n",
    "The first step is to \"train\" the GENSIM model with the function `gensim.models.Word2Vec()`. This function has a couple dozen parameters, some of which are more important than others.\n",
    "\n",
    "Here are a few major ones. Only two are MANDATORY: these are marked with an asterisk:\n",
    "\n",
    "1. `sentences*`: This is where you provide your data. It must be in a format of iterable of iterables.\n",
    "2. `sg`: Your choice of training algorithm. There are two standard ways of training W2V vectors -- 'skipgram' and 'CBOW'. If you enter 1 here the skip-gram is applied; otherwise, the default is CBOW.\n",
    "3. `size*`: This is the length of your resulting word vectors. If you have a large corpus (>few billion tokens) you can go up to 100-300 dimensions. Generally word vectors with more dimensions give better results.\n",
    "4. `window`: This is the window of context words you are training on. In other words, how many words come before and after your given word. A good number is 4 here but this can vary depending on what you are interested in. For instance, if you are more interested in embeddings that embody semantic meaning, smaller window sizes work better. \n",
    "5. `alpha`: The learning rate of your model. If you are interested in machine learning experimentation with your vectors you may experiment with this parameter.\n",
    "6. `seed` (int): This is the random seed for your random initialization. All deep learning models initialize the weights with random floats before training. This is a useful field if you want to replicate your experiments because giving this a seed will initialize 'randomly' deterministically.\n",
    "7. `min_count`: This is the minimum frequency threshold. If a given word appears with lower frequency than provided it will be ignored. This is here because words with very low frequency are hard to train.\n",
    "8. `iter`: This is the number of iterations(entire run) over the corpus, also known as epochs. Usually anything between 1-10 is ok. The trade offs are that if you have higher iterations, it will take longer to train and the model may overfit on your dataset. However, longer training will allow your vectors to perform better on tasks relevant to your dataset.\n",
    "\n",
    "Most of these settings will not concern us. As you'll see below, we are only going to use four arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model = gensim.models.Word2Vec(\n",
    "    sentences = sentences_m,\n",
    "    min_count = 2, # remove words stated only once\n",
    "    size = 100) # size of neuralnet layers; default is 100; higher for larger corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also save our model in case we want to use it again in a later session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.save('congress_model')\n",
    "# congress_model = gensim.models.Word2Vec.load('congress_model') # to load a saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can load a model in the same way (remember this from our topic model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model = gensim.models.Word2Vec.load('congress_model') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `wv.index2word` allows us to see the words in our model (but careful! congress_model.wv.vocab will print out every word in the corpus -- a very long list!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.index2word[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model itself is -- like the SKLEARN CountVectors model -- a matrix of vectors. Every row corresponds to the counts for one word. We can call the entire matrix or call up one row at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the fourth row of the model, represented as a word and as a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = congress_model.wv.index2word[3]\n",
    "word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.vectors[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting Word Context with the GENSIM model, one word at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GENSIM model has all sorts of tools built in for navigating and inspecting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the word context vector for any individual word by using:\n",
    "\n",
    "    model.wv['word']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the words with the highest counts in the context vector for 'man'. In other words, these are words that appear most commonly around 'man' in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "man_vector = congress_model.wv['man']\n",
    "congress_model.wv.similar_by_vector(man_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "woman_vector = congress_model.wv['woman']\n",
    "congress_model.wv.similar_by_vector(woman_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_vector = congress_model.wv['individual']\n",
    "congress_model.wv.similar_by_vector(individual_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soldier_vector = congress_model.wv['soldier']\n",
    "congress_model.wv.similar_by_vector(soldier_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance and Similarity with Vectors in GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarity is cosine similarity -- it's 1 minus cosine distance.  You've used cosine distance before -- you're a whiz with cosine distance already. \n",
    "\n",
    "With similarity, the higher the number, the more alike two terms are in the context in which they are used. \n",
    "\n",
    "When we used cosine distance before, we were doing it one vector at a time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.similarity('women', 'men')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What other words have similar context vectors?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the beauty of the GENSIM package is that it has pre-run all the word vectors for you. So it can call up the most similar word context vectors to the word context vector of any word, using the command, 'most_similar()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the GENSIM documentation: \"This method computes cosine similarity between a simple mean of the projection weight vectors of the given words and the vectors for each word in the model.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"women\", topn = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. So, according to our model, women are like men and individuals and soldiers; they're also like students and parents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting vector similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we get carried away, remember that these results come from a *different* mode of analysis than the CONTEXT VECTOR above.  The results here don't indicate that the words \"individuals\" or \"soldiers\" regularly occur in sentneces with the word \"women.\"  \n",
    "\n",
    "Instead, the model indicates that \"individuals\" and \"soldiers\" are often talked about with the same words that men and women are talked about.  They have employers, wages, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the word context vectors that are most similar to 'men'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"men\", topn = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find that men are spoken about almost in entirely the same context as women. But if women are spoken about in the same context as children, men are spoken about slightly more often in the same context as their homes. (what you see may vary with a different sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remember**: everything the model knows it knows from our corpus. What we're learning are assumptions *immanent* to the corpus.  These aren't FACTS about women or men -- these are data about how women and men were spoken about in Congress, 1985-2005.\n",
    "\n",
    "Both `word2vec` and our model have limitations.\n",
    "\n",
    "Additionally, our training set is selective and small (just a subset of some debates about the environment). Therefore, our analogies can return some wild cards. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"america\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. America is spoken about like freedom, like Iraq, and like the world. It's in a downturn, and when we speak of America, we speak of the same contexts in which we invoke democracy, drugs, and the interests of different peoples, especially workers. (what you see may vary with a different sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try your own hand at interpreting these outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"iraq\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you interpret these similarities?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_model.wv.most_similar(\"britain\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the similarities\n",
    "\n",
    "You'll recall that in Sarah Connell's blog entry, researchers produced a \"dendrogram\" of words related to other words, which we learned was created on the basis of cosine distance scores between word vectors.\n",
    "\n",
    "This dendrogram was used to compare the meaning of \"freedom\" in the seventeenth century (when the word was nearest in meaning to \"friendship\") to the meaning of \"freedom\" in the eighteenth century (when the word became associated with nations and patriotism).\n",
    "\n",
    "Let's see if we can make a dendrogram of words for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['dream', 'bombing', 'warfare', 'racism', 'prosperity', 'wealth', 'happiness', 'today', 'tomorrow', 'past', 'present', 'future', 'america', 'france', 'britain', 'iraq', 'china', 'democratic', 'dictator', 'totalitarian', 'democracy', 'welfare', 'socialism', 'communism', 'russia', 'congress', 'debate', 'hearing', 'protest']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: if you get an error because any of the words above aren't in your sample corpus, edit the list and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_vectors = congress_model.wv[keywords]\n",
    "keyword_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \n",
    "\n",
    "    linkage()\n",
    "    \n",
    "command performs hierarchical clustering -- in other words, it takes the Euclidean similarity score between any two vectors, and then ranks them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "links = linkage(keyword_vectors, method='complete', metric='seuclidean')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ranking gives us a read of which vectors are closest to which vectors.  We can visualize it using matplotlib and the \"dendrogram\" command from SKLEARN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "l = links\n",
    "\n",
    "# calculate full dendrogram\n",
    "plt.figure(figsize=(25, 10))\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.ylabel('word')\n",
    "plt.xlabel('distance')\n",
    "\n",
    "dendrogram(\n",
    "    l,\n",
    "    leaf_rotation=0,  # rotates the x axis labels\n",
    "    leaf_font_size=16,  # font size for the x axis labels\n",
    "    orientation='left',\n",
    "    leaf_label_func=lambda v: str(keywords[v])\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a little tweaking, you can create a list of only those vectors for the words most of interest to you, using GENSIM to visualize their similarity to each other in the corpus.\n",
    "\n",
    "You could even -- like Connell's blog entry indicates -- create a separate dendrogram for 1985 and another for 2005, to see how these terms have changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subtracting Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll recall that we've used vector subtraction before.  Subtracting the context for \"woman\" from the context for \"man\" produces a vector of high scores for the words that only appear around \"man\" but not woman."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['man'] - congress_model.wv['woman']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = congress_model.wv['woman'] - congress_model.wv['man']\n",
    "congress_model.wv.similar_by_vector(diff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Abstract Relatedness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four words making up the analogy can be understood as points in space where each word represents a single point. These points represent words' relationships with one-another.\n",
    "\n",
    "Let's borrow more of Sinykin's code to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# change the figure's size here\n",
    "plt.figure(figsize=(5,5), dpi = 200)\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_pca_scatterplot(congress_model.wv, keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Truth be told, I don't love this visualization; it's visualizing abstract relationships that show the conceptual distance between different entities in the model. I present it to you as a cute toy, not as an approved visualization that i'd like to see in your work. \n",
    "\n",
    "Please use PCA analysis with care; it's almost impossible to get back to what it actually *means* -- at least without pairing it with other visualizations and measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Time with GENSIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might recall that there's a lot of data that we're not using, for instance, the 5yrperiod field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we make use of it?  How about a for loop?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periodnames = sample_m['5yrperiod'].unique().tolist()\n",
    "periodnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's decide what keyword we want to search for, and make it into a variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword1 = 'socialism'  # define the keyword you're looking for. you can change this variable as many times as you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take a while, since we're creating 6 different gensim models. Fortunately, we're saving all of them, so if you want to go back and run this for a different word later, you can just load the old data rather than running the whole thing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context = [] # create an empty dummy variable\n",
    "\n",
    "for period1 in periodnames:\n",
    "    period_data = sample_m[sample_m['5yrperiod'] == period1] # select one period at a time\n",
    "    print('mining ', period1)\n",
    "    sentences = make_sentences(sample_m['speech']).copy() # break data into sentences for that period only \n",
    "    ####### use this code for first run\n",
    "    #period_model = gensim.models.Word2Vec( # make a gensim model for that data\n",
    "    #    sentences = sentences,\n",
    "    #   min_count = 2, \n",
    "    #    size = 100)  \n",
    "    #period_model.save('model-' + str(period1)) # save the model with the name of the period\n",
    "    ########  after the first run, use this line to call the old data without generating it again\n",
    "    period_model = gensim.models.Word2Vec.load('model-' + str(period1)) # to load a saved model\n",
    "    ###########\n",
    "    keyword_context_period = period_model.wv.most_similar(keyword1, topn = 1000) # extract the context of how women were talked about in that period\n",
    "    keyword_context.append(keyword_context_period) # save the context of how women were talked about for later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output should be a list of context vectors for each period, which we can use to show how the context of the keyword was changing from period to period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context[0][0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_context[5][0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can grab just the names this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item[0] for item in keyword_context[1]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I can grab just the numbers for any given year (in this case, the second period -- 1990 -- [1]) this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[item[1] for item in keyword_context[1]][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's annotate the data with how many times the keyword was referred to over time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we made a nice dataframe of how many times each word appears over a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_period2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_per_period2 = words_per_period2.reset_index()\n",
    "words_per_period2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_per_year = words_per_period2[words_per_period2['keyword'] == 'freedom']\n",
    "keyword_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_per_year[keyword_per_year['5yrperiod'] == 1985.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a flattened list of all the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for i in range(6):\n",
    "    words = [item[0] for item in keyword_context[i]][:10]\n",
    "    all_words.append(words)\n",
    "\n",
    "all_words2 = []\n",
    "for list in all_words:\n",
    "    for word in list:\n",
    "        all_words2.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up the colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "from numpy import linspace\n",
    "colors = [ cm.jet(x) for x in linspace(.5, 2, 60) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dots and annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "#from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "\n",
    "from adjustText import adjust_text\n",
    "\n",
    "# change the figure's size here\n",
    "plt.figure(figsize=(10,10), dpi = 200)\n",
    "\n",
    "texts = []\n",
    "\n",
    "# plt.annotate only plots one label per iteration, so we have to use a for loop \n",
    "for i in range(len(periodnames)):    # cycle through the period names\n",
    "    \n",
    "    #yyy = int(keyword_per_year[keyword_per_year['5yrperiod'] == int(xx)]['count'])   # how many times was the keyword used that year?\n",
    "                     \n",
    "    for j in range(10):     # cycle through the first ten words (you can change this variable)\n",
    "        \n",
    "        xx = periodnames[i]        # on the x axis, plot the period name\n",
    "        yy = [item[1] for item in keyword_context[i]][j]         # on the y axis, plot the distance -- how closely the word is related to the keyword\n",
    "        txt = [item[0] for item in keyword_context[i]][j]        # grab the name of each collocated word\n",
    "        colorindex = all_words2.index(txt)                     # this command keeps all dots for the same word the same color\n",
    "        \n",
    "        plt.scatter(                                           # plot dots\n",
    "            xx, #x axis\n",
    "            yy, # y axis\n",
    "            linewidth=1, \n",
    "            color = colors[colorindex],\n",
    "            s = 10, # dot size\n",
    "            alpha=0.8)  # dot transparency\n",
    "\n",
    "        # make a label for each word\n",
    "        texts.append(plt.text(xx, yy, txt))\n",
    "\n",
    "# Code to help with overlapping labels -- may take a minute to run\n",
    "adjust_text(texts, force_points=0.2, force_text=0.2, \n",
    "                    expand_points=(1, 1), expand_text=(1, 1),\n",
    "                    arrowprops=dict(arrowstyle=\"-\", color='black', lw=0.5))\n",
    "\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "# Add titles\n",
    "plt.title(\"What words were most similar to ''\" + keyword1 + \"' in Congressional debates?\", fontsize=20, fontweight=0, color='Red')\n",
    "plt.xlabel(\"period\")\n",
    "plt.ylabel(\"similarity to \" + keyword1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we make a dendrogram of a long list of interesting words and their contextual similarity. \n",
    "\n",
    "We also use a list of periods to create separate GENSIM models for each period.\n",
    "\n",
    "You will put these together to create a dendrogram for 1985 and another dendrogram for 1995 and 2005. \n",
    "\n",
    "#### Coding exercise\n",
    "   * Create a list of keywords that you think would be particularly relevant for Congress during this time -- something that might demonstrate historical change in ideas.\n",
    "        * Using the code above, create a GENSIM model for 1985, 1995, and 2005\n",
    "        * Using the code above, create an array of vectors for your words for each time period\n",
    "        * Using the code above, draw a dendrogram of keyword relatedness for the three time periods.\n",
    "    \n",
    "#### Interpretation exercise    \n",
    "   * Write an interpretive paragraph of at least half a page examining what this dendrogram suggests about change over time.  If there's not enough material in your first experiment, tweak the keyword list and try again -- until you have something to say about history.\n",
    "\n",
    "\n",
    "Turn in your work on Canvas. Do not turn in an ipynb. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
