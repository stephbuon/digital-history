{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the 20C Hansard Debates on the Environment with word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this Notebook we will use a subset of the 20th-century Hansard Parliamentary Debates made up of just debates with the words, \"environment\", \"clean water\", \"clean air\" or \"climate\" in their title. While these debates do not form the entire corpus of Hansard's 20th-century debates on the environment, they can provide insight into how environmental discourse changed in parliament over time. \n",
    "\n",
    "The first 20th-century debate with a keyword in its title was in May of 1965. The last was in December of 1999. To analyze change over time, our corpus breaks this time range into two, 20(ish)-year periods: the 1960s through the 1970s, and the 1980s through the 1990s. \n",
    "\n",
    "From this corpus we will construct a \"model\"--that is, a data set trained to look for particular patterns--using `word2vec`. `word2vec` is used to produce word embeddings, where vocabulary is mapped to numbers. `word2vec`'s algorithm uses a neural network model to learn word associates from a large corpus of text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by importing our modules. `gensim` provides word2vec, while objects from `nltk` will help us tokenize the debate text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are also going to import `pprint` for \"pretty print,\" and write a function that will allow us to print clearer, shorter dictionaries, making their contents easier to inspect. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "\n",
    "def printDict(dict_object):\n",
    "    truncated_dict = dict(list(dict_object.items())[0:20])\n",
    "    pprint.pprint(truncated_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard: 1960s and 70s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in the Hansard data. Our first model will includes debates from the 1960s through the 1970s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/scratch/group/history/hist_3368-jguldi/hansard_c20/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path + \"hansard_c20_1960_70.txt\") as f:\n",
    "    debate_text_1960_70 = [line.rstrip('\\n') for line in f][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing the first 10 lines returns a list where each sentence is a separate character string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\"3. asked the Secretary of State for Education and Science what steps he is taking to ensure that the Natural Environment Research Council gives added emphasis to military oceanography, as stated in the Defence White Paper.\"',\n",
       " 'The Natural Environment Research Council is not responsible for research in oceanography directed specifically to military purposes.',\n",
       " '\"This is the responsibility of the Ministry of Defence, and the Defence White Paper refers to added emphasis being given to military oceanography by the Ministry of Defence.\"',\n",
       " '\"In the field of basic research, there will be a common interest and arrangements are therefore envisaged which will provide for representation of the Ministry on the appropriate Committees of the Research Council.\"',\n",
       " 'Is not the right hon.',\n",
       " '\"Gentleman now generally responsible for the National Institute of Oceanography and its work, and is it not self-evident that the reduced responsibility of the Ministry of Defence for this Council will tend to mean that less work and money will be available for it at the very moment when it needs it most to keep its research ship busy?\"',\n",
       " 'Will the right hon.',\n",
       " 'Gentleman watch this matter and give definite directions to the Council in these matters of co-ordination?',\n",
       " '\"It is quite true that I am now responsible for the Institute, but special arrangements have been made for the Ministry of Defence and the Institute to maintain the close association which they have always had in the past.\"',\n",
       " '\"Sir Graham Sutton, the new Chairman of the Natural Environment Research Council, has already had talks with the Minister responsible and the chief scientist to the Navy.\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debate_text_1960_70[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can borrow Sinykin's `make_sentences()` function to create a vocabulary on which to train our ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "def make_sentences(text):\n",
    "    preprocessed_text = []\n",
    "    for line in text:\n",
    "        lower_case = line.lower()\n",
    "        sentences = sent_tokenize(lower_case)\n",
    "        tokenized_sentences = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "        preprocessed_text += tokenized_sentences\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the structural output of `make_sentences()` in greater detail so we understand what we are giving word2vec to train. With this understanding, you can easily train your own model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1960_70 = make_sentences(debate_text_1960_70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['``', '3.', 'asked', 'the', 'secretary', 'of', 'state', 'for', 'education', 'and', 'science', 'what', 'steps', 'he', 'is', 'taking', 'to', 'ensure', 'that', 'the', 'natural', 'environment', 'research', 'council', 'gives', 'added', 'emphasis', 'to', 'military', 'oceanography', ',', 'as', 'stated', 'in', 'the', 'defence', 'white', 'paper', '.', \"''\"], ['the', 'natural', 'environment', 'research', 'council', 'is', 'not', 'responsible', 'for', 'research', 'in', 'oceanography', 'directed', 'specifically', 'to', 'military', 'purposes', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(sentences_1960_70[:2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sentences` is a list of lists. One large list holds many small lists, and within each small list are the tokens making up a sentence. These tokens will serve as the vocabulary words used to train our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_model_1960_70 = gensim.models.Word2Vec(\n",
    "    sentences_1960_70,\n",
    "    min_count = 2, # remove words stated only once\n",
    "    size = 200) # size of neuralnet layers; default is 100; higher for larger corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also save our model in case we want to use it again in a later session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_model_1960_70.save('hansard_model_1960_70')\n",
    "# hansard_model = gensim.models.Word2Vec.load('hansard_model') # to load a saved model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained word vectors are stored in a `KeyedVectors` instance named `model.wv`. Separating the trained vectors into `KeyedVectors` returns a much smaller and faster object than the whole data set. \n",
    "\n",
    "Let's use the `printDict()` function we defined earlier and inspect `hansard_model_1960_70`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'``': <gensim.models.keyedvectors.Vocab object at 0x2aaaae2df130>,\n",
      " 'and': <gensim.models.keyedvectors.Vocab object at 0x2aaacdc7ea00>,\n",
      " 'asked': <gensim.models.keyedvectors.Vocab object at 0x2aaaae0d6520>,\n",
      " 'education': <gensim.models.keyedvectors.Vocab object at 0x2aaacdc7e850>,\n",
      " 'ensure': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc70d0>,\n",
      " 'environment': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc7250>,\n",
      " 'for': <gensim.models.keyedvectors.Vocab object at 0x2aaacdc7e400>,\n",
      " 'he': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc7ac0>,\n",
      " 'is': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc7850>,\n",
      " 'natural': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc71f0>,\n",
      " 'of': <gensim.models.keyedvectors.Vocab object at 0x2aaacdc7ef10>,\n",
      " 'science': <gensim.models.keyedvectors.Vocab object at 0x2aaacdc7ed60>,\n",
      " 'secretary': <gensim.models.keyedvectors.Vocab object at 0x2aaacdc7efa0>,\n",
      " 'state': <gensim.models.keyedvectors.Vocab object at 0x2aaacdc7ed00>,\n",
      " 'steps': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc7a00>,\n",
      " 'taking': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc7100>,\n",
      " 'that': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc7160>,\n",
      " 'the': <gensim.models.keyedvectors.Vocab object at 0x2aaaae0d6220>,\n",
      " 'to': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc70a0>,\n",
      " 'what': <gensim.models.keyedvectors.Vocab object at 0x2aaacdcc7940>}\n"
     ]
    }
   ],
   "source": [
    "printDict(hansard_model_1960_70.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Similarity in Word Meaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can invoke the `most_similar()` method on our model to see which words word2vec has determined are most similar in meaning to one-another.\n",
    "\n",
    "The following code returns the top 10 words determined to be most similar to the word \"environment\" based on our training vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('protection', 0.7790396809577942),\n",
       " ('nature', 0.7429018616676331),\n",
       " ('environmental', 0.7369294166564941),\n",
       " ('research', 0.7196264863014221),\n",
       " ('responsibility', 0.7156100869178772),\n",
       " ('scotland', 0.7137181758880615),\n",
       " ('ministry', 0.7110477685928345),\n",
       " ('effects', 0.6995458602905273),\n",
       " ('world', 0.6984202265739441),\n",
       " ('law', 0.6951407790184021)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard_model_1960_70.wv.most_similar(\"environment\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare this yield with the words determined most similar to \"sewage.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('effluent', 0.9528419971466064),\n",
       " ('works', 0.9365886449813843),\n",
       " ('treatment', 0.9311678409576416),\n",
       " ('smoke', 0.9103373289108276),\n",
       " ('discharged', 0.9098536968231201),\n",
       " ('polluting', 0.8948899507522583),\n",
       " ('controlled', 0.8946700692176819),\n",
       " ('commercial', 0.8914747834205627),\n",
       " ('poisonous', 0.890518307685852),\n",
       " ('refuse', 0.8862885236740112)]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard_model_1960_70.wv.most_similar(\"sewage\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2vec` can also return analogies in a corpus. To determine similarity in word meaning through analogy, we can supply three words and `word2vec` will determine the last. \n",
    "\n",
    "The following arguments can be read like so: \"Britain is to smoke as sea is to X\" where \"X\" is determined by `word2vec`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hansard_model_1960_70.wv.most_similar(positive=['sea', 'smoke'], negative=['britain']) # remember we transformed all words to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oil: 0.7733\n"
     ]
    }
   ],
   "source": [
    "print(\"{}: {:.4f}\".format(*result[0])) # returns just the first result, formatted to read clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our model, Britain is to smoke as sea is to oil. Based on our training set, word2vec returns an analogy that makes sense: smoke is a pollutant of Britain as oil is a pollutant of water. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing our Analogy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The four words making up the analogy can be understood as points in space where each word represents a single point. These points represent words' relationships with one-another.\n",
    "\n",
    "Let's borrow more of Sinykin's code to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "def display_pca_scatterplot(model, words=None, sample=0):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.wv.vocab.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.wv.vocab ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-52-72619a12c7c4>:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  word_vectors = np.array([model[w] for w in words])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_pca_scatterplot(hansard_model_1960_70, ['britain','smoke','sea','oil']) # remember we transformed all words to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above plot, oil is closer in space to sea than Britain, indicating a meaningful relationship. Likewise, smoke is closer to Britain than sea. However, smoke is closer to oil than Britain, suggesting both smoke and oil share a relationship. We can confirm they do, as they are both described as pollutants in these debates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of our Model and word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `word2vec` and our model have limitations.\n",
    "\n",
    "`word2vec`'s analogies often make the most sense when the word usage of the training set reflects binary thinking. `word2vec` shines when returning results for analogies like \" King : Queen :: Man : X\", but has been noted to return confusing results for words with usages that are not generally assigned to a binary, like apple : green :: banana : X.\n",
    "\n",
    "Additionally, our training set is selective and small (just a subset of some debates about the environment). Therefore, our analogies can return some wild cards. \n",
    "\n",
    "The following analogy reads: \"water is to chemicals as air is to X.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = hansard_model_1960_70.wv.most_similar(positive=['air', 'chemicals'], negative=['water']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "substances: 0.8304\n",
      "dirty: 0.8231\n",
      "midlands: 0.8135\n",
      "pollutants: 0.8081\n",
      "coastal: 0.8078\n"
     ]
    }
   ],
   "source": [
    "for r in range(5):\n",
    "    print(\"{}: {:.4f}\".format(*result[r])) # returns just the first five results, formatted to read clearly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words \"substances\", \"dirty\", and \"pollutants\" makes sense, but without more context, \"midlands\" and \"coastal\" seem like they do not belong.\n",
    "\n",
    "While our training set has drawbacks, comparing results from different time periods can still be insightful. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hansard: 1980 to 1990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform these same operations on our other data set, debates from the 1980s through the 1990s, for the purpose of analyzing language change over time.\n",
    "\n",
    "Note: this model will take a little longer to process because our data set is slightly larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path + 'hansard_c20_1980_90.txt') as f:\n",
    "    debate_text_1980_90 = [line.rstrip('\\n') for line in f][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_1970_90 = make_sentences(debate_text_1980_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_model_1980_90 = gensim.models.Word2Vec(\n",
    "    sentences_1980_90,\n",
    "    min_count = 2, \n",
    "    size = 200, \n",
    "    workers = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worthy of further research is the observation that \"warming\" appears in `hansard_model_1980_90`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('environmental', 0.5770868062973022),\n",
       " ('consumer', 0.49735119938850403),\n",
       " ('warming', 0.48056459426879883),\n",
       " ('employment', 0.4719513952732086),\n",
       " ('science', 0.4586169719696045),\n",
       " ('birds', 0.4468916952610016),\n",
       " ('transport', 0.43481749296188354),\n",
       " ('nature', 0.4347519874572754),\n",
       " ('society', 0.4267409145832062),\n",
       " ('agriculture', 0.42662715911865234)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard_model_1980_90.wv.most_similar(\"environment\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`word2vec` determined that many of the top most words related to \"environment\" in the 1980s and 1990s had to do with technological growth (like \"employment\", \"society\", \"agriculture\", and \"science\"), compared to the 1960s through the 1970s, where top words included \"protection\", \"nature\", and \"responsibility.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('protection', 0.7790396809577942),\n",
       " ('nature', 0.7429018616676331),\n",
       " ('environmental', 0.7369294166564941),\n",
       " ('research', 0.7196264863014221),\n",
       " ('responsibility', 0.7156100869178772),\n",
       " ('scotland', 0.7137181758880615),\n",
       " ('ministry', 0.7110477685928345),\n",
       " ('effects', 0.6995458602905273),\n",
       " ('world', 0.6984202265739441),\n",
       " ('law', 0.6951407790184021)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard_model_1960_70.wv.most_similar(\"environment\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the top words determined to be most similar to \"Britain\" for the 1980s through the 1990s with the top words for the 1960s through the 1970s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('world', 0.6584057211875916),\n",
       " ('europe', 0.6561944484710693),\n",
       " ('poland', 0.6229649782180786),\n",
       " ('nation', 0.5565208792686462),\n",
       " ('region', 0.5420501232147217),\n",
       " ('population', 0.5408176779747009),\n",
       " ('uk', 0.5318464040756226),\n",
       " ('mankind', 0.5081990957260132),\n",
       " ('women', 0.5075250864028931),\n",
       " ('unemployment', 0.4989219903945923)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard_model_1980_90.wv.most_similar(\"britain\", topn = 10) # remember we transformed all words to lowercase "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('city', 0.8222301602363586),\n",
       " ('kew', 0.8148987889289856),\n",
       " ('increasing', 0.808451771736145),\n",
       " ('places', 0.8046635389328003),\n",
       " ('character', 0.8034182190895081),\n",
       " ('spread', 0.8029613494873047),\n",
       " ('growth', 0.7947773337364197),\n",
       " ('activity', 0.7877945899963379),\n",
       " ('potential', 0.7872251272201538),\n",
       " ('counties', 0.7849066257476807)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard_model_1960_70.wv.most_similar(\"britain\", topn = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results alone, it appears that earlier discourses explicitly discussed Britain's local growth, while later discourses about Britain were directed to global concerns. \n",
    "\n",
    "Analogies can also lend insight to discourses in the 1980s through the 1990s. The following analogy reads: \"shipyard is to ship as sea is to X.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sewage: 0.6663\n"
     ]
    }
   ],
   "source": [
    "result = hansard_model_1980_90.wv.most_similar(positive=['sea', 'ship'], negative=['shipyard'])\n",
    "\n",
    "print(\"{}: {:.4f}\".format(*result[0])) # this prints the top result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like ships how ships are found in a shipyard, sewage is found in the sea."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to use word2vec to determine which words are most similar in meaning to one-another. Remembering that our model is a subset of debates about the environment, it may be most insightful to try words like \"nature\" or \"countryside.\" \n",
    "\n",
    "Note, however, that your neural network might make different decisions that will yield different results than the model used by this Notebook, even if provided with the same analogy. One drawback to neural networks is that the decisions made to arrive at a model aren't easily traced. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
