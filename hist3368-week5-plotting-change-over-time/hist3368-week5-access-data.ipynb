{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For citation information, please see the \"Source Information\" section listed in the associated README file: https://github.com/stephbuon/digital-history/tree/master/hist3368-week5-plotting-change-over-time/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instructions for Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this Notebook is to provide access and insight to the many data sets available to this course. \n",
    "\n",
    "You do **NOT** need to read every section of this Notebook. Instead, you can use **command + F / control + F** to skip to the data that is relevant for your research. \n",
    "\n",
    "These are the section names for the data:\n",
    "- EDGAR Database\n",
    "- Hansard\n",
    "- US Congress\n",
    "- Reddit Archive\n",
    "- Dallas and Houston City Council Minutes\n",
    "- NovelTM Datasets for English-Language Fiction, 1700-2009 (preferred source for c19 Novels)\n",
    "- Project Gutenberg (backup source for c19 Novels)\n",
    "\n",
    "*Many of the datasets include exercises for applying what we have learned from previous lessons -- for example, word clouds, word count, etc.  You should study these examples.  Pay particular attention to the REDDIT example as it is the most developed.*\n",
    "\n",
    "#### Your First Job Today is to Successfully Load the Data\n",
    "\n",
    "Some datasets are large enough that they will require larger memory allocations for successful work. Pay attention to anything that makes the kernel crash. If you experience this, start a new session with more memory.  The notes below hold clues about how much memory you might need.\n",
    "\n",
    "#### Your Second Job Today is to Inspect Your Data.  What do the columns hold?\n",
    "\n",
    "The final assignment asks you to apply some of what you have learned about working with tabular data.  To execute this assignment, you will need to **inspect** your data to understand what the column names mean.  \n",
    "- Identify the *text* column in your dataset -- which is called 'speech' in some datasets, 'data' in others, 'body' in some, and 'text' in others.  \n",
    "- You will also need to identify the *contributor* column -- which may be listed as the 'speaker,' 'author,' or something else.  \n",
    "\n",
    "#### Your Third Job Today is to Apply What You Have Learned\n",
    "\n",
    "The final assigment asks you to find the longest pieces of text in your dataset, their contributors, and to count the words in the longest pieces of text.  You will need to borrow code from previous problem sets to make this work. \n",
    "\n",
    "To borrow code successfully, you will need to make sure that the names of the columns match -- for instance, if the name of your text column is 'body,' you will have to change the problem set code where it calls the column 'speech.'\n",
    "\n",
    "The basic commands you will use -- .count(), .replace(), .str.split().explode().dropna().value_counts() -- will be exactly the same for your assignment as they were in the previous problem sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "#import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDGAR Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[EDGAR](https://www.sec.gov/edgar.shtml) is a data base for the Securities and Exchange Commission. You can discover the data for a specific company in multiple ways, depending on your needs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform a superficial search of the EDGAR data base by providing the `find_company_name()` method with a company name. The following code returns companies with  \"Cisco System\" in their name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is used to download the most up-to-date version of the data directly from EDGAR.  There have been problems with the connection. YMMV.\n",
    "#import dask\n",
    "#from edgar import Edgar\n",
    "#edgar = Edgar()\n",
    "#possible_companies = edgar.find_company_name(\"Cisco System\")\n",
    "#possible_companies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we're loading EDGAR's data from a copy saved on M2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Content</th>\n",
       "      <th>Period of Report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>EXXON MOBIL CORP</td>\n",
       "      <td>usgaapusgovernmentdebtsecuritiesmember country...</td>\n",
       "      <td>2019-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>EXXON MOBIL CORP</td>\n",
       "      <td>united state security exchange commission wash...</td>\n",
       "      <td>2018-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EXXON MOBIL CORP</td>\n",
       "      <td>united state security exchange commission wash...</td>\n",
       "      <td>2017-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>EXXON MOBIL CORP</td>\n",
       "      <td>united state security exchange commission wash...</td>\n",
       "      <td>2016-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EXXON MOBIL CORP</td>\n",
       "      <td>united state security exchange commission wash...</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>MARATHON PETROLEUM CORP</td>\n",
       "      <td>table content united state security exchange c...</td>\n",
       "      <td>2015-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>MARATHON PETROLEUM CORP</td>\n",
       "      <td>table content united state security exchange c...</td>\n",
       "      <td>2014-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>MARATHON PETROLEUM CORP</td>\n",
       "      <td>table content united state security exchange c...</td>\n",
       "      <td>2013-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>MARATHON PETROLEUM CORP</td>\n",
       "      <td>table content united state security exchange c...</td>\n",
       "      <td>2012-12-31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>MARATHON PETROLEUM CORP</td>\n",
       "      <td>table content united state security exchange c...</td>\n",
       "      <td>2011-12-31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Company  \\\n",
       "0           EXXON MOBIL CORP   \n",
       "1           EXXON MOBIL CORP   \n",
       "2           EXXON MOBIL CORP   \n",
       "3           EXXON MOBIL CORP   \n",
       "4           EXXON MOBIL CORP   \n",
       "..                       ...   \n",
       "121  MARATHON PETROLEUM CORP   \n",
       "122  MARATHON PETROLEUM CORP   \n",
       "123  MARATHON PETROLEUM CORP   \n",
       "124  MARATHON PETROLEUM CORP   \n",
       "125  MARATHON PETROLEUM CORP   \n",
       "\n",
       "                                               Content Period of Report  \n",
       "0    usgaapusgovernmentdebtsecuritiesmember country...       2019-12-31  \n",
       "1    united state security exchange commission wash...       2018-12-31  \n",
       "2    united state security exchange commission wash...       2017-12-31  \n",
       "3    united state security exchange commission wash...       2016-12-31  \n",
       "4    united state security exchange commission wash...       2015-12-31  \n",
       "..                                                 ...              ...  \n",
       "121  table content united state security exchange c...       2015-12-31  \n",
       "122  table content united state security exchange c...       2014-12-31  \n",
       "123  table content united state security exchange c...       2013-12-31  \n",
       "124  table content united state security exchange c...       2012-12-31  \n",
       "125  table content united state security exchange c...       2011-12-31  \n",
       "\n",
       "[126 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/scratch/group/history/hist_3368-jguldi/edgar-data.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a rough sense of what's in the 'Content' Column by creating a wordcloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /users/jguldi/.local/lib/python3.6/site-packages (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from wordcloud) (1.17.3)\n",
      "Requirement already satisfied: pillow in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from wordcloud) (6.2.1)\n",
      "Requirement already satisfied: matplotlib in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from wordcloud) (2.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.1.0)\n",
      "Requirement already satisfied: six>=1.10 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.12.0)\n",
      "Requirement already satisfied: pytz in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2019.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.8.0)\n",
      "Requirement already satisfied: setuptools in /hpc/applications/anaconda/3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (41.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud --user\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "stop_words = set(STOPWORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordcloud package has its own built-in function to split a block of text.  It just needs one big block of text assembled from all the rows in the 'Content' column.  We'll use the join() command to paste together all the entries in df['Content'], calling the result 'allwords.'  Then we'l use the WordCloud().generate() command to make a wordcloud from the variable 'allwords'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords # this calls all multilingual stopword lists from NLTK\n",
    "stop = stopwords.words('english') # this command calls only the English stopwords, labeling them \"stop\"\n",
    "stop_set = set(stop) # use the Python native command \"set\" to streamline how the stopwords are stored, improve performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allwords = \" \".join(df['Content'])\n",
    "wordcloud = WordCloud(stopwords=stop, background_color=\"white\").generate(allwords)\n",
    "plt.figure(figsize=(12, 12), dpi = 300)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's visualize the most frequent words, breaking the variable 'allwords' down into individual words using split().  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['usgaapusgovernmentdebtsecuritiesmember',\n",
       " 'countryus',\n",
       " 'usgaapforeignplanmember',\n",
       " 'usgaapforeignplanmember',\n",
       " 'usgaapagingofcapitalizedexploratorywellcostsperiodtwomember',\n",
       " 'srtparentcompanymember',\n",
       " 'countryus',\n",
       " 'usgaapforeignplanmember',\n",
       " 'usgaapnonusmember',\n",
       " 'xomchemicalmember']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordlist = allwords.split()\n",
    "wordlist[:10] # look at the first ten elements of the list only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those aren't words!  Surprise: EDGAR uses a lot of financial 'tags' which are strings of words strung together. You may need to strip these out by stopwording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, count the individual words using the pandas commands \"Series()\" and \"value_counts()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "company      35236\n",
       "million      29748\n",
       "financial    27212\n",
       "tax          23999\n",
       "net          22314\n",
       "year         22098\n",
       "product      21980\n",
       "cost         21721\n",
       "asset        20824\n",
       "income       20719\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordcounts = pd.Series(wordlist).value_counts()[:20]\n",
    "wordcounts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot those values as a well-labeled barchart.  Notice that the axes are well-labeled and that the chart has a title that describes the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x2aaae5c69fd0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordcounts.plot(kind='bar', \n",
    "                title='Most frequent words in the CONTENT column of EDGAR for 8 key companies',\n",
    "                 figsize=(6, 6)\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hansard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hansard is a data set of the 19th-century British Parliamentary debates. Please note: to use Hansard you must significantly increase the memory requested for your JupyterLab session. Per the README, you are suggested to request `64G`. This might result in a longer wait for launching your job but will allow you to hold all of the data in dataframe in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>src_file_id</th>\n",
       "      <th>file_section_id</th>\n",
       "      <th>section_sentence_id</th>\n",
       "      <th>section_monologue_id</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>speechdate</th>\n",
       "      <th>debate</th>\n",
       "      <th>section_category</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>constituency</th>\n",
       "      <th>speaker_house</th>\n",
       "      <th>src_image</th>\n",
       "      <th>src_column</th>\n",
       "      <th>sentence_errata</th>\n",
       "      <th>sentence_entities</th>\n",
       "      <th>entity_labels</th>\n",
       "      <th>X20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1V0001P0_0</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>moved that Lord Walsingham be appointed chairm...</td>\n",
       "      <td>Lord Hawkesbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Walsingham</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S1V0001P0_1</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>in seconding the motion, took an opportunity t...</td>\n",
       "      <td>The Lord Chancellor,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S1V0001P0_2</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>â€”The question was put, and the appointment for...</td>\n",
       "      <td>The Lord Chancellor,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S1V0001P0_3</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>then rose, and observed, that he was not in th...</td>\n",
       "      <td>Lord Walsingham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>12.0</td>\n",
       "      <td>False</td>\n",
       "      <td>two</td>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S1V0001P0_4</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>He had to return his thanks to the House for t...</td>\n",
       "      <td>Lord Walsingham</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>12.0</td>\n",
       "      <td>False</td>\n",
       "      <td>House,House,House</td>\n",
       "      <td>ORG,ORG,ORG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id src_file_id  file_section_id  section_sentence_id  \\\n",
       "0  S1V0001P0_0   S1V0001P0                0                    0   \n",
       "1  S1V0001P0_1   S1V0001P0                0                    1   \n",
       "2  S1V0001P0_2   S1V0001P0                0                    2   \n",
       "3  S1V0001P0_3   S1V0001P0                0                    3   \n",
       "4  S1V0001P0_4   S1V0001P0                0                    4   \n",
       "\n",
       "   section_monologue_id  speech_id  debate_id  speechdate  \\\n",
       "0                     0          0          0  1803-11-22   \n",
       "1                     0          1          0  1803-11-22   \n",
       "2                     1          1          0  1803-11-22   \n",
       "3                     0          2          0  1803-11-22   \n",
       "4                     1          2          0  1803-11-22   \n",
       "\n",
       "                       debate section_category  \\\n",
       "0  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "1  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "2  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "3  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "4  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "\n",
       "                                                text               speaker  \\\n",
       "0  moved that Lord Walsingham be appointed chairm...       Lord Hawkesbury   \n",
       "1  in seconding the motion, took an opportunity t...  The Lord Chancellor,   \n",
       "2  â€”The question was put, and the appointment for...  The Lord Chancellor,   \n",
       "3  then rose, and observed, that he was not in th...       Lord Walsingham   \n",
       "4  He had to return his thanks to the House for t...       Lord Walsingham   \n",
       "\n",
       "   constituency    speaker_house       src_image  src_column  sentence_errata  \\\n",
       "0           NaN  HOUSE OF LORDS.  S1V0001P0I0024        11.0            False   \n",
       "1           NaN  HOUSE OF LORDS.  S1V0001P0I0024        11.0            False   \n",
       "2           NaN  HOUSE OF LORDS.  S1V0001P0I0024        11.0            False   \n",
       "3           NaN  HOUSE OF LORDS.  S1V0001P0I0024        12.0            False   \n",
       "4           NaN  HOUSE OF LORDS.  S1V0001P0I0024        12.0            False   \n",
       "\n",
       "   sentence_entities entity_labels  X20  \n",
       "0         Walsingham        PERSON  NaN  \n",
       "1                NaN           NaN  NaN  \n",
       "2                NaN           NaN  NaN  \n",
       "3                two      CARDINAL  NaN  \n",
       "4  House,House,House   ORG,ORG,ORG  NaN  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hansard = pd.read_csv('/scratch/group/history/hist_3368-jguldi/hansard_justnine_12192019.csv')\n",
    "\n",
    "hansard.head(5) # view just the first five rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the Hansard data we can take a subset so its is smaller and easier to work with. A subset can be thought of as a slice of the data containing just the criteria in which you are interested. \n",
    "\n",
    "This section will demonstrate two ways to take a subset: 1) by date and 2) by the contents of a column.\n",
    "\n",
    "To subset the data by date, first convert the speechdate column from integers to datetimes objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard['speechdate']=pd.to_datetime(hansard['speechdate'], errors='coerce') # any errors are forced to NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can filter for debates from before 1900 . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard1800s = hansard[hansard['speechdate']<dt.datetime(1900,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>src_file_id</th>\n",
       "      <th>file_section_id</th>\n",
       "      <th>section_sentence_id</th>\n",
       "      <th>section_monologue_id</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>speechdate</th>\n",
       "      <th>debate</th>\n",
       "      <th>section_category</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>constituency</th>\n",
       "      <th>speaker_house</th>\n",
       "      <th>src_image</th>\n",
       "      <th>src_column</th>\n",
       "      <th>sentence_errata</th>\n",
       "      <th>sentence_entities</th>\n",
       "      <th>entity_labels</th>\n",
       "      <th>X20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>S1V0001P0_0</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>moved that Lord Walsingham be appointed chairm...</td>\n",
       "      <td>Lord Hawkesbury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Walsingham</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S1V0001P0_1</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>in seconding the motion, took an opportunity t...</td>\n",
       "      <td>The Lord Chancellor,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>S1V0001P0_2</td>\n",
       "      <td>S1V0001P0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1803-11-22</td>\n",
       "      <td>[COMMITTEE OF PRIVILEGES]â€”</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>â€”The question was put, and the appointment for...</td>\n",
       "      <td>The Lord Chancellor,</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS.</td>\n",
       "      <td>S1V0001P0I0024</td>\n",
       "      <td>11.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_id src_file_id  file_section_id  section_sentence_id  \\\n",
       "0  S1V0001P0_0   S1V0001P0                0                    0   \n",
       "1  S1V0001P0_1   S1V0001P0                0                    1   \n",
       "2  S1V0001P0_2   S1V0001P0                0                    2   \n",
       "\n",
       "   section_monologue_id  speech_id  debate_id speechdate  \\\n",
       "0                     0          0          0 1803-11-22   \n",
       "1                     0          1          0 1803-11-22   \n",
       "2                     1          1          0 1803-11-22   \n",
       "\n",
       "                       debate section_category  \\\n",
       "0  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "1  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "2  [COMMITTEE OF PRIVILEGES]â€”    Uncategorized   \n",
       "\n",
       "                                                text               speaker  \\\n",
       "0  moved that Lord Walsingham be appointed chairm...       Lord Hawkesbury   \n",
       "1  in seconding the motion, took an opportunity t...  The Lord Chancellor,   \n",
       "2  â€”The question was put, and the appointment for...  The Lord Chancellor,   \n",
       "\n",
       "   constituency    speaker_house       src_image  src_column  sentence_errata  \\\n",
       "0           NaN  HOUSE OF LORDS.  S1V0001P0I0024        11.0            False   \n",
       "1           NaN  HOUSE OF LORDS.  S1V0001P0I0024        11.0            False   \n",
       "2           NaN  HOUSE OF LORDS.  S1V0001P0I0024        11.0            False   \n",
       "\n",
       "  sentence_entities entity_labels  X20  \n",
       "0        Walsingham        PERSON  NaN  \n",
       "1               NaN           NaN  NaN  \n",
       "2               NaN           NaN  NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard1800s.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ". . . or debates from just the 1870s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard_1870 = hansard[(hansard['speechdate'] >= dt.datetime(1870,1,1)) & (hansard['speechdate'] <= dt.datetime(1879,12,31))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>src_file_id</th>\n",
       "      <th>file_section_id</th>\n",
       "      <th>section_sentence_id</th>\n",
       "      <th>section_monologue_id</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>speechdate</th>\n",
       "      <th>debate</th>\n",
       "      <th>section_category</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>constituency</th>\n",
       "      <th>speaker_house</th>\n",
       "      <th>src_image</th>\n",
       "      <th>src_column</th>\n",
       "      <th>sentence_errata</th>\n",
       "      <th>sentence_entities</th>\n",
       "      <th>entity_labels</th>\n",
       "      <th>X20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4606897</th>\n",
       "      <td>S3V0199P0_0</td>\n",
       "      <td>S3V0199P0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>342939</td>\n",
       "      <td>41076</td>\n",
       "      <td>1870-02-08</td>\n",
       "      <td>THE QUEEN'S SPEECH.</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>delivered HER MAJESTY'S SPEECH to both Houses ...</td>\n",
       "      <td>THE LORD CHANCELLOR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS,</td>\n",
       "      <td>S3V0199P0I0051</td>\n",
       "      <td>2.0</td>\n",
       "      <td>False</td>\n",
       "      <td>SPEECH,Houses of Parliament</td>\n",
       "      <td>ORG,ORG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606898</th>\n",
       "      <td>S3V0199P0_1</td>\n",
       "      <td>S3V0199P0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>342940</td>\n",
       "      <td>41077</td>\n",
       "      <td>1870-02-08</td>\n",
       "      <td>THE QUEEN'S SPEECHâ€” ADDRESS IN ANSWER TO HER M...</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>My Lords, I rise to move that a humble Address...</td>\n",
       "      <td>THE MARQUESS OF HUNTLY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS,</td>\n",
       "      <td>S3V0199P0I0054</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>My Lords</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4606899</th>\n",
       "      <td>S3V0199P0_2</td>\n",
       "      <td>S3V0199P0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>342940</td>\n",
       "      <td>41077</td>\n",
       "      <td>1870-02-08</td>\n",
       "      <td>THE QUEEN'S SPEECHâ€” ADDRESS IN ANSWER TO HER M...</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>I venture to ask your Lordships' indulgence in...</td>\n",
       "      <td>THE MARQUESS OF HUNTLY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF LORDS,</td>\n",
       "      <td>S3V0199P0I0054</td>\n",
       "      <td>7.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Lordships</td>\n",
       "      <td>ORG</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentence_id src_file_id  file_section_id  section_sentence_id  \\\n",
       "4606897  S3V0199P0_0   S3V0199P0                0                    0   \n",
       "4606898  S3V0199P0_1   S3V0199P0                4                    0   \n",
       "4606899  S3V0199P0_2   S3V0199P0                4                    1   \n",
       "\n",
       "         section_monologue_id  speech_id  debate_id speechdate  \\\n",
       "4606897                     0     342939      41076 1870-02-08   \n",
       "4606898                     0     342940      41077 1870-02-08   \n",
       "4606899                     1     342940      41077 1870-02-08   \n",
       "\n",
       "                                                    debate section_category  \\\n",
       "4606897                                THE QUEEN'S SPEECH.    Uncategorized   \n",
       "4606898  THE QUEEN'S SPEECHâ€” ADDRESS IN ANSWER TO HER M...    Uncategorized   \n",
       "4606899  THE QUEEN'S SPEECHâ€” ADDRESS IN ANSWER TO HER M...    Uncategorized   \n",
       "\n",
       "                                                      text  \\\n",
       "4606897  delivered HER MAJESTY'S SPEECH to both Houses ...   \n",
       "4606898  My Lords, I rise to move that a humble Address...   \n",
       "4606899  I venture to ask your Lordships' indulgence in...   \n",
       "\n",
       "                        speaker  constituency    speaker_house  \\\n",
       "4606897     THE LORD CHANCELLOR           NaN  HOUSE OF LORDS,   \n",
       "4606898  THE MARQUESS OF HUNTLY           NaN  HOUSE OF LORDS,   \n",
       "4606899  THE MARQUESS OF HUNTLY           NaN  HOUSE OF LORDS,   \n",
       "\n",
       "              src_image  src_column  sentence_errata  \\\n",
       "4606897  S3V0199P0I0051         2.0            False   \n",
       "4606898  S3V0199P0I0054         7.0            False   \n",
       "4606899  S3V0199P0I0054         7.0            False   \n",
       "\n",
       "                   sentence_entities entity_labels  X20  \n",
       "4606897  SPEECH,Houses of Parliament       ORG,ORG  NaN  \n",
       "4606898                     My Lords        PERSON  NaN  \n",
       "4606899                    Lordships           ORG  NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard_1870.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The columns above are annotations that allow us to figure out how each speech corresponds to the nineteenth-century printed transcripts of speeches delivered in parliament.  \"S3V0199P0_1\" corresponds to the Third Session of Parliament, Volume 199, Page 0.  \n",
    "\n",
    "The speeches are grouped into debates, given under the \"debate\" heading.  Because numerous debates have names like \"REPORT,\" each debate has been assigned a debate_id number corresponding to all the speeches given on a particular date about the same subject. \n",
    "\n",
    "Each speech is also given a unique ID number for data purposes (speech_id).  Each section of the monologue has an id number, as does every sentence within the speech.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching Parliamentary Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also filter for speakers with the name \"Gladstone.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard1800s = hansard1800s[(hansard1800s['speaker'].str.contains('Gladstone'))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>src_file_id</th>\n",
       "      <th>file_section_id</th>\n",
       "      <th>section_sentence_id</th>\n",
       "      <th>section_monologue_id</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>speechdate</th>\n",
       "      <th>debate</th>\n",
       "      <th>section_category</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>constituency</th>\n",
       "      <th>speaker_house</th>\n",
       "      <th>src_image</th>\n",
       "      <th>src_column</th>\n",
       "      <th>sentence_errata</th>\n",
       "      <th>sentence_entities</th>\n",
       "      <th>entity_labels</th>\n",
       "      <th>X20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269940</th>\n",
       "      <td>S1V0040P0_4623</td>\n",
       "      <td>S1V0040P0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>25550</td>\n",
       "      <td>3377</td>\n",
       "      <td>1819-05-06</td>\n",
       "      <td>SECOND REPORT.</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>, a member of the House, and a merchant princi...</td>\n",
       "      <td>Mr. Gladstone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF COMMONS.</td>\n",
       "      <td>S1V0040P0I0090</td>\n",
       "      <td>164.0</td>\n",
       "      <td>False</td>\n",
       "      <td>House,East,West,Bank,two,three millions,Britis...</td>\n",
       "      <td>ORG,LOC,LOC,ORG,CARDINAL,CARDINAL,NORP,NORP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269941</th>\n",
       "      <td>S1V0040P0_4624</td>\n",
       "      <td>S1V0040P0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>25550</td>\n",
       "      <td>3377</td>\n",
       "      <td>1819-05-06</td>\n",
       "      <td>SECOND REPORT.</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>After a full consideration of the evidence, an...</td>\n",
       "      <td>Mr. Gladstone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF COMMONS.</td>\n",
       "      <td>S1V0040P0I0090</td>\n",
       "      <td>164.0</td>\n",
       "      <td>False</td>\n",
       "      <td>5th,July,Bank,Committee,5th</td>\n",
       "      <td>ORDINAL,DATE,ORG,ORG,ORDINAL</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269942</th>\n",
       "      <td>S1V0040P0_4625</td>\n",
       "      <td>S1V0040P0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>25550</td>\n",
       "      <td>3377</td>\n",
       "      <td>1819-05-06</td>\n",
       "      <td>SECOND REPORT.</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>III.</td>\n",
       "      <td>Mr. Gladstone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF COMMONS.</td>\n",
       "      <td>S1V0040P0I0090</td>\n",
       "      <td>164.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           sentence_id src_file_id  file_section_id  section_sentence_id  \\\n",
       "269940  S1V0040P0_4623   S1V0040P0                2                   12   \n",
       "269941  S1V0040P0_4624   S1V0040P0                2                   13   \n",
       "269942  S1V0040P0_4625   S1V0040P0                2                   14   \n",
       "\n",
       "        section_monologue_id  speech_id  debate_id speechdate          debate  \\\n",
       "269940                     0      25550       3377 1819-05-06  SECOND REPORT.   \n",
       "269941                     1      25550       3377 1819-05-06  SECOND REPORT.   \n",
       "269942                     2      25550       3377 1819-05-06  SECOND REPORT.   \n",
       "\n",
       "       section_category                                               text  \\\n",
       "269940    Uncategorized  , a member of the House, and a merchant princi...   \n",
       "269941    Uncategorized  After a full consideration of the evidence, an...   \n",
       "269942    Uncategorized                                               III.   \n",
       "\n",
       "              speaker  constituency      speaker_house       src_image  \\\n",
       "269940  Mr. Gladstone           NaN  HOUSE OF COMMONS.  S1V0040P0I0090   \n",
       "269941  Mr. Gladstone           NaN  HOUSE OF COMMONS.  S1V0040P0I0090   \n",
       "269942  Mr. Gladstone           NaN  HOUSE OF COMMONS.  S1V0040P0I0090   \n",
       "\n",
       "        src_column  sentence_errata  \\\n",
       "269940       164.0            False   \n",
       "269941       164.0            False   \n",
       "269942       164.0            False   \n",
       "\n",
       "                                        sentence_entities  \\\n",
       "269940  House,East,West,Bank,two,three millions,Britis...   \n",
       "269941                        5th,July,Bank,Committee,5th   \n",
       "269942                                                NaN   \n",
       "\n",
       "                                      entity_labels  X20  \n",
       "269940  ORG,LOC,LOC,ORG,CARDINAL,CARDINAL,NORP,NORP  NaN  \n",
       "269941                 ORDINAL,DATE,ORG,ORG,ORDINAL  NaN  \n",
       "269942                                          NaN  NaN  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard1800s.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you wanted to be more specific, you could return instances where speaker \"Gladstone\" utters the word \"Dublin.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hansard1800s = hansard1800s[(hansard1800s['speaker'].str.contains('Gladstone'))&(hansard1800s['text'].str.contains('Dublin'))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>src_file_id</th>\n",
       "      <th>file_section_id</th>\n",
       "      <th>section_sentence_id</th>\n",
       "      <th>section_monologue_id</th>\n",
       "      <th>speech_id</th>\n",
       "      <th>debate_id</th>\n",
       "      <th>speechdate</th>\n",
       "      <th>debate</th>\n",
       "      <th>section_category</th>\n",
       "      <th>text</th>\n",
       "      <th>speaker</th>\n",
       "      <th>constituency</th>\n",
       "      <th>speaker_house</th>\n",
       "      <th>src_image</th>\n",
       "      <th>src_column</th>\n",
       "      <th>sentence_errata</th>\n",
       "      <th>sentence_entities</th>\n",
       "      <th>entity_labels</th>\n",
       "      <th>X20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1032945</th>\n",
       "      <td>S3V0018P0_7180</td>\n",
       "      <td>S3V0018P0</td>\n",
       "      <td>3</td>\n",
       "      <td>371</td>\n",
       "      <td>20</td>\n",
       "      <td>87660</td>\n",
       "      <td>9793</td>\n",
       "      <td>1833-06-03</td>\n",
       "      <td>MINISTERIAL PLAN FOR THE ABOLITION OF SLAVERY. ]</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>and learned member for Dublin said, \"the blood...</td>\n",
       "      <td>Mr. William E. Gladstone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF COMMONS,</td>\n",
       "      <td>S3V0018P0I0173</td>\n",
       "      <td>330.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Dublin,the West Indians</td>\n",
       "      <td>GPE,NORP</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258618</th>\n",
       "      <td>S3V0030P0_14983</td>\n",
       "      <td>S3V0030P0</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>104930</td>\n",
       "      <td>11485</td>\n",
       "      <td>1835-08-21</td>\n",
       "      <td>THE SUPPLIES. â€”CONDUCT OF THE PEERS. ]</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>Member for Dublin would seem to show influence...</td>\n",
       "      <td>Mr. William Ewart Gladstone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF COMMONS,</td>\n",
       "      <td>S3V0030P0I0426</td>\n",
       "      <td>824.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Dublin</td>\n",
       "      <td>GPE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1479435</th>\n",
       "      <td>S3V0042P0_4807</td>\n",
       "      <td>S3V0042P0</td>\n",
       "      <td>0</td>\n",
       "      <td>1135</td>\n",
       "      <td>194</td>\n",
       "      <td>119589</td>\n",
       "      <td>13199</td>\n",
       "      <td>1838-03-30</td>\n",
       "      <td>NEGRO APPRENTICESHIPâ€”ADJOURNED DEBATE. ]</td>\n",
       "      <td>Uncategorized</td>\n",
       "      <td>That bill, says the hon. and learned Member fo...</td>\n",
       "      <td>Mr. W. E. Gladstone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HOUSE OF COMMONS,</td>\n",
       "      <td>S3V0042P0I0118</td>\n",
       "      <td>224.0</td>\n",
       "      <td>False</td>\n",
       "      <td>Dublin,O'Connell,the West Indies</td>\n",
       "      <td>GPE,PERSON,GPE</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             sentence_id src_file_id  file_section_id  section_sentence_id  \\\n",
       "1032945   S3V0018P0_7180   S3V0018P0                3                  371   \n",
       "1258618  S3V0030P0_14983   S3V0030P0                3                   28   \n",
       "1479435   S3V0042P0_4807   S3V0042P0                0                 1135   \n",
       "\n",
       "         section_monologue_id  speech_id  debate_id speechdate  \\\n",
       "1032945                    20      87660       9793 1833-06-03   \n",
       "1258618                     8     104930      11485 1835-08-21   \n",
       "1479435                   194     119589      13199 1838-03-30   \n",
       "\n",
       "                                                   debate section_category  \\\n",
       "1032945  MINISTERIAL PLAN FOR THE ABOLITION OF SLAVERY. ]    Uncategorized   \n",
       "1258618            THE SUPPLIES. â€”CONDUCT OF THE PEERS. ]    Uncategorized   \n",
       "1479435          NEGRO APPRENTICESHIPâ€”ADJOURNED DEBATE. ]    Uncategorized   \n",
       "\n",
       "                                                      text  \\\n",
       "1032945  and learned member for Dublin said, \"the blood...   \n",
       "1258618  Member for Dublin would seem to show influence...   \n",
       "1479435  That bill, says the hon. and learned Member fo...   \n",
       "\n",
       "                             speaker  constituency      speaker_house  \\\n",
       "1032945     Mr. William E. Gladstone           NaN  HOUSE OF COMMONS,   \n",
       "1258618  Mr. William Ewart Gladstone           NaN  HOUSE OF COMMONS,   \n",
       "1479435          Mr. W. E. Gladstone           NaN  HOUSE OF COMMONS,   \n",
       "\n",
       "              src_image  src_column  sentence_errata  \\\n",
       "1032945  S3V0018P0I0173       330.0            False   \n",
       "1258618  S3V0030P0I0426       824.0            False   \n",
       "1479435  S3V0042P0I0118       224.0            False   \n",
       "\n",
       "                        sentence_entities   entity_labels  X20  \n",
       "1032945           Dublin,the West Indians        GPE,NORP  NaN  \n",
       "1258618                            Dublin             GPE  NaN  \n",
       "1479435  Dublin,O'Connell,the West Indies  GPE,PERSON,GPE  NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hansard1800s.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hansard is a large file, so it's wise to remove it from memory after you have taken your subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "del hansard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## US Congress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we have given you code for loading Congress 1967-2010.  You are welcome to use this data for research purposes. However, we also have access to data for Congress from 1870 to 2010. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "US Congress data ([Stanford's Congressional Record](https://data.stanford.edu/congress_text)) is available on M2. If you use this data, please refer to the link for proper citation for the dataset.\n",
    "\n",
    "The data lives within multiple text files. The following code merges these files into a single data frame.\n",
    "\n",
    "> Note: as you read in the data you will see many `skipped line` warnings. This is okay. Jupyter is just telling you which lines we are skipping due to formatting errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 207724: expected 2 fields, saw 3\\nSkipping line 208494: expected 2 fields, saw 5\\n'\n",
      "b'Skipping line 45205: expected 2 fields, saw 3\\nSkipping line 96589: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 9177: expected 2 fields, saw 3\\nSkipping line 9232: expected 2 fields, saw 3\\nSkipping line 10391: expected 2 fields, saw 3\\nSkipping line 10767: expected 2 fields, saw 3\\nSkipping line 19439: expected 2 fields, saw 3\\nSkipping line 20135: expected 2 fields, saw 3\\nSkipping line 38635: expected 2 fields, saw 3\\nSkipping line 46625: expected 2 fields, saw 3\\nSkipping line 67408: expected 2 fields, saw 3\\nSkipping line 96433: expected 2 fields, saw 3\\nSkipping line 111918: expected 2 fields, saw 3\\nSkipping line 126420: expected 2 fields, saw 3\\nSkipping line 127531: expected 2 fields, saw 3\\nSkipping line 142222: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 7466: expected 2 fields, saw 3\\nSkipping line 73461: expected 2 fields, saw 3\\nSkipping line 107105: expected 2 fields, saw 3\\nSkipping line 129228: expected 2 fields, saw 3\\nSkipping line 137032: expected 2 fields, saw 3\\nSkipping line 147548: expected 2 fields, saw 3\\nSkipping line 159276: expected 2 fields, saw 3\\nSkipping line 218461: expected 2 fields, saw 3\\nSkipping line 235858: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 267369: expected 2 fields, saw 4\\nSkipping line 273199: expected 2 fields, saw 3\\nSkipping line 280143: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 233834: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 27252: expected 2 fields, saw 3\\nSkipping line 28765: expected 2 fields, saw 3\\nSkipping line 36128: expected 2 fields, saw 3\\nSkipping line 45424: expected 2 fields, saw 3\\nSkipping line 122486: expected 2 fields, saw 5\\nSkipping line 132756: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 2761: expected 2 fields, saw 3\\nSkipping line 29397: expected 2 fields, saw 3\\nSkipping line 36639: expected 2 fields, saw 3\\nSkipping line 40406: expected 2 fields, saw 3\\nSkipping line 52779: expected 2 fields, saw 3\\nSkipping line 80077: expected 2 fields, saw 3\\nSkipping line 99824: expected 2 fields, saw 3\\nSkipping line 99885: expected 2 fields, saw 3\\nSkipping line 139971: expected 2 fields, saw 3\\nSkipping line 141704: expected 2 fields, saw 3\\nSkipping line 142526: expected 2 fields, saw 3\\nSkipping line 144704: expected 2 fields, saw 30\\nSkipping line 144716: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 10301: expected 2 fields, saw 3\\nSkipping line 119600: expected 2 fields, saw 3\\nSkipping line 154253: expected 2 fields, saw 3\\nSkipping line 164746: expected 2 fields, saw 3\\nSkipping line 167621: expected 2 fields, saw 3\\nSkipping line 170016: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 3868: expected 2 fields, saw 3\\nSkipping line 4170: expected 2 fields, saw 3\\nSkipping line 24283: expected 2 fields, saw 3\\nSkipping line 35613: expected 2 fields, saw 3\\nSkipping line 35704: expected 2 fields, saw 3\\nSkipping line 57443: expected 2 fields, saw 3\\nSkipping line 95845: expected 2 fields, saw 3\\nSkipping line 147639: expected 2 fields, saw 3\\nSkipping line 203930: expected 2 fields, saw 3\\nSkipping line 205059: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 67629: expected 2 fields, saw 3\\nSkipping line 134241: expected 2 fields, saw 4\\nSkipping line 134727: expected 2 fields, saw 3\\nSkipping line 140112: expected 2 fields, saw 3\\nSkipping line 141325: expected 2 fields, saw 3\\nSkipping line 201294: expected 2 fields, saw 4\\n'\n",
      "b'Skipping line 265134: expected 2 fields, saw 3\\nSkipping line 300024: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 31774: expected 2 fields, saw 3\\nSkipping line 60887: expected 2 fields, saw 3\\nSkipping line 141368: expected 2 fields, saw 3\\nSkipping line 142181: expected 2 fields, saw 3\\nSkipping line 142318: expected 2 fields, saw 4\\nSkipping line 144096: expected 2 fields, saw 4\\nSkipping line 152700: expected 2 fields, saw 5\\nSkipping line 154100: expected 2 fields, saw 3\\nSkipping line 162148: expected 2 fields, saw 3\\nSkipping line 163211: expected 2 fields, saw 3\\nSkipping line 164692: expected 2 fields, saw 4\\nSkipping line 164743: expected 2 fields, saw 3\\nSkipping line 167285: expected 2 fields, saw 3\\nSkipping line 167320: expected 2 fields, saw 3\\nSkipping line 167321: expected 2 fields, saw 3\\nSkipping line 167322: expected 2 fields, saw 4\\nSkipping line 169278: expected 2 fields, saw 3\\nSkipping line 169375: expected 2 fields, saw 4\\nSkipping line 169594: expected 2 fields, saw 3\\nSkipping line 170004: expected 2 fields, saw 3\\nSkipping line 170382: expected 2 fields, saw 3\\nSkipping line 170918: expected 2 fields, saw 4\\nSkipping line 178806: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 57289: expected 2 fields, saw 3\\nSkipping line 60260: expected 2 fields, saw 3\\nSkipping line 128248: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 88571: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 87605: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 11652: expected 2 fields, saw 3\\nSkipping line 79880: expected 2 fields, saw 3\\nSkipping line 133618: expected 2 fields, saw 3\\nSkipping line 137954: expected 2 fields, saw 4\\nSkipping line 153352: expected 2 fields, saw 3\\nSkipping line 175909: expected 2 fields, saw 3\\nSkipping line 181845: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 15034: expected 2 fields, saw 3\\nSkipping line 24443: expected 2 fields, saw 3\\nSkipping line 49240: expected 2 fields, saw 3\\nSkipping line 148455: expected 2 fields, saw 3\\nSkipping line 205509: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 29589: expected 2 fields, saw 3\\nSkipping line 30647: expected 2 fields, saw 3\\nSkipping line 44579: expected 2 fields, saw 3\\nSkipping line 70223: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 61196: expected 2 fields, saw 3\\nSkipping line 104099: expected 2 fields, saw 3\\nSkipping line 193088: expected 2 fields, saw 3\\nSkipping line 235639: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 263710: expected 2 fields, saw 3\\nSkipping line 264379: expected 2 fields, saw 3\\nSkipping line 264391: expected 2 fields, saw 3\\nSkipping line 266788: expected 2 fields, saw 3\\nSkipping line 275451: expected 2 fields, saw 3\\nSkipping line 287625: expected 2 fields, saw 3\\nSkipping line 288405: expected 2 fields, saw 5\\nSkipping line 288411: expected 2 fields, saw 3\\nSkipping line 288883: expected 2 fields, saw 3\\nSkipping line 289424: expected 2 fields, saw 3\\nSkipping line 291068: expected 2 fields, saw 3\\nSkipping line 296277: expected 2 fields, saw 3\\nSkipping line 314145: expected 2 fields, saw 3\\nSkipping line 314426: expected 2 fields, saw 3\\nSkipping line 324031: expected 2 fields, saw 3\\nSkipping line 337251: expected 2 fields, saw 3\\nSkipping line 337322: expected 2 fields, saw 3\\nSkipping line 337867: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 1613: expected 2 fields, saw 3\\nSkipping line 5801: expected 2 fields, saw 3\\nSkipping line 6472: expected 2 fields, saw 3\\nSkipping line 14666: expected 2 fields, saw 3\\nSkipping line 27024: expected 2 fields, saw 3\\nSkipping line 28592: expected 2 fields, saw 3\\nSkipping line 35353: expected 2 fields, saw 3\\nSkipping line 36393: expected 2 fields, saw 3\\nSkipping line 36419: expected 2 fields, saw 3\\nSkipping line 38365: expected 2 fields, saw 3\\nSkipping line 39550: expected 2 fields, saw 3\\nSkipping line 46766: expected 2 fields, saw 4\\nSkipping line 55249: expected 2 fields, saw 3\\nSkipping line 58314: expected 2 fields, saw 3\\nSkipping line 73801: expected 2 fields, saw 3\\nSkipping line 76822: expected 2 fields, saw 3\\nSkipping line 80416: expected 2 fields, saw 3\\nSkipping line 96358: expected 2 fields, saw 3\\nSkipping line 97563: expected 2 fields, saw 3\\nSkipping line 99039: expected 2 fields, saw 3\\nSkipping line 110467: expected 2 fields, saw 4\\nSkipping line 110472: expected 2 fields, saw 3\\nSkipping line 128974: expected 2 fields, saw 3\\nSkipping line 136244: expected 2 fields, saw 3\\nSkipping line 136247: expected 2 fields, saw 4\\nSkipping line 136259: expected 2 fields, saw 3\\nSkipping line 136489: expected 2 fields, saw 3\\nSkipping line 137200: expected 2 fields, saw 3\\nSkipping line 137201: expected 2 fields, saw 9\\nSkipping line 137203: expected 2 fields, saw 4\\nSkipping line 137292: expected 2 fields, saw 35\\nSkipping line 137293: expected 2 fields, saw 21\\nSkipping line 137599: expected 2 fields, saw 3\\nSkipping line 144489: expected 2 fields, saw 3\\nSkipping line 144734: expected 2 fields, saw 3\\nSkipping line 144754: expected 2 fields, saw 3\\nSkipping line 147059: expected 2 fields, saw 3\\nSkipping line 151600: expected 2 fields, saw 3\\nSkipping line 153308: expected 2 fields, saw 3\\nSkipping line 158607: expected 2 fields, saw 3\\nSkipping line 159418: expected 2 fields, saw 3\\nSkipping line 161343: expected 2 fields, saw 3\\nSkipping line 165302: expected 2 fields, saw 3\\nSkipping line 168662: expected 2 fields, saw 3\\nSkipping line 174861: expected 2 fields, saw 3\\nSkipping line 175022: expected 2 fields, saw 3\\nSkipping line 179763: expected 2 fields, saw 3\\nSkipping line 183560: expected 2 fields, saw 3\\nSkipping line 185726: expected 2 fields, saw 3\\nSkipping line 185735: expected 2 fields, saw 3\\nSkipping line 187439: expected 2 fields, saw 3\\nSkipping line 187527: expected 2 fields, saw 3\\nSkipping line 192149: expected 2 fields, saw 4\\nSkipping line 193564: expected 2 fields, saw 3\\nSkipping line 197292: expected 2 fields, saw 3\\nSkipping line 197316: expected 2 fields, saw 4\\nSkipping line 201447: expected 2 fields, saw 3\\nSkipping line 204671: expected 2 fields, saw 3\\nSkipping line 205672: expected 2 fields, saw 3\\nSkipping line 212461: expected 2 fields, saw 3\\nSkipping line 216107: expected 2 fields, saw 3\\nSkipping line 219150: expected 2 fields, saw 4\\nSkipping line 219690: expected 2 fields, saw 3\\nSkipping line 223423: expected 2 fields, saw 3\\nSkipping line 224296: expected 2 fields, saw 3\\nSkipping line 226476: expected 2 fields, saw 3\\nSkipping line 232269: expected 2 fields, saw 3\\nSkipping line 232861: expected 2 fields, saw 3\\nSkipping line 234784: expected 2 fields, saw 3\\nSkipping line 237601: expected 2 fields, saw 3\\nSkipping line 247983: expected 2 fields, saw 3\\nSkipping line 248697: expected 2 fields, saw 3\\nSkipping line 250326: expected 2 fields, saw 3\\nSkipping line 250757: expected 2 fields, saw 3\\nSkipping line 251133: expected 2 fields, saw 3\\nSkipping line 252550: expected 2 fields, saw 3\\nSkipping line 252792: expected 2 fields, saw 3\\nSkipping line 255725: expected 2 fields, saw 3\\nSkipping line 259206: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 262262: expected 2 fields, saw 3\\nSkipping line 267193: expected 2 fields, saw 3\\nSkipping line 268644: expected 2 fields, saw 3\\nSkipping line 271239: expected 2 fields, saw 3\\nSkipping line 271555: expected 2 fields, saw 3\\nSkipping line 280092: expected 2 fields, saw 3\\nSkipping line 280392: expected 2 fields, saw 3\\nSkipping line 287767: expected 2 fields, saw 3\\nSkipping line 296989: expected 2 fields, saw 3\\nSkipping line 299058: expected 2 fields, saw 3\\nSkipping line 299924: expected 2 fields, saw 3\\nSkipping line 312348: expected 2 fields, saw 3\\nSkipping line 319424: expected 2 fields, saw 3\\nSkipping line 319912: expected 2 fields, saw 3\\nSkipping line 332039: expected 2 fields, saw 3\\nSkipping line 346231: expected 2 fields, saw 3\\nSkipping line 359009: expected 2 fields, saw 3\\nSkipping line 361814: expected 2 fields, saw 3\\nSkipping line 365422: expected 2 fields, saw 4\\nSkipping line 368962: expected 2 fields, saw 3\\nSkipping line 369002: expected 2 fields, saw 4\\nSkipping line 369003: expected 2 fields, saw 3\\nSkipping line 369005: expected 2 fields, saw 4\\nSkipping line 370115: expected 2 fields, saw 3\\nSkipping line 370118: expected 2 fields, saw 3\\nSkipping line 390605: expected 2 fields, saw 3\\nSkipping line 400027: expected 2 fields, saw 3\\nSkipping line 401855: expected 2 fields, saw 3\\nSkipping line 410876: expected 2 fields, saw 3\\nSkipping line 412856: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 18762: expected 2 fields, saw 3\\nSkipping line 89054: expected 2 fields, saw 3\\nSkipping line 121520: expected 2 fields, saw 3\\nSkipping line 130998: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 154538: expected 2 fields, saw 3\\nSkipping line 238979: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 270491: expected 2 fields, saw 3\\nSkipping line 271048: expected 2 fields, saw 3\\n'\n",
      "b'Skipping line 55163: expected 2 fields, saw 3\\n'\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import glob\n",
    "\n",
    "directory = '/scratch/group/oit_research_data/stanford_congress/hein-bound/'\n",
    "file_type = 'txt'\n",
    "delim ='|'\n",
    "\n",
    "speeches_df = pd.concat([pd.read_csv(f, sep=delim, encoding=\"ISO-8859-1\", error_bad_lines=False, quoting=csv.QUOTE_NONE) \n",
    "                         for f in glob.glob(directory + \"speeches_*\"+file_type)])\n",
    "\n",
    "descr_df = pd.concat([pd.read_csv(f, sep=delim, encoding=\"ISO-8859-1\", error_bad_lines=False, quoting=csv.QUOTE_NONE) \n",
    "                      for f in glob.glob(directory + \"descr_*\"+file_type)])\n",
    "\n",
    "all_congressional_data = pd.merge(speeches_df, descr_df, on='speech_id').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_congressional_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the data, you can take a subset. The following code filters the speaker column for just the speaker \"Mr. Dole.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_dole = all_congressional_data[(all_congressional_data.speaker == \"Mr. DOLE\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bob_dole[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already seen examples of working with the Congress data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit Archive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DISCLAIMER**: The Reddit data is broken into 92 (yes, 92!) files totalling ~250G of Reddit data. This is big data!!! You can access individual files using ~`6-15G`, but please see Steph if you need help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reddit data is separated by month (Oct. 2007 to May 2015). Later dates are larger in file size, so you may need to allocated more memory in your JupyterLab session before reading them. September 2010, for example, is only 1G while May 2015 is over 12G."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a data frame of reddit data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import glob\n",
    "\n",
    "reddit_files = glob.glob('/scratch/group/oit_research_data/reddit/*.tsv') # you can replace astrict with a specific reddit file if you want to read just one file\n",
    "\n",
    "# Add or remove entries to this list to change which months are loaded\n",
    "DATES_TO_LOAD = [\n",
    "    # (year, month),\n",
    "    (2008, 10),\n",
    "    (2008, 11),\n",
    "]\n",
    "\n",
    "list_dfs = []\n",
    "for file in reddit_files:\n",
    "    filename = file.rsplit('/', maxsplit=1)[-1].rsplit('.', maxsplit=1)[0]\n",
    "    filetime = datetime.strptime(filename, 'RC_%Y-%m')\n",
    "    if (filetime.year, filetime.month) in DATES_TO_LOAD:\n",
    "        list_dfs.append(pd.read_csv(file, sep='\\t'))\n",
    "\n",
    "reddit_df = pd.concat(list_dfs)\n",
    "del list_dfs[:]\n",
    "\n",
    "# Recommended: drop deleted comments from your dataset    \n",
    "reddit_df = reddit_df[reddit_df['body'] != '[deleted]']\n",
    "\n",
    "# Drop instances of NaN (empty comments)\n",
    "reddit_df = reddit_df[~reddit_df['body'].isna()]\n",
    "\n",
    "# Subreddit filtering example:\n",
    "subreddit_filter = ('politics', 'worldnews', 'news')\n",
    "reddit_df=reddit_df[reddit_df['subreddit'].isin(subreddit_filter)]\n",
    "\n",
    "display(reddit_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that `reddit_files = glob.glob('/scratch/group/oit_research_data/reddit/*.tsv')` can be replaced with a specific reddit file to avoid loading ALL of the data. Consider: `reddit_files = glob.glob('/scratch/group/oit_research_data/reddit/RC_2008-10.tsv')` for just October 2008."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After reading the data, we can take a subset through various ways.\n",
    "\n",
    "For example, we can filter the reddit data frame for specific subreddits, like those containing the word \"climate.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_by_keyword = reddit_df[(reddit_df['body'].str.contains('climate'))].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_by_keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Practice searching and counting with Reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have practiced reading in the data, let's do something with it! \n",
    "\n",
    "For this first exercise we will just count top words in the reddit data from November 2008.\n",
    "\n",
    "First read in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df = pd.read_csv('/scratch/group/oit_research_data/reddit/RC_2008-11.tsv', sep='\\t')\n",
    "\n",
    "reddit_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now let's do a little cleaning for a better outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove comments that have been deleted and thus have \"deleted\" as their text body\n",
    "reddit_df = reddit_df[reddit_df['body'] != '[deleted]']\n",
    "\n",
    "# filter for just the subreddits we wish to operate on\n",
    "subreddit_filter = ('politics', 'worldnews', 'news')\n",
    "reddit_df = reddit_df[reddit_df['subreddit'].isin(subreddit_filter)]\n",
    "\n",
    "# load stopwords\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop = stopwords.words('english')\n",
    "stopwords_regex = r'\\b(?:{})\\b'.format('|'.join(stop))\n",
    "\n",
    "# clean, clean, clean \n",
    "reddit_df['body'] = reddit_df['body'].str.lower() # transform words to lowercase\n",
    "reddit_df['body'] = reddit_df['body'].str.replace('[^\\w\\s]','') # remove punctuation\n",
    "reddit_df['body'] = reddit_df['body'].str.replace(stopwords_regex,'') # remove stopwords\n",
    "\n",
    "reddit_df = reddit_df[~reddit_df['body'].isna()] # remove blank comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at our cleaned data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To count top words we need to operate on the `body` column. We can't go straight to counting words from here, however.\n",
    "\n",
    "To a human reader, every word in the body column appears as a separate, individual word. These words are easy for us to count. But, this isn't the case to for a machine reader. At its present state, a machine reads the text in each row as an entire unit. For this reason, we need to split the words of each row on the spaces. \n",
    "\n",
    "The following code splits the words into individual tokens that a computer can read (split()), then \"explodes\" the words so that each word gets one line, then counts them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_count = reddit_df[\"body\"].str.split().explode().dropna().value_counts()\n",
    "reddit_count[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great--we have counted every word.\n",
    "\n",
    "Looking at this output, however, reveals a problem: words like \"what,\" \"with,\" or \"to\" have high word counts! If we were to visualize the top words without some additional cleaning we might end up with a visualization of \"the\" or \"and,\" which doesn't tell us anything interesting about our data: the subreddits for politics, world news, and news.\n",
    "\n",
    "Data scientists often call these bloat words \"stop words\" and remove them from the data. We can do this by defining a list of stop words and removing items from `reddit_count` that match a stop word. \n",
    "\n",
    "> Note: removing stop words requires careful consideration. The following list removes pronouns like \"he\" or \"she,\" which is useful here, but would be detrimental for an analysis on gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = ['dont', 'im', 'thats', 'way', 'going', 'youre', 'gt', 'doesnt', 'said', 'point', 'actually', 'cant', 'thing',\n",
    "                   'didnt', 'things', 'isnt', 'sure', 'yes', 'two', 'hes', 'got', 'every', 'lot', 'saying', 'anyone', 'probably', 'oh']\n",
    "\n",
    "custom_stopwords_regex = r'\\b(?:{})\\b'.format('|'.join(custom_stopwords))\n",
    "reddit_df['body'] = reddit_df['body'].str.replace(custom_stopwords_regex,'') # remove stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count again and look at the difference: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_count = reddit_df[\"body\"].str.split().explode().dropna().value_counts()\n",
    "reddit_count[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib as plt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = matplotlib.pyplot.gcf()\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "plt.barh(reddit_count.index[:10], reddit_count[:10]) # visualize the count. \n",
    "plt.gca().invert_yaxis() # reverse the order of the bars so the biggest one is on top\n",
    "plt.title = (\"Top Words in the Politics, Worldnews, and News Subreddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot our data using a scatter plot. We will substitute `barh()` (for horizontal bar chart) with `scatter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.scatter(reddit_count.index[:10], reddit_count[:10], color='blue')\n",
    "plt.xlabel(\"Count\")\n",
    "plt.ylabel(\"Word\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.title(\"Top Words in the Politics, Worldnews, and News Subreddit\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dallas and Houston City Council Minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section gives access infomration for two separate datasets: the Dallas City Council minutes and the Houston City Council minutes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "council_dir = '/scratch/group/history/hist_3368-jguldi/city_council'\n",
    "\n",
    "dallas_minutes = pd.read_csv(f'{council_dir}/dallas.csv', sep='|', index_col='index') # read in the Dallas City Council minutes\n",
    "dallas_minutes['Date'] = pd.to_datetime(dallas_minutes['Date'], format='%Y-%m-%d') # cast the data column to datetime objects \n",
    "\n",
    "houston_minutes = pd.read_csv(f'{council_dir}/houston.csv', sep='|', index_col='index')# read in the Houston City Council minutes\n",
    "houston_minutes['Date'] = pd.to_datetime(houston_minutes['Date'], format='%Y-%m-%d') # cast the data column to datetime objects \n",
    "\n",
    "print(dallas_minutes[:5])\n",
    "houston_minutes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the previous examples, you can take a subset of the data. The following code filters for data from before January 1, 2000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dallas_minutes[dallas_minutes['Date'] < datetime(year=2000, month=1, day=1)][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code filters for data from just the month of September."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houston_minutes[houston_minutes['Date'].dt.month == 9][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code filters for data from just Tuesday."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houston_minutes[houston_minutes['Date'].dt.day_name() == 'Tuesday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NovelTM Datasets for English-Language Fiction, 1700-2009\n",
    "\n",
    "The NovelTM datasets are a collection of metadata for 210,305 volumes of fiction. Performing quantitative measures with these datasets can lend insight into literary history. The types of questions a researcher might ask include: how does the proportion of fiction written by British authors or by women change across time?\n",
    "\n",
    "The NovelTM datasets are vast. They include files that can be used to determine authorial gender, standardize author names, detect nationality, and more. These files are spread across different folders. This section will only focus on a few of these folders and their data sets. You are welcome to explore the data that isn't covered in this Notebook by reading the `readme.md` file. \n",
    "\n",
    "For citation information, please see [this link](https://culturalanalytics.org/article/13147-noveltm-datasets-for-english-language-fiction-1700-2009)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading Datasets of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.The Title Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_noveltm_metadata = \"/scratch/group/history/hist_3368-jguldi/tedunderwood-noveltmmeta-451ae72/metadata\"\n",
    "\n",
    "title_metadata = pd.read_csv(f'{path_to_noveltm_metadata}/titlemeta.tsv', sep='\\t')\n",
    "\n",
    "title_metadata.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.The Gender-Balanced Subset\n",
    "\n",
    "The gender-balanced subset was taken from a fiction titles data frame. It has been reduced in size to ensure equal representation of writers who were identified as men or women in each 5-year segment of time. The curators also included a proportional sample of texts where gender was marked \"unknown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_balanced_subset = pd.read_csv(f'{path_to_noveltm_metadata}/gender_balanced_subset.tsv', sep='\\t')\n",
    "\n",
    "gender_balanced_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.The Frequently Reprinted Subset\n",
    "\n",
    "The frequently reprinted subset was taken from a fiction titles data frame. It is made up of volumes where the curators had the largest number of editions and instances attested within 25 years of a title's first appearance in HathiTrust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequently_reprinted_subset = pd.read_csv(f'{path_to_noveltm_metadata}/frequently_reprinted_subset.tsv', sep='\\t')\n",
    "\n",
    "frequently_reprinted_subset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Author Set (a list of authors and their standardized names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_noveltm_dedup = \"/scratch/group/history/hist_3368-jguldi/tedunderwood-noveltmmeta-451ae72/dedup/\"\n",
    "\n",
    "authorsets = pd.read_csv(f'{path_to_noveltm_dedup}/authorsets.tsv', sep='\\t')\n",
    "\n",
    "authorsets.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring NovelTM \n",
    "\n",
    "If you want to further explore NovelTM you can read the `readme.md` file and open files in different directories. \n",
    "\n",
    "Here's a little knowledge that can help you explore the data:\n",
    "\n",
    "The different directories (main directories and sub directories) make up a \"file hierarchy.\" You can view NovelTM's file hierarchy like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "\n",
    "path_to_noveltm = \"/scratch/group/history/hist_3368-jguldi/tedunderwood-noveltmmeta-451ae72/\"\n",
    "\n",
    "glob.glob('{}/*'.format(path_to_noveltm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the names at the end of path represent a folder containing data (minus LICENSE and `readme.md`). You can open a folder by adding it to the directory path defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_noveltm = \"/scratch/group/history/hist_3368-jguldi/tedunderwood-noveltmmeta-451ae72/metadata/\"\n",
    "\n",
    "glob.glob('{}/*'.format(path_to_noveltm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that from the `metadata` folder we only opened `titlemeta.tsv`, `gender_balanced_subset.tsv` and `frequently_reprinted_data.tsv`, but there are many more files!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jupyter does not offer an easy interface to navigate and understand files. For purposes of exploration, you should consider opening a Shell and using the command `cd /scratch/group/history/hist_3368-jguldi/tedunderwood-noveltmmeta-451ae72\"` followed by `ls`. This interface will show all the files in a directory. The folders you can change into are in blue font, whereas data is in white font and ends with `.tsv`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Project Gutenberg](https://www.gutenberg.org/) is a library of over 60,000 digitized books in the public domain. It is best known for its abundant literary texts.\n",
    "\n",
    "First import the modules for accessing and processing the Project Gutenberg text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gutenberg.acquire import load_etext\n",
    "from gutenberg.query import get_metadata\n",
    "from gutenberg.cleanup import strip_headers\n",
    "\n",
    "gutenberg_mirror = 'https://gutenberg.pglaf.org/' # the mirror from which we will access and download books"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have used Project Gutenberg before, you may have noticed that at the top of each html page is meta data that looks something like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Project Gutenberg EBook of Pride and Prejudice, by Jane Austen\n",
    ">\n",
    "> This eBook is for the use of anyone anywhere at no cost and with\n",
    "> almost no restrictions whatsoever.  You may copy it, give it away or\n",
    "> re-use it under the terms of the Project Gutenberg License included\n",
    "> with this eBook or online at www.gutenberg.org\n",
    "> \n",
    ">Title: Pride and Prejudice\n",
    "> \n",
    "> Author: Jane Austen\n",
    "> \n",
    "> Release Date: August 26, 2008 [EBook #1342]\n",
    "> Last Updated: November 12, 2019\n",
    "> \n",
    "> Language: English\n",
    "> \n",
    "> Character set encoding: UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important to note is the EBook #1342 ID tag. Using the gutenberg Python module, you can download books based on this ID. Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = [\n",
    "    [1342,\"Pride and Prejudice\",\"Jane Austen\"],\n",
    "    [11,\"Alice's Adventures in Wonderland\", \"Lewis Carroll\"],\n",
    "    [2701,\"Moby Dick; Or, The Whale\",\"Herman Melville\"],\n",
    "    [84,\"Frankenstein; Or, The Modern Prometheus\", \"Mary Wollenstonecraft Shelley\" ],\n",
    "    [345,\"Dracula\", \"Bram Stoker\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a data frame from the books variable that has columns for the ID, Title, and Author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenDF = pd.DataFrame(books, columns=['ID','Title','Author']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load the books' full text. We will call it 'FullText.'\n",
    "\n",
    "We will not teach the commands below -- however, you will recognize .replace(\"\\n\", \"\"), the command to remove whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenDF['FullText'] = gutenDF.apply(lambda row: strip_headers(load_etext(row['ID'], mirror=gutenberg_mirror)).replace(\"\\n\", \"\").replace(\"[Illustration]\", \"\") , axis=1)\n",
    "\n",
    "gutenDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the FullText column above.  You will still need to lowercase, strip punctuation, and possibly stopword the FullText to work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search c19 Novels for words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter our returns based on criteria, such as the characters a title contains. The following code searches the `Title` column for the word \"Whale\" and returns the full text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whale = gutenDF[gutenDF['Title'].str.contains(\"Whale\")]['FullText'].copy()\n",
    "whale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searching for titles that contains \"Whale\" returns __Moby Dick__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MetaData and Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you plan on doing a lot of work with Project Gutenberg's metadata functionality, you'll need to cache their metadata first. This can take a very long time but makes it possible to query their metadata quickly.\n",
    "\n",
    "We will not use the commands below for now, but they are included for later problem-solving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gutenberg.acquire import get_metadata_cache\n",
    "# cache = get_metadata_cache()\n",
    "# cache.populate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gutenberg.query import get_etexts\n",
    "# from gutenberg.query import get_metadata\n",
    "\n",
    "# print(get_metadata('title', 11))  # prints frozenset([u'Moby Dick; Or, The Whale'])\n",
    "# print(get_metadata('author', 11)) # prints frozenset([u'Melville, Hermann'])\n",
    "\n",
    "# print(get_etexts('title', 'Moby Dick; Or, The Whale'))  # prints frozenset([2701, ...])\n",
    "# print(get_etexts('author', 'Melville, Hermann'))        # prints frozenset([2701, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASSIGNMENT (working in groups):\n",
    "\n",
    "Please work through the code in the Jupyter notebook to load your dataset. With your group's dataset, do the following:\n",
    "\n",
    "1) Find the column for speakers, authors, individual posters or businesses.  \n",
    "\n",
    "   - Count how many contributions each individual made.  \n",
    "   - Take a screenshot of the resulting table. \n",
    "\n",
    "2) Find the text column. \n",
    "\n",
    "   - Print out the first 100 lines of the first speech or post. \n",
    "   - Find the longest item in the text column.\n",
    "   - Apply .split(), .explode and value_counts() to longest item to count how many words it contains.\n",
    "   - Make a bar plot of those words. \n",
    "\n",
    "Upload to Canvas the following as screenshots or images:\n",
    "\n",
    "   - one table of actors and how many items they contributed\n",
    "   - the first lines of a speech\n",
    "   - a bar plot of the word counts of the longest item in the text column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
